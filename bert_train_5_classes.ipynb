{"cells":[{"cell_type":"markdown","metadata":{"id":"5MUEX9u_oycS"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"majG-QpMoycT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7690200-6b0d-42bc-dc4c-904596358012"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","!pip install transformers\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","device = torch.device(\"cuda\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"cQWhdoJQoycW"},"source":["<ul>\n","    <li style=\"font-size:150%;\">The dataset consists of two columns – “label” and “text”. The column “text” contains the message body and the “label” is a binary variable where 1 means spam and 0 means the message is not a spam.</li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"MSyTZ64qoycY"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APqU7Db0oycY"},"outputs":[],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('zhihan1996/DNA_bert_6')\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('zhihan1996/DNA_bert_6')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3Z4SaoTvcsz"},"outputs":[],"source":["import pickle\n","\n","drive_path = \"/content/drive/MyDrive/Virus-DNA-classification-BERT-main/old_code/data10/data5/\"\n","# with open(drive_path + 'train_text_5.pkl', 'rb') as f:\n","#     train_text = pickle.load(f)\n","# with open(drive_path + 'temp_text_5.pkl', 'rb') as f:\n","#     temp_text = pickle.load(f)\n","# with open(drive_path + 'train_labels_5.pkl', 'rb') as f:\n","#     train_labels = pickle.load(f)\n","# with open(drive_path + 'temp_labels_5.pkl', 'rb') as f:\n","#     temp_labels = pickle.load(f)\n","\n","# with open(drive_path + 'val_text_5.pkl', 'rb') as f:\n","#     val_text = pickle.load(f)\n","with open(drive_path + 'test_text_5.pkl', 'rb') as f:\n","    test_text = pickle.load(f)\n","# with open(drive_path + 'val_labels_5.pkl', 'rb') as f:\n","#     val_labels = pickle.load(f)\n","with open(drive_path + 'test_labels_5.pkl', 'rb') as f:\n","    test_labels = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"9k5NRXxToycZ"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">4. Tokenize & Encode the Sequences</h1>"]},{"cell_type":"markdown","metadata":{"id":"SbLay3HDoycZ"},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">Which Tokenization strategy is used by BERT?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the existing words in the vocabulary are iteratively added.</p>\n","<br>\n","<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"markdown","metadata":{"id":"z-Eh8hwbxCkM"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMqGYrPmoyca"},"outputs":[],"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=512,\n","    padding=True,  # pad to max len\n","    truncation=True,  # truncate to max len\n","    return_attention_mask=True,\n","    return_tensors=\"pt\"\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHMPN2u_67CH"},"outputs":[],"source":["# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=512,\n","    padding=True,  # pad to max len\n","    truncation=True,  # truncate to max len\n","    return_attention_mask=True,\n","    return_tensors=\"pt\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBlwfCMr7OTE"},"outputs":[],"source":["# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=512,\n","    padding=True,  # pad to max len\n","    truncation=True,  # truncate to max len\n","    return_attention_mask=True,\n","    return_tensors=\"pt\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"CFCquUO47-C6"},"source":["# Load Tokens"]},{"cell_type":"markdown","metadata":{"id":"_l-5xmWgoycb"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDp1WJ-woycb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691524272701,"user_tz":-180,"elapsed":21,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"outputId":"f7899ab9-ddff-42f8-8898-9fd8edbaac45"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-7fa9788b6389>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  test_seq = torch.tensor(tokens_test['input_ids'])\n","<ipython-input-8-7fa9788b6389>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  test_mask = torch.tensor(tokens_test['attention_mask'])\n"]}],"source":["# train_seq = torch.tensor(tokens_train['input_ids'])\n","# train_mask = torch.tensor(tokens_train['attention_mask'])\n","# train_y = torch.tensor(train_labels.tolist())\n","\n","# val_seq = torch.tensor(tokens_val['input_ids'])\n","# val_mask = torch.tensor(tokens_val['attention_mask'])\n","# val_y = torch.tensor(val_labels.tolist())\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"]},{"cell_type":"markdown","metadata":{"id":"ZXkX49Tqoycb"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8Hy99lloycc","executionInfo":{"status":"error","timestamp":1691524292239,"user_tz":-180,"elapsed":327,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"colab":{"base_uri":"https://localhost:8080/","height":245},"outputId":"207d02d1-b8e4-433a-d179-066759488a76"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-67f4cda08ed8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# sampler for sampling the data during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_seq' is not defined"]}],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"code","source":["train_dataloader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvLGc1cQ-m70","executionInfo":{"status":"ok","timestamp":1691485430439,"user_tz":-180,"elapsed":44,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"outputId":"818d8ed3-8552-451b-9bbc-5014647a14c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7aa98f38d450>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"4hMHk5oLoycc"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-koZtzhmoycc"},"outputs":[],"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCwiSSrzoycd"},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","\n","        self.bert = bert\n","\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,5)\n","\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","        #pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","\n","        x = self.fc1(cls_hs)\n","\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","\n","        # output layer\n","        x = self.fc2(x)\n","\n","        # apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjMTt-kyoyce"},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Okny-lvCoyci","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691524312682,"user_tz":-180,"elapsed":2245,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"outputId":"c413ecb3-fa1c-4ae2-c900-a93fe7a3291f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bK6YObxNoycj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691487168737,"user_tz":-180,"elapsed":135,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"outputId":"6d1dddc0-3e0d-4f4a-f2ea-f6ca68bb8e2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 2 3 4]\n","Class Weights: [1. 1. 1. 1. 1.]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","print(np.unique(train_labels))\n","#compute the class weights\n","class_weights = compute_class_weight(class_weight = \"balanced\", classes = np.unique(train_labels), y = train_labels)\n","\n","print(\"Class Weights:\",class_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9YlgUvroyck","executionInfo":{"status":"error","timestamp":1691524319889,"user_tz":-180,"elapsed":317,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"colab":{"base_uri":"https://localhost:8080/","height":245},"outputId":"ddf1d8da-cb23-4334-eef2-88347a22faa3"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-537097aa269a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# converting list of class weights to a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# push to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'class_weights' is not defined"]}],"source":["\n","# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","# push to GPU\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 500"]},{"cell_type":"markdown","metadata":{"id":"oJpzOQKWoycn"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxFl3-Cooycp"},"outputs":[],"source":["# function to train the model\n","def train():\n","\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save model predictions\n","    total_preds=[]\n","\n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","\n","      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","      # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftIuiIc6oyc2"},"outputs":[],"source":["# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step,batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds,labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(tokens_val)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wShkxn5Eoyc3","outputId":"73512913-d9a9-4936-8ada-1678bda4c6e5","executionInfo":{"status":"error","timestamp":1691523829691,"user_tz":-180,"elapsed":20976,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n"," Epoch 1 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.611\n","Validation Loss: 6.406\n","\n"," Epoch 2 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.606\n","Validation Loss: 6.382\n","\n"," Epoch 3 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.596\n","Validation Loss: 6.356\n","\n"," Epoch 4 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.590\n","Validation Loss: 6.334\n","\n"," Epoch 5 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.589\n","Validation Loss: 6.313\n","\n"," Epoch 6 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.585\n","Validation Loss: 6.296\n","\n"," Epoch 7 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.577\n","Validation Loss: 6.279\n","\n"," Epoch 8 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.572\n","Validation Loss: 6.262\n","\n"," Epoch 9 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.572\n","Validation Loss: 6.245\n","\n"," Epoch 10 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.567\n","Validation Loss: 6.233\n","\n"," Epoch 11 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.561\n","Validation Loss: 6.222\n","\n"," Epoch 12 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.559\n","Validation Loss: 6.207\n","\n"," Epoch 13 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.556\n","Validation Loss: 6.197\n","\n"," Epoch 14 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.552\n","Validation Loss: 6.186\n","\n"," Epoch 15 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.549\n","Validation Loss: 6.177\n","\n"," Epoch 16 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.546\n","Validation Loss: 6.168\n","\n"," Epoch 17 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.542\n","Validation Loss: 6.156\n","\n"," Epoch 18 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.541\n","Validation Loss: 6.147\n","\n"," Epoch 19 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.536\n","Validation Loss: 6.139\n","\n"," Epoch 20 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.537\n","Validation Loss: 6.131\n","\n"," Epoch 21 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.534\n","Validation Loss: 6.122\n","\n"," Epoch 22 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.525\n","Validation Loss: 6.119\n","\n"," Epoch 23 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.525\n","Validation Loss: 6.111\n","\n"," Epoch 24 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.525\n","Validation Loss: 6.101\n","\n"," Epoch 25 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.513\n","Validation Loss: 6.092\n","\n"," Epoch 26 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.511\n","Validation Loss: 6.086\n","\n"," Epoch 27 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.519\n","Validation Loss: 6.079\n","\n"," Epoch 28 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.513\n","Validation Loss: 6.072\n","\n"," Epoch 29 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.511\n","Validation Loss: 6.064\n","\n"," Epoch 30 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.506\n","Validation Loss: 6.057\n","\n"," Epoch 31 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.505\n","Validation Loss: 6.055\n","\n"," Epoch 32 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.494\n","Validation Loss: 6.052\n","\n"," Epoch 33 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.495\n","Validation Loss: 6.049\n","\n"," Epoch 34 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.506\n","Validation Loss: 6.039\n","\n"," Epoch 35 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.493\n","Validation Loss: 6.033\n","\n"," Epoch 36 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.503\n","Validation Loss: 6.029\n","\n"," Epoch 37 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.497\n","Validation Loss: 6.023\n","\n"," Epoch 38 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.497\n","Validation Loss: 6.015\n","\n"," Epoch 39 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.500\n","Validation Loss: 6.006\n","\n"," Epoch 40 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.489\n","Validation Loss: 6.003\n","\n"," Epoch 41 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.494\n","Validation Loss: 5.997\n","\n"," Epoch 42 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.487\n","Validation Loss: 5.994\n","\n"," Epoch 43 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.483\n","Validation Loss: 5.992\n","\n"," Epoch 44 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.486\n","Validation Loss: 5.991\n","\n"," Epoch 45 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.490\n","Validation Loss: 5.987\n","\n"," Epoch 46 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.482\n","Validation Loss: 5.983\n","\n"," Epoch 47 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.492\n","Validation Loss: 5.979\n","\n"," Epoch 48 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.477\n","Validation Loss: 5.977\n","\n"," Epoch 49 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.477\n","Validation Loss: 5.970\n","\n"," Epoch 50 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.484\n","Validation Loss: 5.968\n","\n"," Epoch 51 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.486\n","Validation Loss: 5.962\n","\n"," Epoch 52 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.475\n","Validation Loss: 5.958\n","\n"," Epoch 53 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.475\n","Validation Loss: 5.956\n","\n"," Epoch 54 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.473\n","Validation Loss: 5.956\n","\n"," Epoch 55 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.476\n","Validation Loss: 5.951\n","\n"," Epoch 56 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.467\n","Validation Loss: 5.954\n","\n"," Epoch 57 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.469\n","Validation Loss: 5.954\n","\n"," Epoch 58 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.480\n","Validation Loss: 5.950\n","\n"," Epoch 59 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.475\n","Validation Loss: 5.950\n","\n"," Epoch 60 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.466\n","Validation Loss: 5.948\n","\n"," Epoch 61 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.479\n","Validation Loss: 5.941\n","\n"," Epoch 62 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.466\n","Validation Loss: 5.941\n","\n"," Epoch 63 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.466\n","Validation Loss: 5.938\n","\n"," Epoch 64 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.457\n","Validation Loss: 5.932\n","\n"," Epoch 65 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.467\n","Validation Loss: 5.927\n","\n"," Epoch 66 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.457\n","Validation Loss: 5.924\n","\n"," Epoch 67 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.467\n","Validation Loss: 5.921\n","\n"," Epoch 68 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.464\n","Validation Loss: 5.918\n","\n"," Epoch 69 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.466\n","Validation Loss: 5.918\n","\n"," Epoch 70 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.470\n","Validation Loss: 5.918\n","\n"," Epoch 71 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.451\n","Validation Loss: 5.918\n","\n"," Epoch 72 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.456\n","Validation Loss: 5.917\n","\n"," Epoch 73 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.464\n","Validation Loss: 5.909\n","\n"," Epoch 74 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.451\n","Validation Loss: 5.910\n","\n"," Epoch 75 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.453\n","Validation Loss: 5.911\n","\n"," Epoch 76 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.452\n","Validation Loss: 5.909\n","\n"," Epoch 77 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.464\n","Validation Loss: 5.908\n","\n"," Epoch 78 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.463\n","Validation Loss: 5.904\n","\n"," Epoch 79 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.462\n","Validation Loss: 5.899\n","\n"," Epoch 80 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.455\n","Validation Loss: 5.896\n","\n"," Epoch 81 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.448\n","Validation Loss: 5.897\n","\n"," Epoch 82 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.460\n","Validation Loss: 5.895\n","\n"," Epoch 83 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.449\n","Validation Loss: 5.896\n","\n"," Epoch 84 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.448\n","Validation Loss: 5.891\n","\n"," Epoch 85 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.452\n","Validation Loss: 5.891\n","\n"," Epoch 86 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.462\n","Validation Loss: 5.891\n","\n"," Epoch 87 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.449\n","Validation Loss: 5.887\n","\n"," Epoch 88 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.452\n","Validation Loss: 5.886\n","\n"," Epoch 89 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.443\n","Validation Loss: 5.884\n","\n"," Epoch 90 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.450\n","Validation Loss: 5.882\n","\n"," Epoch 91 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.442\n","Validation Loss: 5.882\n","\n"," Epoch 92 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.445\n","Validation Loss: 5.881\n","\n"," Epoch 93 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.432\n","Validation Loss: 5.879\n","\n"," Epoch 94 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.431\n","Validation Loss: 5.880\n","\n"," Epoch 95 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.451\n","Validation Loss: 5.881\n","\n"," Epoch 96 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.466\n","Validation Loss: 5.879\n","\n"," Epoch 97 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.441\n","Validation Loss: 5.880\n","\n"," Epoch 98 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.446\n","Validation Loss: 5.880\n","\n"," Epoch 99 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.445\n","Validation Loss: 5.876\n","\n"," Epoch 100 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.440\n","Validation Loss: 5.874\n","\n"," Epoch 101 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.453\n","Validation Loss: 5.873\n","\n"," Epoch 102 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.447\n","Validation Loss: 5.868\n","\n"," Epoch 103 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.452\n","Validation Loss: 5.867\n","\n"," Epoch 104 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.436\n","Validation Loss: 5.869\n","\n"," Epoch 105 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.429\n","Validation Loss: 5.868\n","\n"," Epoch 106 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.438\n","Validation Loss: 5.870\n","\n"," Epoch 107 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.439\n","Validation Loss: 5.868\n","\n"," Epoch 108 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.438\n","Validation Loss: 5.866\n","\n"," Epoch 109 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.866\n","\n"," Epoch 110 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.445\n","Validation Loss: 5.864\n","\n"," Epoch 111 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.452\n","Validation Loss: 5.861\n","\n"," Epoch 112 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.443\n","Validation Loss: 5.858\n","\n"," Epoch 113 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.443\n","Validation Loss: 5.858\n","\n"," Epoch 114 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.449\n","Validation Loss: 5.855\n","\n"," Epoch 115 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.435\n","Validation Loss: 5.855\n","\n"," Epoch 116 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.442\n","Validation Loss: 5.853\n","\n"," Epoch 117 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.440\n","Validation Loss: 5.850\n","\n"," Epoch 118 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.436\n","Validation Loss: 5.848\n","\n"," Epoch 119 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.438\n","Validation Loss: 5.846\n","\n"," Epoch 120 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.437\n","Validation Loss: 5.844\n","\n"," Epoch 121 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.436\n","Validation Loss: 5.841\n","\n"," Epoch 122 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.431\n","Validation Loss: 5.837\n","\n"," Epoch 123 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.444\n","Validation Loss: 5.833\n","\n"," Epoch 124 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.444\n","Validation Loss: 5.830\n","\n"," Epoch 125 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.434\n","Validation Loss: 5.826\n","\n"," Epoch 126 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.429\n","Validation Loss: 5.825\n","\n"," Epoch 127 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.438\n","Validation Loss: 5.824\n","\n"," Epoch 128 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.442\n","Validation Loss: 5.826\n","\n"," Epoch 129 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.438\n","Validation Loss: 5.819\n","\n"," Epoch 130 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.437\n","Validation Loss: 5.822\n","\n"," Epoch 131 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.436\n","Validation Loss: 5.820\n","\n"," Epoch 132 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.435\n","Validation Loss: 5.819\n","\n"," Epoch 133 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.431\n","Validation Loss: 5.819\n","\n"," Epoch 134 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.435\n","Validation Loss: 5.818\n","\n"," Epoch 135 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.819\n","\n"," Epoch 136 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.425\n","Validation Loss: 5.817\n","\n"," Epoch 137 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.425\n","Validation Loss: 5.817\n","\n"," Epoch 138 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.433\n","Validation Loss: 5.820\n","\n"," Epoch 139 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.427\n","Validation Loss: 5.820\n","\n"," Epoch 140 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.440\n","Validation Loss: 5.817\n","\n"," Epoch 141 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.427\n","Validation Loss: 5.817\n","\n"," Epoch 142 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.431\n","Validation Loss: 5.820\n","\n"," Epoch 143 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.423\n","Validation Loss: 5.821\n","\n"," Epoch 144 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.436\n","Validation Loss: 5.819\n","\n"," Epoch 145 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.437\n","Validation Loss: 5.816\n","\n"," Epoch 146 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.434\n","Validation Loss: 5.811\n","\n"," Epoch 147 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.426\n","Validation Loss: 5.812\n","\n"," Epoch 148 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.816\n","\n"," Epoch 149 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.432\n","Validation Loss: 5.813\n","\n"," Epoch 150 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.436\n","Validation Loss: 5.810\n","\n"," Epoch 151 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.431\n","Validation Loss: 5.808\n","\n"," Epoch 152 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.808\n","\n"," Epoch 153 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.426\n","Validation Loss: 5.809\n","\n"," Epoch 154 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.436\n","Validation Loss: 5.812\n","\n"," Epoch 155 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.421\n","Validation Loss: 5.809\n","\n"," Epoch 156 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.809\n","\n"," Epoch 157 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.420\n","Validation Loss: 5.809\n","\n"," Epoch 158 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.806\n","\n"," Epoch 159 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.433\n","Validation Loss: 5.806\n","\n"," Epoch 160 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.417\n","Validation Loss: 5.808\n","\n"," Epoch 161 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.425\n","Validation Loss: 5.809\n","\n"," Epoch 162 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.425\n","Validation Loss: 5.809\n","\n"," Epoch 163 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.808\n","\n"," Epoch 164 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.422\n","Validation Loss: 5.812\n","\n"," Epoch 165 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.428\n","Validation Loss: 5.808\n","\n"," Epoch 166 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.429\n","Validation Loss: 5.804\n","\n"," Epoch 167 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.420\n","Validation Loss: 5.804\n","\n"," Epoch 168 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.415\n","Validation Loss: 5.805\n","\n"," Epoch 169 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.417\n","Validation Loss: 5.802\n","\n"," Epoch 170 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.418\n","Validation Loss: 5.799\n","\n"," Epoch 171 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.426\n","Validation Loss: 5.800\n","\n"," Epoch 172 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.425\n","Validation Loss: 5.796\n","\n"," Epoch 173 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.413\n","Validation Loss: 5.798\n","\n"," Epoch 174 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.428\n","Validation Loss: 5.794\n","\n"," Epoch 175 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.794\n","\n"," Epoch 176 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.791\n","\n"," Epoch 177 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.425\n","Validation Loss: 5.789\n","\n"," Epoch 178 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.789\n","\n"," Epoch 179 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.789\n","\n"," Epoch 180 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.786\n","\n"," Epoch 181 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.418\n","Validation Loss: 5.785\n","\n"," Epoch 182 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.419\n","Validation Loss: 5.783\n","\n"," Epoch 183 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.419\n","Validation Loss: 5.784\n","\n"," Epoch 184 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.421\n","Validation Loss: 5.781\n","\n"," Epoch 185 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.419\n","Validation Loss: 5.780\n","\n"," Epoch 186 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.780\n","\n"," Epoch 187 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.417\n","Validation Loss: 5.778\n","\n"," Epoch 188 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.778\n","\n"," Epoch 189 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.411\n","Validation Loss: 5.782\n","\n"," Epoch 190 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.412\n","Validation Loss: 5.785\n","\n"," Epoch 191 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.420\n","Validation Loss: 5.783\n","\n"," Epoch 192 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.407\n","Validation Loss: 5.785\n","\n"," Epoch 193 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.785\n","\n"," Epoch 194 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.408\n","Validation Loss: 5.783\n","\n"," Epoch 195 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.405\n","Validation Loss: 5.782\n","\n"," Epoch 196 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.782\n","\n"," Epoch 197 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.419\n","Validation Loss: 5.777\n","\n"," Epoch 198 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.420\n","Validation Loss: 5.774\n","\n"," Epoch 199 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.411\n","Validation Loss: 5.772\n","\n"," Epoch 200 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.414\n","Validation Loss: 5.771\n","\n"," Epoch 201 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.424\n","Validation Loss: 5.772\n","\n"," Epoch 202 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.417\n","Validation Loss: 5.773\n","\n"," Epoch 203 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.408\n","Validation Loss: 5.776\n","\n"," Epoch 204 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.418\n","Validation Loss: 5.777\n","\n"," Epoch 205 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.415\n","Validation Loss: 5.777\n","\n"," Epoch 206 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.414\n","Validation Loss: 5.780\n","\n"," Epoch 207 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.430\n","Validation Loss: 5.778\n","\n"," Epoch 208 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.409\n","Validation Loss: 5.774\n","\n"," Epoch 209 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.417\n","Validation Loss: 5.772\n","\n"," Epoch 210 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.425\n","Validation Loss: 5.770\n","\n"," Epoch 211 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.407\n","Validation Loss: 5.767\n","\n"," Epoch 212 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.407\n","Validation Loss: 5.767\n","\n"," Epoch 213 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.408\n","Validation Loss: 5.766\n","\n"," Epoch 214 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.420\n","Validation Loss: 5.766\n","\n"," Epoch 215 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.419\n","Validation Loss: 5.766\n","\n"," Epoch 216 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.765\n","\n"," Epoch 217 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.399\n","Validation Loss: 5.765\n","\n"," Epoch 218 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.411\n","Validation Loss: 5.763\n","\n"," Epoch 219 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.407\n","Validation Loss: 5.764\n","\n"," Epoch 220 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.764\n","\n"," Epoch 221 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.411\n","Validation Loss: 5.764\n","\n"," Epoch 222 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.400\n","Validation Loss: 5.762\n","\n"," Epoch 223 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.409\n","Validation Loss: 5.759\n","\n"," Epoch 224 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.422\n","Validation Loss: 5.761\n","\n"," Epoch 225 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.408\n","Validation Loss: 5.763\n","\n"," Epoch 226 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.417\n","Validation Loss: 5.763\n","\n"," Epoch 227 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.404\n","Validation Loss: 5.765\n","\n"," Epoch 228 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.410\n","Validation Loss: 5.767\n","\n"," Epoch 229 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.410\n","Validation Loss: 5.766\n","\n"," Epoch 230 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.422\n","Validation Loss: 5.765\n","\n"," Epoch 231 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.402\n","Validation Loss: 5.761\n","\n"," Epoch 232 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.402\n","Validation Loss: 5.760\n","\n"," Epoch 233 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.410\n","Validation Loss: 5.761\n","\n"," Epoch 234 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.411\n","Validation Loss: 5.760\n","\n"," Epoch 235 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.405\n","Validation Loss: 5.757\n","\n"," Epoch 236 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.410\n","Validation Loss: 5.756\n","\n"," Epoch 237 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.407\n","Validation Loss: 5.757\n","\n"," Epoch 238 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.417\n","Validation Loss: 5.755\n","\n"," Epoch 239 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.753\n","\n"," Epoch 240 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.753\n","\n"," Epoch 241 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.756\n","\n"," Epoch 242 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.412\n","Validation Loss: 5.759\n","\n"," Epoch 243 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.404\n","Validation Loss: 5.758\n","\n"," Epoch 244 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.759\n","\n"," Epoch 245 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.762\n","\n"," Epoch 246 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.399\n","Validation Loss: 5.761\n","\n"," Epoch 247 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.413\n","Validation Loss: 5.764\n","\n"," Epoch 248 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.762\n","\n"," Epoch 249 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.392\n","Validation Loss: 5.763\n","\n"," Epoch 250 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.760\n","\n"," Epoch 251 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.759\n","\n"," Epoch 252 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.410\n","Validation Loss: 5.756\n","\n"," Epoch 253 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.412\n","Validation Loss: 5.757\n","\n"," Epoch 254 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.397\n","Validation Loss: 5.755\n","\n"," Epoch 255 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.759\n","\n"," Epoch 256 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.410\n","Validation Loss: 5.759\n","\n"," Epoch 257 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.402\n","Validation Loss: 5.758\n","\n"," Epoch 258 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.399\n","Validation Loss: 5.751\n","\n"," Epoch 259 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.753\n","\n"," Epoch 260 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.404\n","Validation Loss: 5.754\n","\n"," Epoch 261 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.395\n","Validation Loss: 5.755\n","\n"," Epoch 262 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.394\n","Validation Loss: 5.750\n","\n"," Epoch 263 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.751\n","\n"," Epoch 264 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.750\n","\n"," Epoch 265 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.747\n","\n"," Epoch 266 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.399\n","Validation Loss: 5.749\n","\n"," Epoch 267 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.408\n","Validation Loss: 5.743\n","\n"," Epoch 268 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.395\n","Validation Loss: 5.740\n","\n"," Epoch 269 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.384\n","Validation Loss: 5.743\n","\n"," Epoch 270 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.405\n","Validation Loss: 5.745\n","\n"," Epoch 271 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.397\n","Validation Loss: 5.743\n","\n"," Epoch 272 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.743\n","\n"," Epoch 273 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.745\n","\n"," Epoch 274 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.404\n","Validation Loss: 5.743\n","\n"," Epoch 275 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.402\n","Validation Loss: 5.743\n","\n"," Epoch 276 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.414\n","Validation Loss: 5.741\n","\n"," Epoch 277 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.740\n","\n"," Epoch 278 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.407\n","Validation Loss: 5.741\n","\n"," Epoch 279 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.400\n","Validation Loss: 5.740\n","\n"," Epoch 280 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.744\n","\n"," Epoch 281 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.416\n","Validation Loss: 5.744\n","\n"," Epoch 282 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.382\n","Validation Loss: 5.742\n","\n"," Epoch 283 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.739\n","\n"," Epoch 284 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.740\n","\n"," Epoch 285 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.738\n","\n"," Epoch 286 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.398\n","Validation Loss: 5.740\n","\n"," Epoch 287 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.398\n","Validation Loss: 5.739\n","\n"," Epoch 288 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.392\n","Validation Loss: 5.740\n","\n"," Epoch 289 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.399\n","Validation Loss: 5.744\n","\n"," Epoch 290 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.745\n","\n"," Epoch 291 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.744\n","\n"," Epoch 292 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.395\n","Validation Loss: 5.745\n","\n"," Epoch 293 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.390\n","Validation Loss: 5.745\n","\n"," Epoch 294 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.745\n","\n"," Epoch 295 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.406\n","Validation Loss: 5.744\n","\n"," Epoch 296 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.395\n","Validation Loss: 5.744\n","\n"," Epoch 297 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.402\n","Validation Loss: 5.742\n","\n"," Epoch 298 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.398\n","Validation Loss: 5.743\n","\n"," Epoch 299 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.404\n","Validation Loss: 5.738\n","\n"," Epoch 300 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.395\n","Validation Loss: 5.736\n","\n"," Epoch 301 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.734\n","\n"," Epoch 302 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.390\n","Validation Loss: 5.737\n","\n"," Epoch 303 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.390\n","Validation Loss: 5.735\n","\n"," Epoch 304 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.405\n","Validation Loss: 5.739\n","\n"," Epoch 305 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.392\n","Validation Loss: 5.738\n","\n"," Epoch 306 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.390\n","Validation Loss: 5.733\n","\n"," Epoch 307 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.403\n","Validation Loss: 5.736\n","\n"," Epoch 308 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.397\n","Validation Loss: 5.737\n","\n"," Epoch 309 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.732\n","\n"," Epoch 310 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.730\n","\n"," Epoch 311 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.728\n","\n"," Epoch 312 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.399\n","Validation Loss: 5.725\n","\n"," Epoch 313 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.387\n","Validation Loss: 5.724\n","\n"," Epoch 314 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.396\n","Validation Loss: 5.727\n","\n"," Epoch 315 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.726\n","\n"," Epoch 316 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.395\n","Validation Loss: 5.722\n","\n"," Epoch 317 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.388\n","Validation Loss: 5.724\n","\n"," Epoch 318 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.721\n","\n"," Epoch 319 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.719\n","\n"," Epoch 320 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.384\n","Validation Loss: 5.721\n","\n"," Epoch 321 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.398\n","Validation Loss: 5.716\n","\n"," Epoch 322 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.396\n","Validation Loss: 5.717\n","\n"," Epoch 323 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.394\n","Validation Loss: 5.715\n","\n"," Epoch 324 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.390\n","Validation Loss: 5.714\n","\n"," Epoch 325 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.400\n","Validation Loss: 5.712\n","\n"," Epoch 326 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.396\n","Validation Loss: 5.709\n","\n"," Epoch 327 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.710\n","\n"," Epoch 328 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.710\n","\n"," Epoch 329 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.398\n","Validation Loss: 5.706\n","\n"," Epoch 330 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.707\n","\n"," Epoch 331 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.392\n","Validation Loss: 5.706\n","\n"," Epoch 332 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.708\n","\n"," Epoch 333 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.711\n","\n"," Epoch 334 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.396\n","Validation Loss: 5.714\n","\n"," Epoch 335 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.394\n","Validation Loss: 5.715\n","\n"," Epoch 336 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.714\n","\n"," Epoch 337 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.390\n","Validation Loss: 5.712\n","\n"," Epoch 338 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.713\n","\n"," Epoch 339 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.715\n","\n"," Epoch 340 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.714\n","\n"," Epoch 341 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.383\n","Validation Loss: 5.711\n","\n"," Epoch 342 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.710\n","\n"," Epoch 343 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.705\n","\n"," Epoch 344 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.396\n","Validation Loss: 5.709\n","\n"," Epoch 345 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.706\n","\n"," Epoch 346 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.704\n","\n"," Epoch 347 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.707\n","\n"," Epoch 348 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.388\n","Validation Loss: 5.710\n","\n"," Epoch 349 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.711\n","\n"," Epoch 350 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.404\n","Validation Loss: 5.715\n","\n"," Epoch 351 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.394\n","Validation Loss: 5.717\n","\n"," Epoch 352 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.379\n","Validation Loss: 5.712\n","\n"," Epoch 353 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.402\n","Validation Loss: 5.711\n","\n"," Epoch 354 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.709\n","\n"," Epoch 355 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.384\n","Validation Loss: 5.707\n","\n"," Epoch 356 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.401\n","Validation Loss: 5.706\n","\n"," Epoch 357 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.705\n","\n"," Epoch 358 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.395\n","Validation Loss: 5.707\n","\n"," Epoch 359 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.378\n","Validation Loss: 5.706\n","\n"," Epoch 360 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.711\n","\n"," Epoch 361 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.365\n","Validation Loss: 5.713\n","\n"," Epoch 362 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.713\n","\n"," Epoch 363 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.383\n","Validation Loss: 5.712\n","\n"," Epoch 364 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.388\n","Validation Loss: 5.712\n","\n"," Epoch 365 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.380\n","Validation Loss: 5.711\n","\n"," Epoch 366 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.712\n","\n"," Epoch 367 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.384\n","Validation Loss: 5.712\n","\n"," Epoch 368 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.394\n","Validation Loss: 5.710\n","\n"," Epoch 369 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.373\n","Validation Loss: 5.713\n","\n"," Epoch 370 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.712\n","\n"," Epoch 371 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.712\n","\n"," Epoch 372 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.375\n","Validation Loss: 5.715\n","\n"," Epoch 373 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.712\n","\n"," Epoch 374 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.388\n","Validation Loss: 5.709\n","\n"," Epoch 375 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.706\n","\n"," Epoch 376 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.708\n","\n"," Epoch 377 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.378\n","Validation Loss: 5.709\n","\n"," Epoch 378 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.400\n","Validation Loss: 5.705\n","\n"," Epoch 379 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.703\n","\n"," Epoch 380 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.388\n","Validation Loss: 5.703\n","\n"," Epoch 381 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.399\n","Validation Loss: 5.701\n","\n"," Epoch 382 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.373\n","Validation Loss: 5.702\n","\n"," Epoch 383 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.373\n","Validation Loss: 5.700\n","\n"," Epoch 384 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.696\n","\n"," Epoch 385 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.375\n","Validation Loss: 5.699\n","\n"," Epoch 386 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.696\n","\n"," Epoch 387 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.397\n","Validation Loss: 5.695\n","\n"," Epoch 388 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.387\n","Validation Loss: 5.696\n","\n"," Epoch 389 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.698\n","\n"," Epoch 390 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.372\n","Validation Loss: 5.701\n","\n"," Epoch 391 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.392\n","Validation Loss: 5.703\n","\n"," Epoch 392 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.701\n","\n"," Epoch 393 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.382\n","Validation Loss: 5.695\n","\n"," Epoch 394 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.383\n","Validation Loss: 5.691\n","\n"," Epoch 395 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.374\n","Validation Loss: 5.690\n","\n"," Epoch 396 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.375\n","Validation Loss: 5.696\n","\n"," Epoch 397 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.377\n","Validation Loss: 5.694\n","\n"," Epoch 398 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.371\n","Validation Loss: 5.695\n","\n"," Epoch 399 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.695\n","\n"," Epoch 400 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.377\n","Validation Loss: 5.695\n","\n"," Epoch 401 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.383\n","Validation Loss: 5.692\n","\n"," Epoch 402 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.381\n","Validation Loss: 5.690\n","\n"," Epoch 403 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.690\n","\n"," Epoch 404 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.390\n","Validation Loss: 5.689\n","\n"," Epoch 405 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.380\n","Validation Loss: 5.688\n","\n"," Epoch 406 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.382\n","Validation Loss: 5.690\n","\n"," Epoch 407 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.370\n","Validation Loss: 5.691\n","\n"," Epoch 408 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.377\n","Validation Loss: 5.689\n","\n"," Epoch 409 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.389\n","Validation Loss: 5.689\n","\n"," Epoch 410 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.369\n","Validation Loss: 5.687\n","\n"," Epoch 411 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.377\n","Validation Loss: 5.687\n","\n"," Epoch 412 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.387\n","Validation Loss: 5.684\n","\n"," Epoch 413 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.682\n","\n"," Epoch 414 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.373\n","Validation Loss: 5.680\n","\n"," Epoch 415 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.379\n","Validation Loss: 5.682\n","\n"," Epoch 416 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.381\n","Validation Loss: 5.685\n","\n"," Epoch 417 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.378\n","Validation Loss: 5.685\n","\n"," Epoch 418 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.688\n","\n"," Epoch 419 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.372\n","Validation Loss: 5.685\n","\n"," Epoch 420 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.369\n","Validation Loss: 5.686\n","\n"," Epoch 421 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.366\n","Validation Loss: 5.686\n","\n"," Epoch 422 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.375\n","Validation Loss: 5.687\n","\n"," Epoch 423 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.384\n","Validation Loss: 5.688\n","\n"," Epoch 424 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.694\n","\n"," Epoch 425 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.363\n","Validation Loss: 5.691\n","\n"," Epoch 426 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.357\n","Validation Loss: 5.691\n","\n"," Epoch 427 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.688\n","\n"," Epoch 428 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.374\n","Validation Loss: 5.686\n","\n"," Epoch 429 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.687\n","\n"," Epoch 430 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.381\n","Validation Loss: 5.682\n","\n"," Epoch 431 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.375\n","Validation Loss: 5.683\n","\n"," Epoch 432 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.681\n","\n"," Epoch 433 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.382\n","Validation Loss: 5.679\n","\n"," Epoch 434 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.380\n","Validation Loss: 5.679\n","\n"," Epoch 435 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.368\n","Validation Loss: 5.684\n","\n"," Epoch 436 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.683\n","\n"," Epoch 437 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.683\n","\n"," Epoch 438 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.386\n","Validation Loss: 5.690\n","\n"," Epoch 439 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.383\n","Validation Loss: 5.688\n","\n"," Epoch 440 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.377\n","Validation Loss: 5.687\n","\n"," Epoch 441 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.394\n","Validation Loss: 5.683\n","\n"," Epoch 442 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.682\n","\n"," Epoch 443 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.385\n","Validation Loss: 5.681\n","\n"," Epoch 444 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.366\n","Validation Loss: 5.682\n","\n"," Epoch 445 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.378\n","Validation Loss: 5.685\n","\n"," Epoch 446 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.372\n","Validation Loss: 5.684\n","\n"," Epoch 447 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.375\n","Validation Loss: 5.685\n","\n"," Epoch 448 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.366\n","Validation Loss: 5.686\n","\n"," Epoch 449 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.373\n","Validation Loss: 5.686\n","\n"," Epoch 450 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.368\n","Validation Loss: 5.689\n","\n"," Epoch 451 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.377\n","Validation Loss: 5.691\n","\n"," Epoch 452 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.393\n","Validation Loss: 5.685\n","\n"," Epoch 453 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.379\n","Validation Loss: 5.680\n","\n"," Epoch 454 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.377\n","Validation Loss: 5.681\n","\n"," Epoch 455 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.391\n","Validation Loss: 5.680\n","\n"," Epoch 456 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.380\n","Validation Loss: 5.680\n","\n"," Epoch 457 / 500\n","  Batch    50  of     55.\n","\n","Evaluating...\n","\n","Training Loss: 1.376\n","Validation Loss: 5.679\n","\n"," Epoch 458 / 500\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-cd41d1a26d9a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-c85916b2a54d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# add on to the total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# backward pass to calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    #train model\n","    train_loss, _ = train()\n","\n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    #save the best model\n","    if epoch % 10 == 0 and valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), drive_path + 'saved_weights_5.pkl')\n","        with open(drive_path + 'train_losses_5.pkl', 'wb') as f:\n","            pickle.dump(train_losses, f)\n","        with open(drive_path + 'valid_losses_5.pkl', 'wb') as f:\n","            pickle.dump(valid_losses, f)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]},{"cell_type":"code","source":["with open(drive_path + 'train_losses_5.pkl', 'wb') as f:\n","    pickle.dump(train_losses, f)\n","with open(drive_path + 'valid_losses_5.pkl', 'wb') as f:\n","    pickle.dump(valid_losses, f)"],"metadata":{"id":"ZKvXGfKX8aUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(valid_losses, label='Validation Loss')\n","\n","last_train_loss = round(train_losses[-1], 4)\n","last_valid_loss = round(valid_losses[-1], 4)\n","\n","plt.text(len(train_losses), train_losses[-1], str(last_train_loss), ha='right', va='top')\n","plt.text(len(valid_losses), valid_losses[-1], str(last_valid_loss), ha='right', va='top')\n","\n","# Add labels and title\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Train and Validation Losses Until Last Saved Best Model')\n","\n","# Add a legend to differentiate train and validation losses\n","plt.legend()\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"ZvRu-Zbgwf6v","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1691523940969,"user_tz":-180,"elapsed":926,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"outputId":"38ce0d45-507f-4359-d682-6dd73413ec00"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPo8M6Aooyc3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691524333918,"user_tz":-180,"elapsed":7207,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"}},"outputId":"0c497a10-2f70-41cb-e9e8-858378d49fa9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":17}],"source":["#load weights of best model\n","\n","path = \"/content/drive/MyDrive/Virus-DNA-classification-BERT-main/old_code/data10/data5/saved_weights_5.pkl\"\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{"id":"SKmzY2R9oyc4"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">9. Make Predictions</h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hm9C8K4goyc4"},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1691524345633,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"},"user_tz":-180},"id":"00LG5cUioyc4","outputId":"60648810-1e8e-4538-a417-19912bf619da"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.56      0.12      0.20        75\n","           1       0.57      0.65      0.61        75\n","           2       0.28      0.57      0.37        75\n","           3       0.21      0.05      0.09        75\n","           4       0.30      0.40      0.34        75\n","\n","    accuracy                           0.36       375\n","   macro avg       0.38      0.36      0.32       375\n","weighted avg       0.38      0.36      0.32       375\n","\n"]}],"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]},{"cell_type":"markdown","metadata":{"id":"KNtFXHd3oyc5"},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">REFERENCES & CREDITS</h1></center>"]},{"cell_type":"markdown","metadata":{"id":"VrTs1qhboyc5"},"source":["<ol>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.reddit.com/r/MachineLearning/comments/ao23cp/p_how_to_use_bert_in_kaggle_competitions_a/\">How to use BERT in Kaggle competitions - Reddit Thread</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\">A visual guide to using BERT by Jay Alammar</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\">Demystifying BERT: Groundbreaking NLP Framework by Mohd Sanad Zaki Rizvi</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\">BERT for Dummies step by step tutorial by Michel Kana</a></li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"id":"JzcrrWRToyc6"},"source":["<br>\n","<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">CONCLUSION</h1></center>"]},{"cell_type":"markdown","metadata":{"id":"KhNy7_-foyc6"},"source":["<p style=\"font-size:150%; font-family:verdana;\">BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it’s approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this Notebook we have discussed about BERT (Theoritical + Practical Part).</p>"]},{"cell_type":"markdown","metadata":{"id":"vstECEGroyc6"},"source":["<center><h1 style=\"font-size:200%; color:green;\">Please give this kernel an UPVOTE to show your appreciation, if you find it useful.</h1></center>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQVQxH2toyc7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}