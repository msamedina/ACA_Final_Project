{"cells":[{"cell_type":"markdown","metadata":{"id":"5MUEX9u_oycS"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49191,"status":"ok","timestamp":1691494037051,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"},"user_tz":-180},"id":"majG-QpMoycT","outputId":"c573c344-e40f-4faf-9d97-81b379486cb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","!pip install transformers\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","device = torch.device(\"cuda\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"cQWhdoJQoycW"},"source":["<ul>\n","    <li style=\"font-size:150%;\">The dataset consists of two columns – “label” and “text”. The column “text” contains the message body and the “label” is a binary variable where 1 means spam and 0 means the message is not a spam.</li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"MSyTZ64qoycY"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2357,"status":"ok","timestamp":1691494200658,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"},"user_tz":-180},"id":"APqU7Db0oycY"},"outputs":[],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('zhihan1996/DNA_bert_6')\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('zhihan1996/DNA_bert_6')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"S3Z4SaoTvcsz","executionInfo":{"status":"ok","timestamp":1691494379759,"user_tz":-180,"elapsed":7790,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["import pickle\n","\n","#drive_path = \"/content/drive/MyDrive/Virus-DNA-classification-BERT-main/old_code/data10/data3\"\n","drive_path = \"/content/drive/MyDrive/old_code/data10/data3/data3\"\n","\n","with open(drive_path + 'train_text_3.pkl', 'rb') as f:\n","    train_text = pickle.load(f)\n","with open(drive_path + 'temp_text_3.pkl', 'rb') as f:\n","    temp_text = pickle.load(f)\n","with open(drive_path + 'train_labels_3.pkl', 'rb') as f:\n","    train_labels = pickle.load(f)\n","with open(drive_path + 'temp_labels_3.pkl', 'rb') as f:\n","    temp_labels = pickle.load(f)\n","\n","with open(drive_path + 'val_text_3.pkl', 'rb') as f:\n","    val_text = pickle.load(f)\n","with open(drive_path + 'test_text_3.pkl', 'rb') as f:\n","    test_text = pickle.load(f)\n","with open(drive_path + 'val_labels_3.pkl', 'rb') as f:\n","    val_labels = pickle.load(f)\n","with open(drive_path + 'test_labels_3.pkl', 'rb') as f:\n","    test_labels = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"9k5NRXxToycZ"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">4. Tokenize & Encode the Sequences</h1>"]},{"cell_type":"markdown","metadata":{"id":"SbLay3HDoycZ"},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">Which Tokenization strategy is used by BERT?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the existing words in the vocabulary are iteratively added.</p>\n","<br>\n","<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"markdown","metadata":{"id":"z-Eh8hwbxCkM"},"source":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"NMqGYrPmoyca","executionInfo":{"status":"ok","timestamp":1691494803243,"user_tz":-180,"elapsed":52265,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length=512,\n","    padding=True,  # pad to max len\n","    truncation=True,  # truncate to max len\n","    return_attention_mask=True,\n","    return_tensors=\"pt\"\n",")\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ZHMPN2u_67CH","executionInfo":{"status":"ok","timestamp":1691494982173,"user_tz":-180,"elapsed":11541,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length=512,\n","    padding=True,  # pad to max len\n","    truncation=True,  # truncate to max len\n","    return_attention_mask=True,\n","    return_tensors=\"pt\"\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"CBlwfCMr7OTE","executionInfo":{"status":"ok","timestamp":1691494992258,"user_tz":-180,"elapsed":10126,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length=512,\n","    padding=True,  # pad to max len\n","    truncation=True,  # truncate to max len\n","    return_attention_mask=True,\n","    return_tensors=\"pt\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"CFCquUO47-C6"},"source":["# Load Tokens"]},{"cell_type":"markdown","metadata":{"id":"_l-5xmWgoycb"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"QDp1WJ-woycb","executionInfo":{"status":"ok","timestamp":1691494992260,"user_tz":-180,"elapsed":33,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"efa5326b-d0c5-4f92-c9ef-0909df2da31a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-40b617136a1e>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_seq = torch.tensor(tokens_train['input_ids'])\n","<ipython-input-14-40b617136a1e>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  train_mask = torch.tensor(tokens_train['attention_mask'])\n","<ipython-input-14-40b617136a1e>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  val_seq = torch.tensor(tokens_val['input_ids'])\n","<ipython-input-14-40b617136a1e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  val_mask = torch.tensor(tokens_val['attention_mask'])\n","<ipython-input-14-40b617136a1e>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  test_seq = torch.tensor(tokens_test['input_ids'])\n","<ipython-input-14-40b617136a1e>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  test_mask = torch.tensor(tokens_test['attention_mask'])\n"]}],"source":["train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())"]},{"cell_type":"markdown","metadata":{"id":"ZXkX49Tqoycb"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"w8Hy99lloycc","executionInfo":{"status":"ok","timestamp":1691494992261,"user_tz":-180,"elapsed":26,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"code","source":["train_dataloader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvLGc1cQ-m70","executionInfo":{"status":"ok","timestamp":1691494992262,"user_tz":-180,"elapsed":25,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"e460f61d-bd0c-4a97-9acb-109d5e35c4c7"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x790ae1d468f0>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"4hMHk5oLoycc"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"-koZtzhmoycc","executionInfo":{"status":"ok","timestamp":1691494992263,"user_tz":-180,"elapsed":19,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"FCwiSSrzoycd","executionInfo":{"status":"ok","timestamp":1691494992264,"user_tz":-180,"elapsed":20,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","\n","        self.bert = bert\n","\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,3)\n","\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","        #pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","\n","        x = self.fc1(cls_hs)\n","\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","\n","        # output layer\n","        x = self.fc2(x)\n","\n","        # apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"kjMTt-kyoyce","executionInfo":{"status":"ok","timestamp":1691494992890,"user_tz":-180,"elapsed":643,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"Okny-lvCoyci","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1691494994679,"user_tz":-180,"elapsed":1804,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"88d4ccf7-559b-413c-a9c2-3b347c1245a2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"bK6YObxNoycj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691494994680,"user_tz":-180,"elapsed":21,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"519b8ec2-dd2c-4bae-84f0-38eb23d26775"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 2]\n","Class Weights: [1. 1. 1.]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","print(np.unique(train_labels))\n","#compute the class weights\n","class_weights = compute_class_weight(class_weight = \"balanced\", classes = np.unique(train_labels), y = train_labels)\n","\n","print(\"Class Weights:\",class_weights)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"l9YlgUvroyck","executionInfo":{"status":"ok","timestamp":1691494994680,"user_tz":-180,"elapsed":15,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["\n","# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","# push to GPU\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 500"]},{"cell_type":"markdown","metadata":{"id":"oJpzOQKWoycn"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"FxFl3-Cooycp","executionInfo":{"status":"ok","timestamp":1691494994681,"user_tz":-180,"elapsed":14,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# function to train the model\n","def train():\n","\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save model predictions\n","    total_preds=[]\n","\n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss = total_loss + loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss / len(train_dataloader)\n","\n","      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","      # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ftIuiIc6oyc2","executionInfo":{"status":"ok","timestamp":1691494994681,"user_tz":-180,"elapsed":13,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step,batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds,labels)\n","\n","            total_loss = total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss / len(tokens_val)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wShkxn5Eoyc3","outputId":"bbb99dd2-fc49-4cb8-886a-eb382c20bcad","executionInfo":{"status":"ok","timestamp":1691516946056,"user_tz":-180,"elapsed":8600895,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 1 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.095\n","Validation Loss: 2.887\n","\n"," Epoch 2 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.089\n","Validation Loss: 2.868\n","\n"," Epoch 3 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.082\n","Validation Loss: 2.842\n","\n"," Epoch 4 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.076\n","Validation Loss: 2.821\n","\n"," Epoch 5 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.070\n","Validation Loss: 2.803\n","\n"," Epoch 6 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.067\n","Validation Loss: 2.785\n","\n"," Epoch 7 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.060\n","Validation Loss: 2.763\n","\n"," Epoch 8 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.055\n","Validation Loss: 2.748\n","\n"," Epoch 9 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.051\n","Validation Loss: 2.730\n","\n"," Epoch 10 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.049\n","Validation Loss: 2.714\n","\n"," Epoch 11 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.043\n","Validation Loss: 2.701\n","\n"," Epoch 12 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.043\n","Validation Loss: 2.680\n","\n"," Epoch 13 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.034\n","Validation Loss: 2.665\n","\n"," Epoch 14 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.027\n","Validation Loss: 2.655\n","\n"," Epoch 15 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.024\n","Validation Loss: 2.644\n","\n"," Epoch 16 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.020\n","Validation Loss: 2.629\n","\n"," Epoch 17 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.024\n","Validation Loss: 2.620\n","\n"," Epoch 18 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.016\n","Validation Loss: 2.607\n","\n"," Epoch 19 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.013\n","Validation Loss: 2.594\n","\n"," Epoch 20 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.001\n","Validation Loss: 2.583\n","\n"," Epoch 21 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.009\n","Validation Loss: 2.571\n","\n"," Epoch 22 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.998\n","Validation Loss: 2.563\n","\n"," Epoch 23 / 500\n","\n","Evaluating...\n","\n","Training Loss: 1.004\n","Validation Loss: 2.554\n","\n"," Epoch 24 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.995\n","Validation Loss: 2.546\n","\n"," Epoch 25 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.994\n","Validation Loss: 2.539\n","\n"," Epoch 26 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.995\n","Validation Loss: 2.530\n","\n"," Epoch 27 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.995\n","Validation Loss: 2.523\n","\n"," Epoch 28 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.979\n","Validation Loss: 2.514\n","\n"," Epoch 29 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.978\n","Validation Loss: 2.503\n","\n"," Epoch 30 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.977\n","Validation Loss: 2.498\n","\n"," Epoch 31 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.974\n","Validation Loss: 2.490\n","\n"," Epoch 32 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.976\n","Validation Loss: 2.486\n","\n"," Epoch 33 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.972\n","Validation Loss: 2.482\n","\n"," Epoch 34 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.967\n","Validation Loss: 2.476\n","\n"," Epoch 35 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.960\n","Validation Loss: 2.472\n","\n"," Epoch 36 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.964\n","Validation Loss: 2.460\n","\n"," Epoch 37 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.966\n","Validation Loss: 2.452\n","\n"," Epoch 38 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.963\n","Validation Loss: 2.444\n","\n"," Epoch 39 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.969\n","Validation Loss: 2.440\n","\n"," Epoch 40 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.963\n","Validation Loss: 2.436\n","\n"," Epoch 41 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.964\n","Validation Loss: 2.432\n","\n"," Epoch 42 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.953\n","Validation Loss: 2.424\n","\n"," Epoch 43 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.953\n","Validation Loss: 2.422\n","\n"," Epoch 44 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.953\n","Validation Loss: 2.414\n","\n"," Epoch 45 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.952\n","Validation Loss: 2.404\n","\n"," Epoch 46 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.953\n","Validation Loss: 2.399\n","\n"," Epoch 47 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.953\n","Validation Loss: 2.396\n","\n"," Epoch 48 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.943\n","Validation Loss: 2.388\n","\n"," Epoch 49 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.945\n","Validation Loss: 2.385\n","\n"," Epoch 50 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.940\n","Validation Loss: 2.384\n","\n"," Epoch 51 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.950\n","Validation Loss: 2.389\n","\n"," Epoch 52 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.953\n","Validation Loss: 2.383\n","\n"," Epoch 53 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.953\n","Validation Loss: 2.380\n","\n"," Epoch 54 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.936\n","Validation Loss: 2.371\n","\n"," Epoch 55 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.955\n","Validation Loss: 2.371\n","\n"," Epoch 56 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.938\n","Validation Loss: 2.368\n","\n"," Epoch 57 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.948\n","Validation Loss: 2.362\n","\n"," Epoch 58 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.928\n","Validation Loss: 2.363\n","\n"," Epoch 59 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.935\n","Validation Loss: 2.365\n","\n"," Epoch 60 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.946\n","Validation Loss: 2.362\n","\n"," Epoch 61 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.938\n","Validation Loss: 2.358\n","\n"," Epoch 62 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.928\n","Validation Loss: 2.355\n","\n"," Epoch 63 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.934\n","Validation Loss: 2.355\n","\n"," Epoch 64 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.931\n","Validation Loss: 2.349\n","\n"," Epoch 65 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.941\n","Validation Loss: 2.347\n","\n"," Epoch 66 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.931\n","Validation Loss: 2.348\n","\n"," Epoch 67 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.937\n","Validation Loss: 2.346\n","\n"," Epoch 68 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.930\n","Validation Loss: 2.346\n","\n"," Epoch 69 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.931\n","Validation Loss: 2.341\n","\n"," Epoch 70 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.925\n","Validation Loss: 2.342\n","\n"," Epoch 71 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.910\n","Validation Loss: 2.344\n","\n"," Epoch 72 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.923\n","Validation Loss: 2.349\n","\n"," Epoch 73 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.931\n","Validation Loss: 2.348\n","\n"," Epoch 74 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.919\n","Validation Loss: 2.340\n","\n"," Epoch 75 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.923\n","Validation Loss: 2.335\n","\n"," Epoch 76 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.925\n","Validation Loss: 2.334\n","\n"," Epoch 77 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.915\n","Validation Loss: 2.331\n","\n"," Epoch 78 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.927\n","Validation Loss: 2.329\n","\n"," Epoch 79 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.930\n","Validation Loss: 2.323\n","\n"," Epoch 80 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.931\n","Validation Loss: 2.317\n","\n"," Epoch 81 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.917\n","Validation Loss: 2.317\n","\n"," Epoch 82 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.911\n","Validation Loss: 2.314\n","\n"," Epoch 83 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.917\n","Validation Loss: 2.310\n","\n"," Epoch 84 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.927\n","Validation Loss: 2.313\n","\n"," Epoch 85 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.918\n","Validation Loss: 2.314\n","\n"," Epoch 86 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.914\n","Validation Loss: 2.312\n","\n"," Epoch 87 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.917\n","Validation Loss: 2.314\n","\n"," Epoch 88 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.912\n","Validation Loss: 2.311\n","\n"," Epoch 89 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.919\n","Validation Loss: 2.312\n","\n"," Epoch 90 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.911\n","Validation Loss: 2.311\n","\n"," Epoch 91 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.916\n","Validation Loss: 2.313\n","\n"," Epoch 92 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.908\n","Validation Loss: 2.307\n","\n"," Epoch 93 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.897\n","Validation Loss: 2.306\n","\n"," Epoch 94 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.923\n","Validation Loss: 2.308\n","\n"," Epoch 95 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.895\n","Validation Loss: 2.298\n","\n"," Epoch 96 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.902\n","Validation Loss: 2.297\n","\n"," Epoch 97 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.903\n","Validation Loss: 2.293\n","\n"," Epoch 98 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.906\n","Validation Loss: 2.292\n","\n"," Epoch 99 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.907\n","Validation Loss: 2.289\n","\n"," Epoch 100 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.914\n","Validation Loss: 2.287\n","\n"," Epoch 101 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.915\n","Validation Loss: 2.282\n","\n"," Epoch 102 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.928\n","Validation Loss: 2.284\n","\n"," Epoch 103 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.894\n","Validation Loss: 2.287\n","\n"," Epoch 104 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.892\n","Validation Loss: 2.290\n","\n"," Epoch 105 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.894\n","Validation Loss: 2.288\n","\n"," Epoch 106 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.905\n","Validation Loss: 2.284\n","\n"," Epoch 107 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.904\n","Validation Loss: 2.285\n","\n"," Epoch 108 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.915\n","Validation Loss: 2.285\n","\n"," Epoch 109 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.912\n","Validation Loss: 2.282\n","\n"," Epoch 110 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.910\n","Validation Loss: 2.285\n","\n"," Epoch 111 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.900\n","Validation Loss: 2.281\n","\n"," Epoch 112 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.915\n","Validation Loss: 2.274\n","\n"," Epoch 113 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.898\n","Validation Loss: 2.273\n","\n"," Epoch 114 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.904\n","Validation Loss: 2.274\n","\n"," Epoch 115 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.901\n","Validation Loss: 2.274\n","\n"," Epoch 116 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.897\n","Validation Loss: 2.282\n","\n"," Epoch 117 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.886\n","Validation Loss: 2.284\n","\n"," Epoch 118 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.908\n","Validation Loss: 2.277\n","\n"," Epoch 119 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.907\n","Validation Loss: 2.275\n","\n"," Epoch 120 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.908\n","Validation Loss: 2.277\n","\n"," Epoch 121 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.907\n","Validation Loss: 2.270\n","\n"," Epoch 122 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.897\n","Validation Loss: 2.269\n","\n"," Epoch 123 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.915\n","Validation Loss: 2.270\n","\n"," Epoch 124 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.894\n","Validation Loss: 2.272\n","\n"," Epoch 125 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.897\n","Validation Loss: 2.272\n","\n"," Epoch 126 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.916\n","Validation Loss: 2.270\n","\n"," Epoch 127 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.894\n","Validation Loss: 2.266\n","\n"," Epoch 128 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.900\n","Validation Loss: 2.266\n","\n"," Epoch 129 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.900\n","Validation Loss: 2.268\n","\n"," Epoch 130 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.877\n","Validation Loss: 2.266\n","\n"," Epoch 131 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.901\n","Validation Loss: 2.262\n","\n"," Epoch 132 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.902\n","Validation Loss: 2.261\n","\n"," Epoch 133 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.881\n","Validation Loss: 2.258\n","\n"," Epoch 134 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.920\n","Validation Loss: 2.266\n","\n"," Epoch 135 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.895\n","Validation Loss: 2.272\n","\n"," Epoch 136 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.880\n","Validation Loss: 2.268\n","\n"," Epoch 137 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.895\n","Validation Loss: 2.264\n","\n"," Epoch 138 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.901\n","Validation Loss: 2.262\n","\n"," Epoch 139 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.891\n","Validation Loss: 2.265\n","\n"," Epoch 140 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.881\n","Validation Loss: 2.266\n","\n"," Epoch 141 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.901\n","Validation Loss: 2.264\n","\n"," Epoch 142 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.901\n","Validation Loss: 2.271\n","\n"," Epoch 143 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.881\n","Validation Loss: 2.269\n","\n"," Epoch 144 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.887\n","Validation Loss: 2.267\n","\n"," Epoch 145 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.901\n","Validation Loss: 2.267\n","\n"," Epoch 146 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.883\n","Validation Loss: 2.270\n","\n"," Epoch 147 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.885\n","Validation Loss: 2.270\n","\n"," Epoch 148 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.894\n","Validation Loss: 2.267\n","\n"," Epoch 149 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.883\n","Validation Loss: 2.267\n","\n"," Epoch 150 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.882\n","Validation Loss: 2.261\n","\n"," Epoch 151 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.897\n","Validation Loss: 2.262\n","\n"," Epoch 152 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.892\n","Validation Loss: 2.266\n","\n"," Epoch 153 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.892\n","Validation Loss: 2.264\n","\n"," Epoch 154 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.889\n","Validation Loss: 2.265\n","\n"," Epoch 155 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.900\n","Validation Loss: 2.265\n","\n"," Epoch 156 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.894\n","Validation Loss: 2.262\n","\n"," Epoch 157 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.882\n","Validation Loss: 2.262\n","\n"," Epoch 158 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.884\n","Validation Loss: 2.262\n","\n"," Epoch 159 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.888\n","Validation Loss: 2.267\n","\n"," Epoch 160 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.872\n","Validation Loss: 2.268\n","\n"," Epoch 161 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.888\n","Validation Loss: 2.266\n","\n"," Epoch 162 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.887\n","Validation Loss: 2.263\n","\n"," Epoch 163 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.890\n","Validation Loss: 2.266\n","\n"," Epoch 164 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.886\n","Validation Loss: 2.264\n","\n"," Epoch 165 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.878\n","Validation Loss: 2.265\n","\n"," Epoch 166 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.887\n","Validation Loss: 2.261\n","\n"," Epoch 167 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.876\n","Validation Loss: 2.261\n","\n"," Epoch 168 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.879\n","Validation Loss: 2.259\n","\n"," Epoch 169 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.885\n","Validation Loss: 2.255\n","\n"," Epoch 170 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.886\n","Validation Loss: 2.248\n","\n"," Epoch 171 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.885\n","Validation Loss: 2.247\n","\n"," Epoch 172 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.893\n","Validation Loss: 2.247\n","\n"," Epoch 173 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.876\n","Validation Loss: 2.247\n","\n"," Epoch 174 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.873\n","Validation Loss: 2.249\n","\n"," Epoch 175 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.873\n","Validation Loss: 2.248\n","\n"," Epoch 176 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.869\n","Validation Loss: 2.246\n","\n"," Epoch 177 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.884\n","Validation Loss: 2.247\n","\n"," Epoch 178 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.875\n","Validation Loss: 2.247\n","\n"," Epoch 179 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.876\n","Validation Loss: 2.245\n","\n"," Epoch 180 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.884\n","Validation Loss: 2.241\n","\n"," Epoch 181 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.883\n","Validation Loss: 2.238\n","\n"," Epoch 182 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.893\n","Validation Loss: 2.239\n","\n"," Epoch 183 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.895\n","Validation Loss: 2.238\n","\n"," Epoch 184 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.854\n","Validation Loss: 2.244\n","\n"," Epoch 185 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.867\n","Validation Loss: 2.241\n","\n"," Epoch 186 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.877\n","Validation Loss: 2.244\n","\n"," Epoch 187 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.867\n","Validation Loss: 2.243\n","\n"," Epoch 188 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.863\n","Validation Loss: 2.244\n","\n"," Epoch 189 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.889\n","Validation Loss: 2.240\n","\n"," Epoch 190 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.884\n","Validation Loss: 2.237\n","\n"," Epoch 191 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.872\n","Validation Loss: 2.237\n","\n"," Epoch 192 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.858\n","Validation Loss: 2.233\n","\n"," Epoch 193 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.877\n","Validation Loss: 2.234\n","\n"," Epoch 194 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.875\n","Validation Loss: 2.232\n","\n"," Epoch 195 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.863\n","Validation Loss: 2.234\n","\n"," Epoch 196 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.896\n","Validation Loss: 2.237\n","\n"," Epoch 197 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.872\n","Validation Loss: 2.236\n","\n"," Epoch 198 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.880\n","Validation Loss: 2.234\n","\n"," Epoch 199 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.880\n","Validation Loss: 2.234\n","\n"," Epoch 200 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.875\n","Validation Loss: 2.233\n","\n"," Epoch 201 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.887\n","Validation Loss: 2.232\n","\n"," Epoch 202 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.870\n","Validation Loss: 2.234\n","\n"," Epoch 203 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.874\n","Validation Loss: 2.231\n","\n"," Epoch 204 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.898\n","Validation Loss: 2.231\n","\n"," Epoch 205 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.867\n","Validation Loss: 2.234\n","\n"," Epoch 206 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.233\n","\n"," Epoch 207 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.870\n","Validation Loss: 2.235\n","\n"," Epoch 208 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.869\n","Validation Loss: 2.234\n","\n"," Epoch 209 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.873\n","Validation Loss: 2.229\n","\n"," Epoch 210 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.882\n","Validation Loss: 2.233\n","\n"," Epoch 211 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.862\n","Validation Loss: 2.236\n","\n"," Epoch 212 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.885\n","Validation Loss: 2.232\n","\n"," Epoch 213 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.870\n","Validation Loss: 2.229\n","\n"," Epoch 214 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.880\n","Validation Loss: 2.230\n","\n"," Epoch 215 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.865\n","Validation Loss: 2.227\n","\n"," Epoch 216 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.881\n","Validation Loss: 2.224\n","\n"," Epoch 217 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.866\n","Validation Loss: 2.226\n","\n"," Epoch 218 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.878\n","Validation Loss: 2.227\n","\n"," Epoch 219 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.868\n","Validation Loss: 2.230\n","\n"," Epoch 220 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.880\n","Validation Loss: 2.233\n","\n"," Epoch 221 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.861\n","Validation Loss: 2.231\n","\n"," Epoch 222 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.867\n","Validation Loss: 2.230\n","\n"," Epoch 223 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.859\n","Validation Loss: 2.234\n","\n"," Epoch 224 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.233\n","\n"," Epoch 225 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.865\n","Validation Loss: 2.225\n","\n"," Epoch 226 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.874\n","Validation Loss: 2.225\n","\n"," Epoch 227 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.857\n","Validation Loss: 2.222\n","\n"," Epoch 228 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.876\n","Validation Loss: 2.219\n","\n"," Epoch 229 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.876\n","Validation Loss: 2.220\n","\n"," Epoch 230 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.886\n","Validation Loss: 2.219\n","\n"," Epoch 231 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.214\n","\n"," Epoch 232 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.865\n","Validation Loss: 2.213\n","\n"," Epoch 233 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.213\n","\n"," Epoch 234 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.877\n","Validation Loss: 2.213\n","\n"," Epoch 235 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.865\n","Validation Loss: 2.216\n","\n"," Epoch 236 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.877\n","Validation Loss: 2.219\n","\n"," Epoch 237 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.870\n","Validation Loss: 2.221\n","\n"," Epoch 238 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.223\n","\n"," Epoch 239 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.871\n","Validation Loss: 2.228\n","\n"," Epoch 240 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.880\n","Validation Loss: 2.225\n","\n"," Epoch 241 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.226\n","\n"," Epoch 242 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.224\n","\n"," Epoch 243 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.877\n","Validation Loss: 2.221\n","\n"," Epoch 244 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.862\n","Validation Loss: 2.221\n","\n"," Epoch 245 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.867\n","Validation Loss: 2.225\n","\n"," Epoch 246 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.874\n","Validation Loss: 2.226\n","\n"," Epoch 247 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.879\n","Validation Loss: 2.230\n","\n"," Epoch 248 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.861\n","Validation Loss: 2.231\n","\n"," Epoch 249 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.870\n","Validation Loss: 2.230\n","\n"," Epoch 250 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.856\n","Validation Loss: 2.226\n","\n"," Epoch 251 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.869\n","Validation Loss: 2.221\n","\n"," Epoch 252 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.875\n","Validation Loss: 2.224\n","\n"," Epoch 253 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.225\n","\n"," Epoch 254 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.869\n","Validation Loss: 2.218\n","\n"," Epoch 255 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.863\n","Validation Loss: 2.220\n","\n"," Epoch 256 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.869\n","Validation Loss: 2.217\n","\n"," Epoch 257 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.852\n","Validation Loss: 2.218\n","\n"," Epoch 258 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.857\n","Validation Loss: 2.217\n","\n"," Epoch 259 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.865\n","Validation Loss: 2.216\n","\n"," Epoch 260 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.868\n","Validation Loss: 2.214\n","\n"," Epoch 261 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.872\n","Validation Loss: 2.210\n","\n"," Epoch 262 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.212\n","\n"," Epoch 263 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.866\n","Validation Loss: 2.214\n","\n"," Epoch 264 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.210\n","\n"," Epoch 265 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.873\n","Validation Loss: 2.207\n","\n"," Epoch 266 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.846\n","Validation Loss: 2.208\n","\n"," Epoch 267 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.849\n","Validation Loss: 2.204\n","\n"," Epoch 268 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.877\n","Validation Loss: 2.209\n","\n"," Epoch 269 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.868\n","Validation Loss: 2.208\n","\n"," Epoch 270 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.205\n","\n"," Epoch 271 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.868\n","Validation Loss: 2.211\n","\n"," Epoch 272 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.867\n","Validation Loss: 2.213\n","\n"," Epoch 273 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.875\n","Validation Loss: 2.214\n","\n"," Epoch 274 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.214\n","\n"," Epoch 275 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.851\n","Validation Loss: 2.214\n","\n"," Epoch 276 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.214\n","\n"," Epoch 277 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.858\n","Validation Loss: 2.218\n","\n"," Epoch 278 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.871\n","Validation Loss: 2.217\n","\n"," Epoch 279 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.848\n","Validation Loss: 2.219\n","\n"," Epoch 280 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.873\n","Validation Loss: 2.217\n","\n"," Epoch 281 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.869\n","Validation Loss: 2.216\n","\n"," Epoch 282 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.861\n","Validation Loss: 2.217\n","\n"," Epoch 283 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.218\n","\n"," Epoch 284 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.224\n","\n"," Epoch 285 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.857\n","Validation Loss: 2.221\n","\n"," Epoch 286 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.223\n","\n"," Epoch 287 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.218\n","\n"," Epoch 288 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.840\n","Validation Loss: 2.218\n","\n"," Epoch 289 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.871\n","Validation Loss: 2.218\n","\n"," Epoch 290 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.865\n","Validation Loss: 2.219\n","\n"," Epoch 291 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.856\n","Validation Loss: 2.218\n","\n"," Epoch 292 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.842\n","Validation Loss: 2.211\n","\n"," Epoch 293 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.878\n","Validation Loss: 2.215\n","\n"," Epoch 294 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.856\n","Validation Loss: 2.217\n","\n"," Epoch 295 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.215\n","\n"," Epoch 296 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.859\n","Validation Loss: 2.215\n","\n"," Epoch 297 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.214\n","\n"," Epoch 298 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.868\n","Validation Loss: 2.209\n","\n"," Epoch 299 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.843\n","Validation Loss: 2.214\n","\n"," Epoch 300 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.210\n","\n"," Epoch 301 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.214\n","\n"," Epoch 302 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.864\n","Validation Loss: 2.210\n","\n"," Epoch 303 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.213\n","\n"," Epoch 304 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.862\n","Validation Loss: 2.215\n","\n"," Epoch 305 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.846\n","Validation Loss: 2.210\n","\n"," Epoch 306 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.858\n","Validation Loss: 2.210\n","\n"," Epoch 307 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.867\n","Validation Loss: 2.209\n","\n"," Epoch 308 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.858\n","Validation Loss: 2.214\n","\n"," Epoch 309 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.854\n","Validation Loss: 2.213\n","\n"," Epoch 310 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.851\n","Validation Loss: 2.207\n","\n"," Epoch 311 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.839\n","Validation Loss: 2.206\n","\n"," Epoch 312 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.846\n","Validation Loss: 2.212\n","\n"," Epoch 313 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.210\n","\n"," Epoch 314 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.851\n","Validation Loss: 2.207\n","\n"," Epoch 315 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.841\n","Validation Loss: 2.202\n","\n"," Epoch 316 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.852\n","Validation Loss: 2.201\n","\n"," Epoch 317 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.858\n","Validation Loss: 2.196\n","\n"," Epoch 318 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.853\n","Validation Loss: 2.193\n","\n"," Epoch 319 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.194\n","\n"," Epoch 320 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.860\n","Validation Loss: 2.199\n","\n"," Epoch 321 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.853\n","Validation Loss: 2.203\n","\n"," Epoch 322 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.842\n","Validation Loss: 2.204\n","\n"," Epoch 323 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.861\n","Validation Loss: 2.211\n","\n"," Epoch 324 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.852\n","Validation Loss: 2.207\n","\n"," Epoch 325 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.852\n","Validation Loss: 2.211\n","\n"," Epoch 326 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.845\n","Validation Loss: 2.212\n","\n"," Epoch 327 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.843\n","Validation Loss: 2.216\n","\n"," Epoch 328 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.220\n","\n"," Epoch 329 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.220\n","\n"," Epoch 330 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.856\n","Validation Loss: 2.221\n","\n"," Epoch 331 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.846\n","Validation Loss: 2.221\n","\n"," Epoch 332 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.845\n","Validation Loss: 2.215\n","\n"," Epoch 333 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.835\n","Validation Loss: 2.213\n","\n"," Epoch 334 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.860\n","Validation Loss: 2.213\n","\n"," Epoch 335 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.214\n","\n"," Epoch 336 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.856\n","Validation Loss: 2.209\n","\n"," Epoch 337 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.820\n","Validation Loss: 2.211\n","\n"," Epoch 338 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.858\n","Validation Loss: 2.214\n","\n"," Epoch 339 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.215\n","\n"," Epoch 340 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.851\n","Validation Loss: 2.214\n","\n"," Epoch 341 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.214\n","\n"," Epoch 342 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.214\n","\n"," Epoch 343 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.839\n","Validation Loss: 2.210\n","\n"," Epoch 344 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.863\n","Validation Loss: 2.206\n","\n"," Epoch 345 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.204\n","\n"," Epoch 346 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.853\n","Validation Loss: 2.205\n","\n"," Epoch 347 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.824\n","Validation Loss: 2.209\n","\n"," Epoch 348 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.840\n","Validation Loss: 2.209\n","\n"," Epoch 349 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.859\n","Validation Loss: 2.210\n","\n"," Epoch 350 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.211\n","\n"," Epoch 351 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.213\n","\n"," Epoch 352 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.211\n","\n"," Epoch 353 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.849\n","Validation Loss: 2.210\n","\n"," Epoch 354 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.833\n","Validation Loss: 2.207\n","\n"," Epoch 355 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.206\n","\n"," Epoch 356 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.858\n","Validation Loss: 2.206\n","\n"," Epoch 357 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.852\n","Validation Loss: 2.207\n","\n"," Epoch 358 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.212\n","\n"," Epoch 359 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.208\n","\n"," Epoch 360 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.206\n","\n"," Epoch 361 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.852\n","Validation Loss: 2.204\n","\n"," Epoch 362 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.828\n","Validation Loss: 2.202\n","\n"," Epoch 363 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.205\n","\n"," Epoch 364 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.202\n","\n"," Epoch 365 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.197\n","\n"," Epoch 366 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.197\n","\n"," Epoch 367 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.842\n","Validation Loss: 2.199\n","\n"," Epoch 368 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.198\n","\n"," Epoch 369 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.846\n","Validation Loss: 2.203\n","\n"," Epoch 370 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.204\n","\n"," Epoch 371 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.845\n","Validation Loss: 2.206\n","\n"," Epoch 372 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.834\n","Validation Loss: 2.210\n","\n"," Epoch 373 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.850\n","Validation Loss: 2.209\n","\n"," Epoch 374 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.846\n","Validation Loss: 2.209\n","\n"," Epoch 375 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.212\n","\n"," Epoch 376 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.835\n","Validation Loss: 2.208\n","\n"," Epoch 377 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.205\n","\n"," Epoch 378 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.206\n","\n"," Epoch 379 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.207\n","\n"," Epoch 380 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.835\n","Validation Loss: 2.203\n","\n"," Epoch 381 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.853\n","Validation Loss: 2.201\n","\n"," Epoch 382 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.841\n","Validation Loss: 2.201\n","\n"," Epoch 383 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.200\n","\n"," Epoch 384 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.205\n","\n"," Epoch 385 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.860\n","Validation Loss: 2.198\n","\n"," Epoch 386 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.843\n","Validation Loss: 2.198\n","\n"," Epoch 387 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.831\n","Validation Loss: 2.196\n","\n"," Epoch 388 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.834\n","Validation Loss: 2.194\n","\n"," Epoch 389 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.193\n","\n"," Epoch 390 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.196\n","\n"," Epoch 391 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.842\n","Validation Loss: 2.196\n","\n"," Epoch 392 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.196\n","\n"," Epoch 393 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.861\n","Validation Loss: 2.199\n","\n"," Epoch 394 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.843\n","Validation Loss: 2.198\n","\n"," Epoch 395 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.195\n","\n"," Epoch 396 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.190\n","\n"," Epoch 397 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.839\n","Validation Loss: 2.193\n","\n"," Epoch 398 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.193\n","\n"," Epoch 399 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.190\n","\n"," Epoch 400 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.822\n","Validation Loss: 2.188\n","\n"," Epoch 401 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.853\n","Validation Loss: 2.188\n","\n"," Epoch 402 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.833\n","Validation Loss: 2.184\n","\n"," Epoch 403 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.828\n","Validation Loss: 2.184\n","\n"," Epoch 404 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.824\n","Validation Loss: 2.181\n","\n"," Epoch 405 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.183\n","\n"," Epoch 406 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.181\n","\n"," Epoch 407 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.179\n","\n"," Epoch 408 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.825\n","Validation Loss: 2.179\n","\n"," Epoch 409 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.182\n","\n"," Epoch 410 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.833\n","Validation Loss: 2.186\n","\n"," Epoch 411 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.831\n","Validation Loss: 2.193\n","\n"," Epoch 412 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.851\n","Validation Loss: 2.191\n","\n"," Epoch 413 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.848\n","Validation Loss: 2.189\n","\n"," Epoch 414 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.831\n","Validation Loss: 2.190\n","\n"," Epoch 415 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.188\n","\n"," Epoch 416 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.840\n","Validation Loss: 2.190\n","\n"," Epoch 417 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.192\n","\n"," Epoch 418 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.835\n","Validation Loss: 2.194\n","\n"," Epoch 419 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.853\n","Validation Loss: 2.199\n","\n"," Epoch 420 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.840\n","Validation Loss: 2.202\n","\n"," Epoch 421 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.859\n","Validation Loss: 2.203\n","\n"," Epoch 422 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.833\n","Validation Loss: 2.204\n","\n"," Epoch 423 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.208\n","\n"," Epoch 424 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.848\n","Validation Loss: 2.205\n","\n"," Epoch 425 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.849\n","Validation Loss: 2.203\n","\n"," Epoch 426 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.826\n","Validation Loss: 2.196\n","\n"," Epoch 427 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.828\n","Validation Loss: 2.196\n","\n"," Epoch 428 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.839\n","Validation Loss: 2.198\n","\n"," Epoch 429 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.860\n","Validation Loss: 2.197\n","\n"," Epoch 430 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.831\n","Validation Loss: 2.194\n","\n"," Epoch 431 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.191\n","\n"," Epoch 432 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.848\n","Validation Loss: 2.194\n","\n"," Epoch 433 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.855\n","Validation Loss: 2.197\n","\n"," Epoch 434 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.848\n","Validation Loss: 2.202\n","\n"," Epoch 435 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.203\n","\n"," Epoch 436 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.840\n","Validation Loss: 2.204\n","\n"," Epoch 437 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.843\n","Validation Loss: 2.199\n","\n"," Epoch 438 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.840\n","Validation Loss: 2.201\n","\n"," Epoch 439 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.828\n","Validation Loss: 2.196\n","\n"," Epoch 440 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.822\n","Validation Loss: 2.195\n","\n"," Epoch 441 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.197\n","\n"," Epoch 442 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.205\n","\n"," Epoch 443 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.202\n","\n"," Epoch 444 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.824\n","Validation Loss: 2.199\n","\n"," Epoch 445 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.826\n","Validation Loss: 2.194\n","\n"," Epoch 446 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.844\n","Validation Loss: 2.197\n","\n"," Epoch 447 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.824\n","Validation Loss: 2.198\n","\n"," Epoch 448 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.845\n","Validation Loss: 2.201\n","\n"," Epoch 449 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.198\n","\n"," Epoch 450 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.200\n","\n"," Epoch 451 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.201\n","\n"," Epoch 452 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.201\n","\n"," Epoch 453 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.817\n","Validation Loss: 2.202\n","\n"," Epoch 454 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.833\n","Validation Loss: 2.201\n","\n"," Epoch 455 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.203\n","\n"," Epoch 456 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.845\n","Validation Loss: 2.200\n","\n"," Epoch 457 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.847\n","Validation Loss: 2.195\n","\n"," Epoch 458 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.813\n","Validation Loss: 2.198\n","\n"," Epoch 459 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.189\n","\n"," Epoch 460 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.826\n","Validation Loss: 2.189\n","\n"," Epoch 461 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.821\n","Validation Loss: 2.188\n","\n"," Epoch 462 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.828\n","Validation Loss: 2.188\n","\n"," Epoch 463 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.187\n","\n"," Epoch 464 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.185\n","\n"," Epoch 465 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.822\n","Validation Loss: 2.184\n","\n"," Epoch 466 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.853\n","Validation Loss: 2.186\n","\n"," Epoch 467 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.189\n","\n"," Epoch 468 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.816\n","Validation Loss: 2.190\n","\n"," Epoch 469 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.842\n","Validation Loss: 2.188\n","\n"," Epoch 470 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.831\n","Validation Loss: 2.188\n","\n"," Epoch 471 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.829\n","Validation Loss: 2.194\n","\n"," Epoch 472 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.836\n","Validation Loss: 2.190\n","\n"," Epoch 473 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.834\n","Validation Loss: 2.190\n","\n"," Epoch 474 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.825\n","Validation Loss: 2.195\n","\n"," Epoch 475 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.195\n","\n"," Epoch 476 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.194\n","\n"," Epoch 477 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.826\n","Validation Loss: 2.193\n","\n"," Epoch 478 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.861\n","Validation Loss: 2.193\n","\n"," Epoch 479 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.825\n","Validation Loss: 2.193\n","\n"," Epoch 480 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.824\n","Validation Loss: 2.196\n","\n"," Epoch 481 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.829\n","Validation Loss: 2.197\n","\n"," Epoch 482 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.835\n","Validation Loss: 2.194\n","\n"," Epoch 483 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.820\n","Validation Loss: 2.197\n","\n"," Epoch 484 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.198\n","\n"," Epoch 485 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.837\n","Validation Loss: 2.202\n","\n"," Epoch 486 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.819\n","Validation Loss: 2.207\n","\n"," Epoch 487 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.822\n","Validation Loss: 2.205\n","\n"," Epoch 488 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.820\n","Validation Loss: 2.206\n","\n"," Epoch 489 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.203\n","\n"," Epoch 490 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.198\n","\n"," Epoch 491 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.812\n","Validation Loss: 2.200\n","\n"," Epoch 492 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.835\n","Validation Loss: 2.200\n","\n"," Epoch 493 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.833\n","Validation Loss: 2.200\n","\n"," Epoch 494 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.838\n","Validation Loss: 2.193\n","\n"," Epoch 495 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.839\n","Validation Loss: 2.193\n","\n"," Epoch 496 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.813\n","Validation Loss: 2.193\n","\n"," Epoch 497 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.830\n","Validation Loss: 2.187\n","\n"," Epoch 498 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.832\n","Validation Loss: 2.184\n","\n"," Epoch 499 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.827\n","Validation Loss: 2.186\n","\n"," Epoch 500 / 500\n","\n","Evaluating...\n","\n","Training Loss: 0.829\n","Validation Loss: 2.185\n"]}],"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    #train model\n","    train_loss, _ = train()\n","\n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    #save the best model\n","    if epoch % 10 == 0 and valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), drive_path + 'saved_weights_3.pkl')\n","        with open(drive_path + 'train_losses_3.pkl', 'wb') as f:\n","            pickle.dump(train_losses, f)\n","        with open(drive_path + 'valid_losses_3.pkl', 'wb') as f:\n","            pickle.dump(valid_losses, f)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]},{"cell_type":"code","source":["with open(drive_path + 'train_losses_3.pkl', 'rb') as f:\n","        train_losses = pickle.load(f)\n","with open(drive_path + 'valid_losses_3.pkl', 'rb') as f:\n","        valid_losses = pickle.load(f)"],"metadata":{"id":"fRQjQuqC8VD9","executionInfo":{"status":"ok","timestamp":1691516946058,"user_tz":-180,"elapsed":8,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Plot train_losses and valid_losses against their indices\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(valid_losses, label='Validation Loss')\n","\n","last_train_loss = round(train_losses[-1], 4)\n","last_valid_loss = round(valid_losses[-1], 4)\n","\n","plt.text(len(train_losses), train_losses[-1], str(last_train_loss), ha='right', va='top')\n","plt.text(len(valid_losses), valid_losses[-1], str(last_valid_loss), ha='right', va='top')\n","\n","# Add labels and title\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Train and Validation Losses Until Last Saved Best Model')\n","\n","# Add a legend to differentiate train and validation losses\n","plt.legend()\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"ZvRu-Zbgwf6v","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1691517146202,"user_tz":-180,"elapsed":1205,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"88966e3c-8964-40e0-e402-804370c92d0d"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":29,"metadata":{"id":"gPo8M6Aooyc3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691517153903,"user_tz":-180,"elapsed":1022,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"55ba85e1-4fb3-4588-bd0b-bd216406047a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":29}],"source":["#load weights of best model\n","\n","#path = \"/content/drive/MyDrive/Virus-DNA-classification-BERT-main/old_code/data10/data3/saved_weights_3.pkl\"\n","path = drive_path + \"saved_weights_3.pkl\"\n","\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{"id":"SKmzY2R9oyc4"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">9. Make Predictions</h1>"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"hm9C8K4goyc4","executionInfo":{"status":"ok","timestamp":1691517179052,"user_tz":-180,"elapsed":807,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":522,"status":"ok","timestamp":1691517185341,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"},"user_tz":-180},"id":"00LG5cUioyc4","outputId":"720763ca-6607-4a28-e472-3a12a5530127"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.47      0.21      0.29        75\n","           1       0.62      0.69      0.65        75\n","           2       0.48      0.68      0.56        75\n","\n","    accuracy                           0.53       225\n","   macro avg       0.52      0.53      0.50       225\n","weighted avg       0.52      0.53      0.50       225\n","\n"]}],"source":["# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]},{"cell_type":"markdown","metadata":{"id":"KNtFXHd3oyc5"},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">REFERENCES & CREDITS</h1></center>"]},{"cell_type":"markdown","metadata":{"id":"VrTs1qhboyc5"},"source":["<ol>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.reddit.com/r/MachineLearning/comments/ao23cp/p_how_to_use_bert_in_kaggle_competitions_a/\">How to use BERT in Kaggle competitions - Reddit Thread</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\">A visual guide to using BERT by Jay Alammar</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\">Demystifying BERT: Groundbreaking NLP Framework by Mohd Sanad Zaki Rizvi</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\">BERT for Dummies step by step tutorial by Michel Kana</a></li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"id":"JzcrrWRToyc6"},"source":["<br>\n","<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">CONCLUSION</h1></center>"]},{"cell_type":"markdown","metadata":{"id":"KhNy7_-foyc6"},"source":["<p style=\"font-size:150%; font-family:verdana;\">BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it’s approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this Notebook we have discussed about BERT (Theoritical + Practical Part).</p>"]},{"cell_type":"markdown","metadata":{"id":"vstECEGroyc6"},"source":["<center><h1 style=\"font-size:200%; color:green;\">Please give this kernel an UPVOTE to show your appreciation, if you find it useful.</h1></center>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQVQxH2toyc7","executionInfo":{"status":"aborted","timestamp":1691516946068,"user_tz":-180,"elapsed":14,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}