{"cells":[{"cell_type":"code","source":["!wget http://launchpadlibrarian.net/367274644/libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb\n","!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/google-perftools_2.5-2.2ubuntu3_all.deb\n","!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\n","!wget https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\n","!apt install -qq libunwind8-dev\n","!dpkg -i *.deb\n","%env LD_PRELOAD=libtcmalloc.so"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBssqjCvuftm","executionInfo":{"status":"ok","timestamp":1691004985727,"user_tz":-180,"elapsed":9287,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"08d1bac1-26f5-4a1c-b9fa-05e9ecbef915"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-08-02 19:36:16--  http://launchpadlibrarian.net/367274644/libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb\n","Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.228, 185.125.189.229, 2620:2d:4000:1001::8008, ...\n","Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.228|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 203696 (199K) [application/x-debian-package]\n","Saving to: ‘libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb.1’\n","\n","libgoogle-perftools 100%[===================>] 198.92K   545KB/s    in 0.4s    \n","\n","2023-08-02 19:36:17 (545 KB/s) - ‘libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb.1’ saved [203696/203696]\n","\n","--2023-08-02 19:36:17--  https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/google-perftools_2.5-2.2ubuntu3_all.deb\n","Resolving launchpad.net (launchpad.net)... 185.125.189.222, 185.125.189.223, 2620:2d:4000:1001::8003, ...\n","Connecting to launchpad.net (launchpad.net)|185.125.189.222|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://launchpadlibrarian.net/367274642/google-perftools_2.5-2.2ubuntu3_all.deb [following]\n","--2023-08-02 19:36:17--  https://launchpadlibrarian.net/367274642/google-perftools_2.5-2.2ubuntu3_all.deb\n","Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.228, 185.125.189.229, 2620:2d:4000:1001::8008, ...\n","Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 294224 (287K) [application/x-debian-package]\n","Saving to: ‘google-perftools_2.5-2.2ubuntu3_all.deb.1’\n","\n","google-perftools_2. 100%[===================>] 287.33K   717KB/s    in 0.4s    \n","\n","2023-08-02 19:36:18 (717 KB/s) - ‘google-perftools_2.5-2.2ubuntu3_all.deb.1’ saved [294224/294224]\n","\n","--2023-08-02 19:36:18--  https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\n","Resolving launchpad.net (launchpad.net)... 185.125.189.222, 185.125.189.223, 2620:2d:4000:1001::8003, ...\n","Connecting to launchpad.net (launchpad.net)|185.125.189.222|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://launchpadlibrarian.net/367274648/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb [following]\n","--2023-08-02 19:36:19--  https://launchpadlibrarian.net/367274648/libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb\n","Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.228, 185.125.189.229, 2620:2d:4000:1001::8008, ...\n","Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 91644 (89K) [application/x-debian-package]\n","Saving to: ‘libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb.1’\n","\n","libtcmalloc-minimal 100%[===================>]  89.50K   428KB/s    in 0.2s    \n","\n","2023-08-02 19:36:19 (428 KB/s) - ‘libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb.1’ saved [91644/91644]\n","\n","--2023-08-02 19:36:19--  https://launchpad.net/ubuntu/+source/google-perftools/2.5-2.2ubuntu3/+build/14795286/+files/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\n","Resolving launchpad.net (launchpad.net)... 185.125.189.222, 185.125.189.223, 2620:2d:4000:1001::8003, ...\n","Connecting to launchpad.net (launchpad.net)|185.125.189.222|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://launchpadlibrarian.net/367274647/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb [following]\n","--2023-08-02 19:36:20--  https://launchpadlibrarian.net/367274647/libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb\n","Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 185.125.189.228, 185.125.189.229, 2620:2d:4000:1001::8008, ...\n","Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|185.125.189.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 189628 (185K) [application/x-debian-package]\n","Saving to: ‘libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb.1’\n","\n","libgoogle-perftools 100%[===================>] 185.18K   588KB/s    in 0.3s    \n","\n","2023-08-02 19:36:20 (588 KB/s) - ‘libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb.1’ saved [189628/189628]\n","\n","libunwind-dev is already the newest version (1.3.2-2build2).\n","0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n","(Reading database ... 120643 files and directories currently installed.)\n","Preparing to unpack google-perftools_2.5-2.2ubuntu3_all.deb ...\n","Unpacking google-perftools (2.5-2.2ubuntu3) over (2.5-2.2ubuntu3) ...\n","Preparing to unpack libgoogle-perftools4_2.5-2.2ubuntu3_amd64.deb ...\n","Unpacking libgoogle-perftools4 (2.5-2.2ubuntu3) over (2.5-2.2ubuntu3) ...\n","Preparing to unpack libgoogle-perftools-dev_2.5-2.2ubuntu3_amd64.deb ...\n","Unpacking libgoogle-perftools-dev (2.5-2.2ubuntu3) over (2.5-2.2ubuntu3) ...\n","Preparing to unpack libtcmalloc-minimal4_2.5-2.2ubuntu3_amd64.deb ...\n","Unpacking libtcmalloc-minimal4 (2.5-2.2ubuntu3) over (2.5-2.2ubuntu3) ...\n","Setting up libtcmalloc-minimal4 (2.5-2.2ubuntu3) ...\n","Setting up libgoogle-perftools4 (2.5-2.2ubuntu3) ...\n","Setting up libgoogle-perftools-dev (2.5-2.2ubuntu3) ...\n","Setting up google-perftools (2.5-2.2ubuntu3) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","env: LD_PRELOAD=libtcmalloc.so\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4untBtSoyiUG","executionInfo":{"status":"ok","timestamp":1691087035169,"user_tz":-180,"elapsed":18773,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"4386bee6-5333-4c4e-d522-a23964b9405b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["drive_path = '/content/drive/MyDrive/BERT/'"],"metadata":{"id":"kXO68GlPaMzn","executionInfo":{"status":"ok","timestamp":1691087054101,"user_tz":-180,"elapsed":279,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#import os\n","#assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"],"metadata":{"id":"bf5URj_yzsMg","executionInfo":{"status":"ok","timestamp":1690985096671,"user_tz":-180,"elapsed":411,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5MUEX9u_oycS"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">1. Import Required Libraries & Dataset</h1>"]},{"cell_type":"code","source":["#!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n","#!pip install cloud-tpu-client==0.10 torch==2.0.0  torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/cuda/117/torch_xla-2.0-cp39-cp39-linux_x86_64.whl --force-reinstall\n","\n","%pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"frpXm6--0Ha7","executionInfo":{"status":"ok","timestamp":1691004993689,"user_tz":-180,"elapsed":5078,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"072d2f76-0544-44b8-84db-7d5cba9b53ae"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8061,"status":"ok","timestamp":1691005001692,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"},"user_tz":-180},"id":"majG-QpMoycT"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from babel.dates import format_date, format_datetime, format_time\n","from datetime import date, datetime, time\n","import torch\n","import torch.nn as nn\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","# imports the torch_xla package\n","#import torch_xla\n","#import torch_xla.core.xla_model as xm\n"]},{"cell_type":"code","source":["print(torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-y0Ic5qm92o","executionInfo":{"status":"ok","timestamp":1691005002019,"user_tz":-180,"elapsed":17,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"169651a3-8b67-41dd-c4f1-3cd1603d7153"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["# setting device on GPU if available, else CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n","\n","# Creates a random tensor on xla:1 (a Cloud TPU core)\n","#device = xm.xla_device()\n","#t1 = torch.ones(3, 3, device = device)\n","#print(t1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIfNVL8H0LVP","executionInfo":{"status":"ok","timestamp":1691005002020,"user_tz":-180,"elapsed":14,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"df4d6cec-910b-4415-b298-f749f3e08f23"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","Tesla T4\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:416: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3acE_DOmCqZ","executionInfo":{"status":"ok","timestamp":1691005017818,"user_tz":-180,"elapsed":15806,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"cc0cda56-065e-4efd-bbb2-32675c5e999f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gputil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (4.6.0)\n"]}]},{"cell_type":"code","source":["printm()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkTJ-lqSmb_B","executionInfo":{"status":"ok","timestamp":1691005017819,"user_tz":-180,"elapsed":20,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"8861a0f4-a41c-4037-c68c-f5532795aaf4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Gen RAM Free: 25.4 GB  | Proc size: 578.3 MB\n","GPU RAM Free: 15098MB | Used: 3MB | Util   0% | Total 15360MB\n"]}]},{"cell_type":"markdown","metadata":{"id":"MSyTZ64qoycY"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">3. Import Bert - base- uncased</h1>"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"APqU7Db0oycY","executionInfo":{"status":"ok","timestamp":1691005021299,"user_tz":-180,"elapsed":3492,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# import BERT-base pretrained model\n","bert = AutoModel.from_pretrained('zhihan1996/DNA_bert_6')\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('zhihan1996/DNA_bert_6')"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"S3Z4SaoTvcsz","executionInfo":{"status":"ok","timestamp":1691005025129,"user_tz":-180,"elapsed":6,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["import pickle\n","\n","#drive_path = \"C:/Users/bromotdi/BERT/\"\n","# with open(drive_path + 'train_text.pkl', 'rb') as f:\n","#     train_text = pickle.load(f)\n","# with open(drive_path + 'temp_text.pkl', 'rb') as f:\n","#     temp_text = pickle.load(f)\n","with open(drive_path + 'train_labels.pkl', 'rb') as f:\n","    train_labels = pickle.load(f)\n","# with open(drive_path + 'temp_labels.pkl', 'rb') as f:\n","#     temp_labels = pickle.load(f)\n","\n","# with open(drive_path + 'val_text.pkl', 'rb') as f:\n","#     val_text = pickle.load(f)\n","# with open(drive_path + 'test_text.pkl', 'rb') as f:\n","#     test_text = pickle.load(f)\n","with open(drive_path + 'val_labels.pkl', 'rb') as f:\n","    val_labels = pickle.load(f)\n","with open(drive_path + 'test_labels.pkl', 'rb') as f:\n","    test_labels = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"9k5NRXxToycZ"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">4. Tokenize & Encode the Sequences</h1>"]},{"cell_type":"markdown","metadata":{"id":"SbLay3HDoycZ"},"source":["<u><h2 style=\"font-size:170%; font-family:cursive;\">Which Tokenization strategy is used by BERT?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the existing words in the vocabulary are iteratively added.</p>\n","<br>\n","<u><h2 style=\"font-size:170%; font-family:cursive;\">What is the maximum sequence length of the input?</h2></u>\n","\n","<p style=\"font-size:150%; font-family:verdana;\">The maximum sequence length of the input = 512</p>"]},{"cell_type":"markdown","metadata":{"id":"z-Eh8hwbxCkM"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31180,"status":"ok","timestamp":1690738242355,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"},"user_tz":-180},"id":"NMqGYrPmoyca","outputId":"8dcf04e9-beda-4c76-d4c7-7c58f629758c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# tokenize and encode sequences in the training set\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text.tolist(),\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDsN6kvEvt74"},"outputs":[],"source":["with open(drive_path + 'tokens_train.pkl', 'wb') as f:\n","    pickle.dump(tokens_train, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6624,"status":"ok","timestamp":1690739444894,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"},"user_tz":-180},"id":"ZHMPN2u_67CH","outputId":"85de620d-10f4-4174-c80a-a90d34c4b251"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text.tolist(),\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coECEKAI7H0H"},"outputs":[],"source":["with open(drive_path + 'tokens_val.pkl', 'wb') as f:\n","    pickle.dump(tokens_val, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6846,"status":"ok","timestamp":1690739494581,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"},"user_tz":-180},"id":"CBlwfCMr7OTE","outputId":"c1505d5f-5d69-4c83-bbaa-3e65c2a2f9d2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text.tolist(),\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yCYl3pt7Obp"},"outputs":[],"source":["with open(drive_path + 'tokens_test.pkl', 'wb') as f:\n","    pickle.dump(tokens_test, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKGCtBPL79o0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"CFCquUO47-C6"},"source":["# Load Tokens"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"DjNjbcWS7zXO","executionInfo":{"status":"ok","timestamp":1691005122086,"user_tz":-180,"elapsed":89389,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["with open(drive_path + 'tokens_train.pkl', 'rb') as f:\n","    tokens_train = pickle.load(f)\n","with open(drive_path + 'tokens_val.pkl', 'rb') as f:\n","    tokens_val = pickle.load(f)\n","#with open(drive_path + 'tokens_test.pkl', 'rb') as f:\n","#    tokens_test = pickle.load(f)\n"]},{"cell_type":"markdown","metadata":{"id":"_l-5xmWgoycb"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">5. List to Tensors</h1>"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"QDp1WJ-woycb","executionInfo":{"status":"ok","timestamp":1691005122089,"user_tz":-180,"elapsed":52,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels.tolist())\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels.tolist())\n","\n","#test_seq = torch.tensor(tokens_test['input_ids'])\n","#test_mask = torch.tensor(tokens_test['attention_mask'])\n","#test_y = torch.tensor(test_labels.tolist())"]},{"cell_type":"markdown","source":["Clear token variables as they are no longer needed."],"metadata":{"id":"LE6H6ONHOUgR"}},{"cell_type":"code","source":["del tokens_train, tokens_val #, tokens_test"],"metadata":{"id":"jL_7ILkkOZzi","executionInfo":{"status":"ok","timestamp":1691005122091,"user_tz":-180,"elapsed":52,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZXkX49Tqoycb"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">6. Data Loader</h1>"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"w8Hy99lloycc","executionInfo":{"status":"ok","timestamp":1691005141302,"user_tz":-180,"elapsed":307,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"4hMHk5oLoycc"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">7. Model Architecture</h1>"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-koZtzhmoycc","executionInfo":{"status":"ok","timestamp":1691005148541,"user_tz":-180,"elapsed":359,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# freeze all the parameters\n","for param in bert.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"FCwiSSrzoycd","executionInfo":{"status":"ok","timestamp":1691005161196,"user_tz":-180,"elapsed":362,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["class BERT_Arch(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(BERT_Arch, self).__init__()\n","\n","        self.bert = bert\n","\n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","\n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,10)\n","\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","        #pass the inputs to the model\n","        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n","\n","        x = self.fc1(cls_hs)\n","\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","\n","        # output layer\n","        x = self.fc2(x)\n","\n","        # apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"kjMTt-kyoyce","executionInfo":{"status":"ok","timestamp":1691005171299,"user_tz":-180,"elapsed":6251,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# pass the pre-trained BERT to our define architecture\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","model = model.to(device)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6749,"status":"ok","timestamp":1691005178004,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"},"user_tz":-180},"id":"Okny-lvCoyci","outputId":"66cf6695-721e-417f-a3f4-9cffaac28f2f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# optimizer from hugging face transformers\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),lr = 1e-5)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1073,"status":"ok","timestamp":1691005179062,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"},"user_tz":-180},"id":"bK6YObxNoycj","outputId":"5bde7f4f-b2e7-4f33-9682-0bfdfa9716d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 2 3 4 5 6 7 8 9]\n","Class Weights: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"]}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","print(np.unique(train_labels))\n","#compute the class weights\n","class_weights = compute_class_weight(class_weight = \"balanced\", classes = np.unique(train_labels), y = train_labels)\n","\n","print(\"Class Weights:\",class_weights)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"l9YlgUvroyck","executionInfo":{"status":"ok","timestamp":1691005184461,"user_tz":-180,"elapsed":2806,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# converting list of class weights to a tensor\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","# push to GPU\n","weights = weights.to(device)\n","\n","# define the loss function\n","cross_entropy  = nn.NLLLoss(weight=weights)\n","\n","# number of training epochs\n","epochs = 10000"]},{"cell_type":"markdown","metadata":{"id":"oJpzOQKWoycn"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">8. Fine - Tune</h1>"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"FxFl3-Cooycp","executionInfo":{"status":"ok","timestamp":1691005251276,"user_tz":-180,"elapsed":320,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# function to train the model\n","def train():\n","\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save model predictions\n","    total_preds=[]\n","\n","    # iterate over batches\n","    for step,batch in enumerate(train_dataloader):\n","\n","        # progress update after every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [r.to(device) for r in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # clear previously calculated gradients\n","        model.zero_grad()\n","\n","        # get model predictions for the current batch\n","        preds = model(sent_id, mask)\n","\n","        # compute the loss between actual and predicted values\n","        loss = cross_entropy(preds, labels)\n","\n","        # add on to the total loss\n","        total_loss = total_loss + loss.detach()#total_loss + loss.item()\n","\n","        # backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # update parameters\n","        optimizer.step()\n","\n","        # model predictions are stored on GPU. So, push it to CPU\n","        preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","    # compute the training loss of the epoch\n","    avg_loss = total_loss.item() / len(train_dataloader) #total_loss / len(train_dataloader)\n","\n","      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","      # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    #returns the loss and predictions\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"ftIuiIc6oyc2","executionInfo":{"status":"ok","timestamp":1691005257090,"user_tz":-180,"elapsed":331,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[],"source":["# function for evaluating the model\n","def evaluate():\n","\n","    print(\"\\nEvaluating...\")\n","\n","    # deactivate dropout layers\n","    model.eval()\n","\n","    total_loss, total_accuracy = 0, 0\n","\n","    # empty list to save the model predictions\n","    total_preds = []\n","\n","    # iterate over batches\n","    for step,batch in enumerate(val_dataloader):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","\n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","        # push the batch to gpu\n","        batch = [t.to(device) for t in batch]\n","\n","        sent_id, mask, labels = batch\n","\n","        # deactivate autograd\n","        with torch.no_grad():\n","\n","            # model predictions\n","            preds = model(sent_id, mask)\n","\n","            # compute the validation loss between actual and predicted values\n","            loss = cross_entropy(preds,labels)\n","\n","            total_loss = total_loss + loss.detach()#total_loss + loss.item()\n","\n","            preds = preds.detach().cpu().numpy()\n","\n","            total_preds.append(preds)\n","\n","    # compute the validation loss of the epoch\n","    avg_loss = total_loss.item() / len(val_dataloader) #total_loss / len(val_dataloader)\n","\n","    # reshape the predictions in form of (number of samples, no. of classes)\n","    total_preds  = np.concatenate(total_preds, axis=0)\n","\n","    return avg_loss, total_preds"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wShkxn5Eoyc3","outputId":"88e477c0-23df-4a62-cb2a-167d6baad159","executionInfo":{"status":"ok","timestamp":1691066460547,"user_tz":-180,"elapsed":61100780,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.151\n","\n"," Epoch 9446 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9447 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.149\n","\n"," Epoch 9448 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.148\n","\n"," Epoch 9449 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.149\n","\n"," Epoch 9450 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.149\n","\n"," Epoch 9451 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.151\n","\n"," Epoch 9452 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.150\n","\n"," Epoch 9453 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.151\n","\n"," Epoch 9454 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.150\n","\n"," Epoch 9455 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9456 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.150\n","\n"," Epoch 9457 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.150\n","\n"," Epoch 9458 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9459 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.151\n","\n"," Epoch 9460 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9461 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9462 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.150\n","\n"," Epoch 9463 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.151\n","\n"," Epoch 9464 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.150\n","\n"," Epoch 9465 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.150\n","\n"," Epoch 9466 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9467 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.151\n","\n"," Epoch 9468 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.151\n","\n"," Epoch 9469 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.151\n","\n"," Epoch 9470 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.973\n","Validation Loss: 2.152\n","\n"," Epoch 9471 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.152\n","\n"," Epoch 9472 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.151\n","\n"," Epoch 9473 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.153\n","\n"," Epoch 9474 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.152\n","\n"," Epoch 9475 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.999\n","Validation Loss: 2.152\n","\n"," Epoch 9476 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.153\n","\n"," Epoch 9477 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.153\n","\n"," Epoch 9478 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.153\n","\n"," Epoch 9479 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.152\n","\n"," Epoch 9480 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.152\n","\n"," Epoch 9481 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.152\n","\n"," Epoch 9482 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.151\n","\n"," Epoch 9483 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.151\n","\n"," Epoch 9484 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9485 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9486 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.149\n","\n"," Epoch 9487 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.149\n","\n"," Epoch 9488 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.974\n","Validation Loss: 2.149\n","\n"," Epoch 9489 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.992\n","Validation Loss: 2.149\n","\n"," Epoch 9490 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.999\n","Validation Loss: 2.149\n","\n"," Epoch 9491 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.149\n","\n"," Epoch 9492 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.150\n","\n"," Epoch 9493 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.151\n","\n"," Epoch 9494 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.151\n","\n"," Epoch 9495 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.999\n","Validation Loss: 2.151\n","\n"," Epoch 9496 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.150\n","\n"," Epoch 9497 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.149\n","\n"," Epoch 9498 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.149\n","\n"," Epoch 9499 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.147\n","\n"," Epoch 9500 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.147\n","\n"," Epoch 9501 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.147\n","\n"," Epoch 9502 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.147\n","\n"," Epoch 9503 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9504 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.147\n","\n"," Epoch 9505 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.148\n","\n"," Epoch 9506 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.147\n","\n"," Epoch 9507 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9508 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.148\n","\n"," Epoch 9509 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9510 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.149\n","\n"," Epoch 9511 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9512 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9513 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9514 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9515 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9516 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9517 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9518 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.149\n","\n"," Epoch 9519 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.149\n","\n"," Epoch 9520 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.150\n","\n"," Epoch 9521 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.150\n","\n"," Epoch 9522 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.150\n","\n"," Epoch 9523 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.150\n","\n"," Epoch 9524 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.149\n","\n"," Epoch 9525 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.150\n","\n"," Epoch 9526 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9527 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9528 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9529 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.148\n","\n"," Epoch 9530 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.147\n","\n"," Epoch 9531 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9532 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.992\n","Validation Loss: 2.147\n","\n"," Epoch 9533 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.146\n","\n"," Epoch 9534 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.147\n","\n"," Epoch 9535 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.147\n","\n"," Epoch 9536 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.147\n","\n"," Epoch 9537 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.146\n","\n"," Epoch 9538 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.148\n","\n"," Epoch 9539 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.147\n","\n"," Epoch 9540 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.147\n","\n"," Epoch 9541 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.147\n","\n"," Epoch 9542 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.147\n","\n"," Epoch 9543 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9544 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.148\n","\n"," Epoch 9545 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.997\n","Validation Loss: 2.147\n","\n"," Epoch 9546 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9547 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9548 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9549 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.148\n","\n"," Epoch 9550 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.149\n","\n"," Epoch 9551 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9552 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.149\n","\n"," Epoch 9553 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9554 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.994\n","Validation Loss: 2.150\n","\n"," Epoch 9555 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9556 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9557 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9558 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9559 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.994\n","Validation Loss: 2.149\n","\n"," Epoch 9560 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.149\n","\n"," Epoch 9561 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9562 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.149\n","\n"," Epoch 9563 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9564 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.148\n","\n"," Epoch 9565 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9566 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9567 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9568 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.150\n","\n"," Epoch 9569 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.149\n","\n"," Epoch 9570 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9571 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.149\n","\n"," Epoch 9572 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.149\n","\n"," Epoch 9573 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.992\n","Validation Loss: 2.147\n","\n"," Epoch 9574 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.992\n","Validation Loss: 2.147\n","\n"," Epoch 9575 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.147\n","\n"," Epoch 9576 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9577 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9578 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.976\n","Validation Loss: 2.148\n","\n"," Epoch 9579 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9580 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9581 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.146\n","\n"," Epoch 9582 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9583 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9584 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9585 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9586 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9587 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.150\n","\n"," Epoch 9588 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.972\n","Validation Loss: 2.149\n","\n"," Epoch 9589 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.150\n","\n"," Epoch 9590 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.149\n","\n"," Epoch 9591 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9592 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9593 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.147\n","\n"," Epoch 9594 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9595 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9596 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.148\n","\n"," Epoch 9597 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9598 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9599 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.149\n","\n"," Epoch 9600 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.148\n","\n"," Epoch 9601 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.149\n","\n"," Epoch 9602 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9603 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.149\n","\n"," Epoch 9604 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.149\n","\n"," Epoch 9605 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.150\n","\n"," Epoch 9606 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9607 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9608 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.149\n","\n"," Epoch 9609 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.150\n","\n"," Epoch 9610 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.149\n","\n"," Epoch 9611 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.992\n","Validation Loss: 2.149\n","\n"," Epoch 9612 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.149\n","\n"," Epoch 9613 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.147\n","\n"," Epoch 9614 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.148\n","\n"," Epoch 9615 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9616 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9617 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.147\n","\n"," Epoch 9618 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9619 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9620 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.146\n","\n"," Epoch 9621 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.147\n","\n"," Epoch 9622 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.146\n","\n"," Epoch 9623 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.146\n","\n"," Epoch 9624 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9625 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.148\n","\n"," Epoch 9626 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.147\n","\n"," Epoch 9627 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.147\n","\n"," Epoch 9628 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.148\n","\n"," Epoch 9629 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.148\n","\n"," Epoch 9630 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.148\n","\n"," Epoch 9631 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.147\n","\n"," Epoch 9632 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.147\n","\n"," Epoch 9633 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.148\n","\n"," Epoch 9634 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9635 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.148\n","\n"," Epoch 9636 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.148\n","\n"," Epoch 9637 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9638 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.146\n","\n"," Epoch 9639 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9640 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.147\n","\n"," Epoch 9641 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.148\n","\n"," Epoch 9642 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.149\n","\n"," Epoch 9643 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.149\n","\n"," Epoch 9644 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.149\n","\n"," Epoch 9645 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.149\n","\n"," Epoch 9646 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9647 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.147\n","\n"," Epoch 9648 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.147\n","\n"," Epoch 9649 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.147\n","\n"," Epoch 9650 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.147\n","\n"," Epoch 9651 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9652 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.148\n","\n"," Epoch 9653 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9654 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9655 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.147\n","\n"," Epoch 9656 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9657 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.148\n","\n"," Epoch 9658 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.148\n","\n"," Epoch 9659 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.148\n","\n"," Epoch 9660 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.147\n","\n"," Epoch 9661 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.147\n","\n"," Epoch 9662 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9663 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.147\n","\n"," Epoch 9664 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9665 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9666 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.146\n","\n"," Epoch 9667 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.146\n","\n"," Epoch 9668 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.146\n","\n"," Epoch 9669 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.146\n","\n"," Epoch 9670 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.146\n","\n"," Epoch 9671 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.146\n","\n"," Epoch 9672 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.145\n","\n"," Epoch 9673 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.145\n","\n"," Epoch 9674 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.146\n","\n"," Epoch 9675 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.147\n","\n"," Epoch 9676 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.145\n","\n"," Epoch 9677 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.145\n","\n"," Epoch 9678 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.145\n","\n"," Epoch 9679 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.145\n","\n"," Epoch 9680 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.145\n","\n"," Epoch 9681 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.147\n","\n"," Epoch 9682 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9683 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9684 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.147\n","\n"," Epoch 9685 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.147\n","\n"," Epoch 9686 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.147\n","\n"," Epoch 9687 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 2.000\n","Validation Loss: 2.147\n","\n"," Epoch 9688 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9689 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.148\n","\n"," Epoch 9690 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9691 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.148\n","\n"," Epoch 9692 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9693 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.149\n","\n"," Epoch 9694 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.149\n","\n"," Epoch 9695 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.148\n","\n"," Epoch 9696 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.146\n","\n"," Epoch 9697 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.146\n","\n"," Epoch 9698 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.147\n","\n"," Epoch 9699 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.148\n","\n"," Epoch 9700 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9701 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9702 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9703 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.149\n","\n"," Epoch 9704 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.150\n","\n"," Epoch 9705 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9706 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.149\n","\n"," Epoch 9707 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.994\n","Validation Loss: 2.149\n","\n"," Epoch 9708 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.148\n","\n"," Epoch 9709 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.149\n","\n"," Epoch 9710 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9711 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.148\n","\n"," Epoch 9712 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9713 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.149\n","\n"," Epoch 9714 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.148\n","\n"," Epoch 9715 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.148\n","\n"," Epoch 9716 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.148\n","\n"," Epoch 9717 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.976\n","Validation Loss: 2.148\n","\n"," Epoch 9718 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9719 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9720 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.147\n","\n"," Epoch 9721 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.147\n","\n"," Epoch 9722 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9723 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.974\n","Validation Loss: 2.146\n","\n"," Epoch 9724 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9725 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.146\n","\n"," Epoch 9726 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.147\n","\n"," Epoch 9727 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9728 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.146\n","\n"," Epoch 9729 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.147\n","\n"," Epoch 9730 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9731 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.146\n","\n"," Epoch 9732 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.146\n","\n"," Epoch 9733 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9734 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.145\n","\n"," Epoch 9735 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.145\n","\n"," Epoch 9736 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.145\n","\n"," Epoch 9737 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.145\n","\n"," Epoch 9738 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.145\n","\n"," Epoch 9739 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.146\n","\n"," Epoch 9740 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.147\n","\n"," Epoch 9741 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.147\n","\n"," Epoch 9742 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9743 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.148\n","\n"," Epoch 9744 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.148\n","\n"," Epoch 9745 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.148\n","\n"," Epoch 9746 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.996\n","Validation Loss: 2.148\n","\n"," Epoch 9747 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.148\n","\n"," Epoch 9748 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.147\n","\n"," Epoch 9749 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.146\n","\n"," Epoch 9750 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.147\n","\n"," Epoch 9751 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.148\n","\n"," Epoch 9752 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9753 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.148\n","\n"," Epoch 9754 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9755 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9756 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.147\n","\n"," Epoch 9757 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.147\n","\n"," Epoch 9758 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.148\n","\n"," Epoch 9759 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9760 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.992\n","Validation Loss: 2.147\n","\n"," Epoch 9761 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.146\n","\n"," Epoch 9762 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.146\n","\n"," Epoch 9763 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.146\n","\n"," Epoch 9764 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.145\n","\n"," Epoch 9765 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.145\n","\n"," Epoch 9766 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.146\n","\n"," Epoch 9767 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.976\n","Validation Loss: 2.145\n","\n"," Epoch 9768 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.146\n","\n"," Epoch 9769 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.145\n","\n"," Epoch 9770 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.146\n","\n"," Epoch 9771 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.147\n","\n"," Epoch 9772 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.147\n","\n"," Epoch 9773 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.146\n","\n"," Epoch 9774 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.146\n","\n"," Epoch 9775 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.146\n","\n"," Epoch 9776 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.147\n","\n"," Epoch 9777 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.149\n","\n"," Epoch 9778 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.971\n","Validation Loss: 2.150\n","\n"," Epoch 9779 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.150\n","\n"," Epoch 9780 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.150\n","\n"," Epoch 9781 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.149\n","\n"," Epoch 9782 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.149\n","\n"," Epoch 9783 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.149\n","\n"," Epoch 9784 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.995\n","Validation Loss: 2.148\n","\n"," Epoch 9785 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9786 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.148\n","\n"," Epoch 9787 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.148\n","\n"," Epoch 9788 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.148\n","\n"," Epoch 9789 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.146\n","\n"," Epoch 9790 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.145\n","\n"," Epoch 9791 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.146\n","\n"," Epoch 9792 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.146\n","\n"," Epoch 9793 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.146\n","\n"," Epoch 9794 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.144\n","\n"," Epoch 9795 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.976\n","Validation Loss: 2.144\n","\n"," Epoch 9796 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.144\n","\n"," Epoch 9797 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.145\n","\n"," Epoch 9798 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.145\n","\n"," Epoch 9799 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.147\n","\n"," Epoch 9800 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.973\n","Validation Loss: 2.146\n","\n"," Epoch 9801 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.146\n","\n"," Epoch 9802 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.146\n","\n"," Epoch 9803 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.148\n","\n"," Epoch 9804 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.147\n","\n"," Epoch 9805 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9806 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.147\n","\n"," Epoch 9807 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.147\n","\n"," Epoch 9808 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.149\n","\n"," Epoch 9809 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9810 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.148\n","\n"," Epoch 9811 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.149\n","\n"," Epoch 9812 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.150\n","\n"," Epoch 9813 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9814 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9815 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.149\n","\n"," Epoch 9816 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.150\n","\n"," Epoch 9817 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9818 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.150\n","\n"," Epoch 9819 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9820 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9821 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.150\n","\n"," Epoch 9822 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9823 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.147\n","\n"," Epoch 9824 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.149\n","\n"," Epoch 9825 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.150\n","\n"," Epoch 9826 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9827 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.150\n","\n"," Epoch 9828 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.150\n","\n"," Epoch 9829 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9830 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.149\n","\n"," Epoch 9831 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.151\n","\n"," Epoch 9832 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9833 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.149\n","\n"," Epoch 9834 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9835 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9836 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.994\n","Validation Loss: 2.151\n","\n"," Epoch 9837 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.153\n","\n"," Epoch 9838 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.154\n","\n"," Epoch 9839 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.154\n","\n"," Epoch 9840 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.154\n","\n"," Epoch 9841 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.153\n","\n"," Epoch 9842 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.153\n","\n"," Epoch 9843 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.153\n","\n"," Epoch 9844 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.154\n","\n"," Epoch 9845 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.155\n","\n"," Epoch 9846 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.155\n","\n"," Epoch 9847 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.154\n","\n"," Epoch 9848 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.152\n","\n"," Epoch 9849 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.152\n","\n"," Epoch 9850 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.151\n","\n"," Epoch 9851 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.151\n","\n"," Epoch 9852 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.150\n","\n"," Epoch 9853 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9854 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.149\n","\n"," Epoch 9855 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.150\n","\n"," Epoch 9856 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9857 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9858 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9859 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9860 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.148\n","\n"," Epoch 9861 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.148\n","\n"," Epoch 9862 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9863 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.149\n","\n"," Epoch 9864 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.149\n","\n"," Epoch 9865 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9866 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9867 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.150\n","\n"," Epoch 9868 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9869 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9870 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.151\n","\n"," Epoch 9871 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.150\n","\n"," Epoch 9872 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9873 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9874 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9875 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.150\n","\n"," Epoch 9876 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.150\n","\n"," Epoch 9877 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.995\n","Validation Loss: 2.150\n","\n"," Epoch 9878 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.976\n","Validation Loss: 2.150\n","\n"," Epoch 9879 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9880 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.149\n","\n"," Epoch 9881 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9882 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9883 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.149\n","\n"," Epoch 9884 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9885 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.147\n","\n"," Epoch 9886 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.148\n","\n"," Epoch 9887 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.149\n","\n"," Epoch 9888 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9889 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.149\n","\n"," Epoch 9890 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.149\n","\n"," Epoch 9891 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9892 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.149\n","\n"," Epoch 9893 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9894 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.148\n","\n"," Epoch 9895 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.149\n","\n"," Epoch 9896 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9897 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.148\n","\n"," Epoch 9898 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.149\n","\n"," Epoch 9899 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.972\n","Validation Loss: 2.149\n","\n"," Epoch 9900 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.149\n","\n"," Epoch 9901 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.149\n","\n"," Epoch 9902 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9903 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.149\n","\n"," Epoch 9904 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.150\n","\n"," Epoch 9905 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.151\n","\n"," Epoch 9906 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9907 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.150\n","\n"," Epoch 9908 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.149\n","\n"," Epoch 9909 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.151\n","\n"," Epoch 9910 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.152\n","\n"," Epoch 9911 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.151\n","\n"," Epoch 9912 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.151\n","\n"," Epoch 9913 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.992\n","Validation Loss: 2.150\n","\n"," Epoch 9914 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.151\n","\n"," Epoch 9915 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.151\n","\n"," Epoch 9916 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9917 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.151\n","\n"," Epoch 9918 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9919 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.150\n","\n"," Epoch 9920 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.151\n","\n"," Epoch 9921 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.152\n","\n"," Epoch 9922 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.153\n","\n"," Epoch 9923 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.153\n","\n"," Epoch 9924 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.152\n","\n"," Epoch 9925 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.152\n","\n"," Epoch 9926 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.151\n","\n"," Epoch 9927 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.150\n","\n"," Epoch 9928 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.150\n","\n"," Epoch 9929 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.979\n","Validation Loss: 2.150\n","\n"," Epoch 9930 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.150\n","\n"," Epoch 9931 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.150\n","\n"," Epoch 9932 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.151\n","\n"," Epoch 9933 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.151\n","\n"," Epoch 9934 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.151\n","\n"," Epoch 9935 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.151\n","\n"," Epoch 9936 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.151\n","\n"," Epoch 9937 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.153\n","\n"," Epoch 9938 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.153\n","\n"," Epoch 9939 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.152\n","\n"," Epoch 9940 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.153\n","\n"," Epoch 9941 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.974\n","Validation Loss: 2.153\n","\n"," Epoch 9942 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.153\n","\n"," Epoch 9943 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.154\n","\n"," Epoch 9944 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.153\n","\n"," Epoch 9945 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.153\n","\n"," Epoch 9946 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.153\n","\n"," Epoch 9947 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.154\n","\n"," Epoch 9948 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.153\n","\n"," Epoch 9949 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.153\n","\n"," Epoch 9950 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.151\n","\n"," Epoch 9951 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.151\n","\n"," Epoch 9952 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.151\n","\n"," Epoch 9953 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.151\n","\n"," Epoch 9954 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.151\n","\n"," Epoch 9955 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.151\n","\n"," Epoch 9956 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9957 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.149\n","\n"," Epoch 9958 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.977\n","Validation Loss: 2.149\n","\n"," Epoch 9959 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.149\n","\n"," Epoch 9960 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.993\n","Validation Loss: 2.150\n","\n"," Epoch 9961 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.988\n","Validation Loss: 2.149\n","\n"," Epoch 9962 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9963 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.151\n","\n"," Epoch 9964 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.151\n","\n"," Epoch 9965 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.152\n","\n"," Epoch 9966 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.152\n","\n"," Epoch 9967 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.152\n","\n"," Epoch 9968 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.151\n","\n"," Epoch 9969 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.994\n","Validation Loss: 2.150\n","\n"," Epoch 9970 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.150\n","\n"," Epoch 9971 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.990\n","Validation Loss: 2.152\n","\n"," Epoch 9972 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.976\n","Validation Loss: 2.152\n","\n"," Epoch 9973 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.150\n","\n"," Epoch 9974 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.149\n","\n"," Epoch 9975 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.148\n","\n"," Epoch 9976 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.149\n","\n"," Epoch 9977 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.150\n","\n"," Epoch 9978 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.151\n","\n"," Epoch 9979 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.151\n","\n"," Epoch 9980 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.150\n","\n"," Epoch 9981 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.151\n","\n"," Epoch 9982 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.975\n","Validation Loss: 2.151\n","\n"," Epoch 9983 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.152\n","\n"," Epoch 9984 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.980\n","Validation Loss: 2.151\n","\n"," Epoch 9985 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.151\n","\n"," Epoch 9986 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.991\n","Validation Loss: 2.150\n","\n"," Epoch 9987 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.151\n","\n"," Epoch 9988 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.981\n","Validation Loss: 2.153\n","\n"," Epoch 9989 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.982\n","Validation Loss: 2.153\n","\n"," Epoch 9990 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.153\n","\n"," Epoch 9991 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.983\n","Validation Loss: 2.152\n","\n"," Epoch 9992 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.985\n","Validation Loss: 2.153\n","\n"," Epoch 9993 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.989\n","Validation Loss: 2.154\n","\n"," Epoch 9994 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.153\n","\n"," Epoch 9995 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.152\n","\n"," Epoch 9996 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.987\n","Validation Loss: 2.151\n","\n"," Epoch 9997 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.984\n","Validation Loss: 2.151\n","\n"," Epoch 9998 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.974\n","Validation Loss: 2.152\n","\n"," Epoch 9999 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.978\n","Validation Loss: 2.152\n","\n"," Epoch 10000 / 10000\n","  Batch    50  of    110.\n","  Batch   100  of    110.\n","\n","Evaluating...\n","\n","Training Loss: 1.986\n","Validation Loss: 2.151\n"]}],"source":["# set initial loss to infinite\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","\n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","\n","    #train model\n","    train_loss, _ = train()\n","\n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","\n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    #save the best model\n","    if epoch % 10 == 0 and valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), drive_path + 'saved_weights_10000_uncase.pkl')\n","        with open(drive_path + 'train_losses.pkl', 'wb') as f:\n","            pickle.dump(train_losses, f)\n","        with open(drive_path + 'valid_losses.pkl', 'wb') as f:\n","            pickle.dump(valid_losses, f)\n","\n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')"]},{"cell_type":"code","source":["del train_seq, train_mask, train_y\n","del val_seq, val_mask, val_y"],"metadata":{"id":"viNvIqHtvsgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del train_losses, valid_losses"],"metadata":{"id":"Z_lLieh7rVcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"3JARCAUNr4WG","executionInfo":{"status":"ok","timestamp":1691087798657,"user_tz":-180,"elapsed":480,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["t_load = False\n","v_load = False\n","\n","try:\n","    # Try to access train_losses\n","    if len(train_losses) > 0:\n","        # Do something with train_losses if it exists and is not empty\n","        print(\"train_losses is defined and has a length greater than 0.\")\n","    else:\n","        # Do something else when train_losses is defined but empty\n","        print(\"train_losses is defined, but it is empty.\")\n","except NameError:\n","    # Handle the case when train_losses is not defined\n","    train_losses = []\n","    t_load = True\n","\n","try:\n","    # Try to access valid_losses\n","    if len(valid_losses) > 0:\n","        # Do something with valid_losses if it exists and is not empty\n","        print(\"valid_losses is defined and has a length greater than 0.\")\n","    else:\n","        # Do something else when valid_losses is defined but empty\n","        print(\"valid_losses is defined, but it is empty.\")\n","except NameError:\n","    # Handle the case when valid_losses is not defined\n","    valid_losses = []\n","    v_load = True\n","\n","if t_load:\n","    with open(drive_path + 'train_losses.pkl', 'rb') as f:\n","        train_losses = pickle.load(f)\n","if v_load:\n","    with open(drive_path + 'valid_losses.pkl', 'rb') as f:\n","        valid_losses = pickle.load(f)"],"metadata":{"id":"3OicaRTPowG1","executionInfo":{"status":"ok","timestamp":1691087802650,"user_tz":-180,"elapsed":286,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"p5DN-JsNo9nt","executionInfo":{"status":"ok","timestamp":1691088156579,"user_tz":-180,"elapsed":6,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Plot train_losses and valid_losses against their indices\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(valid_losses, label='Validation Loss')\n","\n","last_train_loss = round(train_losses[-1], 4)\n","last_valid_loss = round(valid_losses[-1], 4)\n","\n","plt.text(len(train_losses), train_losses[-1], str(last_train_loss), ha='right', va='top')\n","plt.text(len(valid_losses), valid_losses[-1], str(last_valid_loss), ha='right', va='top')\n","\n","# Add labels and title\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Train and Validation Losses Until Last Saved Best Model')\n","\n","# Add a legend to differentiate train and validation losses\n","plt.legend()\n","\n","# Show the plot\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"FDj3msAYnbxA","executionInfo":{"status":"ok","timestamp":1691088610169,"user_tz":-180,"elapsed":397,"user":{"displayName":"Michelle Aluf-Medina","userId":"18255522415074256355"}},"outputId":"66659e65-5a33-45ca-dbf1-90a0bb48d4b8"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACgQklEQVR4nOzdd3gUxRvA8e+l94RACoFAQuhd6b1FAiJdmihdFAOK/uwoVQ1NRUVAUUFQEFARpYUaeu+9dwg9Cenl9vfHcpdccpeeXCDv53nycLs7uzt3uePezLwzo1EURUEIIYQQohixMHcFhBBCCCEKmwRAQgghhCh2JAASQgghRLEjAZAQQgghih0JgIQQQghR7EgAJIQQQohiRwIgIYQQQhQ7EgAJIYQQotiRAEgIIYQQxY4EQE+BQYMG4efnZ+5q5Err1q1p3bp1od/X2Gum0WgYP358lueOHz8ejUaTr/UJCwtDo9EQFhaWr9cVRUde3nOiYDzJ/3cWJXn5/2v+/PloNBouX76c7/XKigRABUij0WTrR770TDt48CAajYZPPvnEZJlz586h0Wh45513CrFmuTNr1izmz59v7moYaN26NTVr1jR3NQqVRqNh5MiRRo/9+eefuf5c3rx5k/Hjx3P48OG8VTCNy5cvo9FomD59er5dMzOxsbGMHz8+R8//8uXLDB48mICAAOzs7PD29qZly5aMGzeu4CpaiFq3bm3wf7aNjQ3+/v4MHz6ca9euFdh9d+7cyfjx44mIiMhW+UGDBqHRaHBxcSEuLi7Dcd3/lYX5firKrMxdgafZwoULDbYXLFjA+vXrM+yvVq1anu4zd+5ctFptnq5RVD377LNUrVqVxYsX89lnnxkts2jRIgBefvnlPN0rLi4OK6uC/UjMmjWLUqVKMWjQIIP9LVu2JC4uDhsbmwK9vyhYN2/eZMKECfj5+VG3bl2DY0/K5zQ2NpYJEyYAZKt19vz58zRo0AB7e3uGDBmCn58ft27d4uDBg0yZMkV/rSdd2bJlCQkJASAxMZGTJ08yZ84cQkNDOXXqFA4ODvl+z507dzJhwgQGDRqEm5tbts6xsrIiNjaW//77j969exsc+/3337GzsyM+Pj7f6/okkgCoAKX/Qt69ezfr16/P8os6NjY2Rx8ma2vrXNXvSdG/f38+/fRTdu/eTePGjTMcX7x4MVWrVuXZZ5/N033s7OzydH5eWFhYmPX+ouA9rZ/Tr7/+mujoaA4fPkz58uUNjt25c8dMtcp/rq6uGf7v9vf3Z+TIkezYsYPnnnvOTDUzZGtrS7NmzVi8eHGGAGjRokV06tSJv/76y0y1K1qkC8zMdN0PBw4coGXLljg4OPDxxx8DsGLFCjp16oSPjw+2trYEBAQwadIkUlJSDK6Rvh87bZP5jz/+SEBAALa2tjRo0IB9+/ZlWacHDx7w7rvvUqtWLZycnHBxcaFjx44cOXLEoJyu33fp0qV8/vnnlC1bFjs7O9q1a8f58+czXFdXF3t7exo2bMi2bduy9Rr1798fSG3pSevAgQOcOXNGXya7r5kxxvIxtm/fToMGDbCzsyMgIIAffvjB6Lnz5s2jbdu2eHp6YmtrS/Xq1Zk9e7ZBGT8/P06cOMGWLVv0zdC6v7BN9aEvW7aMevXqYW9vT6lSpXj55Ze5ceOGQZlBgwbh5OTEjRs36NatG05OTnh4ePDuu+9m63ln16xZs6hRowa2trb4+PgQHBycoWn+3Llz9OzZE29vb+zs7Chbtix9+/YlMjJSX2b9+vU0b94cNzc3nJycqFKliv49r5OQkMC4ceOoWLEitra2+Pr68v7775OQkGBQLjvXyg+6z+nJkydp06YNDg4OlClThqlTp+rLhIWF0aBBAwAGDx6s/x3rujwLOt8kO+9BgP379xMUFESpUqWwt7fH39+fIUOGAOr/HR4eHgBMmDBB/xwyy1O6cOECZcuWzRD8AHh6ehpsZ+fzOXLkSJycnIiNjc1wvX79+uHt7W1Qfs2aNbRo0QJHR0ecnZ3p1KkTJ06cyHDuP//8Q82aNbGzs6NmzZosX77c5HPKLm9vb4AMLcc3btxgyJAheHl5YWtrS40aNfjll18ynP/dd99Ro0YNHBwcKFGiBPXr19f/Pzd+/Hjee+89QA20dL+L7OTKvPTSS6xZs8bg87lv3z7OnTvHSy+9ZPScixcv0qtXL9zd3XFwcKBx48asWrUqQ7nr16/TrVs3HB0d8fT05O23387wudTZs2cPHTp0wNXVFQcHB1q1asWOHTuyrH9hkRagIuD+/ft07NiRvn378vLLL+Pl5QWoyWFOTk688847ODk5sWnTJsaOHUtUVBTTpk3L8rqLFi3i0aNHvPbaa2g0GqZOnUqPHj24ePFipn+NXrx4kX/++YdevXrh7+/P7du3+eGHH2jVqhUnT57Ex8fHoPzkyZOxsLDg3XffJTIykqlTp9K/f3/27NmjL/Pzzz/z2muv0bRpU0aPHs3Fixfp0qUL7u7u+Pr6Zvo8/P39adq0KUuXLuXrr7/G0tLS4DkC+g91Xl+ztI4dO0b79u3x8PBg/PjxJCcnM27cOP3vJ63Zs2dTo0YNunTpgpWVFf/99x9vvPEGWq2W4OBgAGbMmMGoUaNwcnJizJgxAEavpTN//nwGDx5MgwYNCAkJ4fbt23zzzTfs2LGDQ4cOGTSJp6SkEBQURKNGjZg+fTobNmzgyy+/JCAggBEjRuToeRszfvx4JkyYQGBgICNGjODMmTPMnj2bffv2sWPHDqytrUlMTCQoKIiEhARGjRqFt7c3N27cYOXKlURERODq6sqJEyd44YUXqF27NhMnTsTW1pbz588b/Keo1Wrp0qUL27dvZ/jw4VSrVo1jx47x9ddfc/bsWf755x+AbF0rPz18+JAOHTrQo0cPevfuzZ9//skHH3xArVq16NixI9WqVWPixImMHTuW4cOH06JFCwCaNm1aIPVJLzvvwTt37ujf0x9++CFubm5cvnyZv//+GwAPDw9mz57NiBEj6N69Oz169ACgdu3aJu9bvnx5NmzYwKZNm2jbtm2mdczO57NPnz58//33rFq1il69eunP1XXrDBo0SP9/wMKFCxk4cCBBQUFMmTKF2NhYZs+eTfPmzTl06JA+4Fy3bh09e/akevXqhISEcP/+fQYPHkzZsmWz/fqmpKRw7949AJKSkjh16pQ+SG/WrJm+3O3bt2ncuLE+z8zDw4M1a9YwdOhQoqKiGD16NKB2ib755pu8+OKLvPXWW8THx3P06FH27NnDSy+9RI8ePTh79iyLFy/m66+/plSpUvrfUVZ69OjB66+/zt9//60PbhctWmSypfz27ds0bdqU2NhY3nzzTUqWLMmvv/5Kly5d+PPPP+nevTugpgm0a9eOq1ev8uabb+Lj48PChQvZtGlThmtu2rSJjh07Uq9ePcaNG4eFhYU+SN+2bRsNGzbM9mtfYBRRaIKDg5X0L3mrVq0UQJkzZ06G8rGxsRn2vfbaa4qDg4MSHx+v3zdw4EClfPny+u1Lly4pgFKyZEnlwYMH+v0rVqxQAOW///7LtJ7x8fFKSkqKwb5Lly4ptra2ysSJE/X7Nm/erABKtWrVlISEBP3+b775RgGUY8eOKYqiKImJiYqnp6dSt25dg3I//vijAiitWrXKtD6Koijff/+9AiihoaH6fSkpKUqZMmWUJk2a6Pfl9jVTFEUBlHHjxum3u3XrptjZ2SlXrlzR7zt58qRiaWmZ4fdo7L5BQUFKhQoVDPbVqFHD6PPVvZabN29WFCX1NatZs6YSFxenL7dy5UoFUMaOHWvwXACD342iKMozzzyj1KtXL8O90mvVqpVSo0YNk8fv3Lmj2NjYKO3btzd4X8ycOVMBlF9++UVRFEU5dOiQAijLli0zea2vv/5aAZS7d++aLLNw4ULFwsJC2bZtm8H+OXPmKICyY8eObF/LFEAJDg42emzZsmUGvwtFSf2cLliwQL8vISFB8fb2Vnr27Knft2/fPgVQ5s2bl+G62XnPGaP7PE+bNi3Tctl5Dy5fvlwBlH379pm8zt27d7NVL53jx48r9vb2CqDUrVtXeeutt5R//vlHiYmJyVYd038+tVqtUqZMGYPXVVEUZenSpQqgbN26VVEURXn06JHi5uamvPrqqwblwsPDFVdXV4P9devWVUqXLq1ERETo961bt04BMvxOjNH9/tP/VKtWTbl48aJB2aFDhyqlS5dW7t27Z7C/b9++iqurq/416Nq1a6afO0VRlGnTpimAcunSpSzrqCjqe8zR0VFRFEV58cUXlXbt2imKov5f6e3trUyYMMHo+2n06NEKYPCZe/TokeLv76/4+fnpP/czZsxQAGXp0qX6cjExMUrFihUNPjNarVapVKmSEhQUpGi1Wn3Z2NhYxd/fX3nuuef0++bNm5ej55ifpAusCLC1tWXw4MEZ9tvb2+sfP3r0iHv37tGiRQtiY2M5ffp0ltft06cPJUqU0G/r/iK9ePFilvWxsFDfGikpKdy/f1/fvXDw4MEM5QcPHmyQvJv+Pvv37+fOnTu8/vrrBuUGDRqEq6trls9D91ysra0NusG2bNnCjRs39N1fkPfXTCclJYXQ0FC6detGuXLl9PurVatGUFBQhvJp7xsZGcm9e/do1aoVFy9eNOj+yS7da/bGG28Y5AZ16tSJqlWrGm2afv311w22W7RokeXvOjs2bNhAYmIio0eP1r8vAF599VVcXFz0ddH9LkNDQ412XwD6VqsVK1aYTAhetmwZ1apVo2rVqty7d0//o2td2Lx5c7avlZ+cnJwMckBsbGxo2LBhvrzG+SE770Hda7Zy5UqSkpLy5b41atTg8OHDvPzyy1y+fJlvvvmGbt264eXlxdy5c03W0dTnU6PR0KtXL1avXk10dLS+/JIlSyhTpgzNmzcH1O7PiIgI+vXrZ/A+sbS0pFGjRvr3ya1btzh8+DADBw40+P/mueeeo3r16tl+nn5+fqxfv57169ezZs0aZsyYQWRkJB07duTu3bsAKIrCX3/9RefOnVEUxaBeQUFBREZG6v8PdXNz4/r169lKS8iNl156ibCwMMLDw9m0aRPh4eEmu79Wr15Nw4YN9a8tqO/34cOHc/nyZU6ePKkvV7p0aV588UV9OQcHB4YPH25wvcOHD+u72+7fv69/DWJiYmjXrh1bt24tEgMCJAAqAsqUKWN09M+JEyfo3r07rq6uuLi44OHhof8PODtfqmm/uAF9MPTw4cNMz9NqtXz99ddUqlQJW1tbSpUqhYeHB0ePHjV636zuc+XKFQAqVapkUM7a2poKFSpk+TwASpYsSVBQEMuXL9ePYFi0aBFWVlYGiX55fc107t69S1xcXIY6A1SpUiXDvh07dhAYGIijoyNubm54eHjoc1FyEwDpXjNj96patar+uI6dnV2GpvESJUpk+bvOS11sbGyoUKGC/ri/vz/vvPMOP/30E6VKlSIoKIjvv//e4Pn36dOHZs2aMWzYMLy8vOjbty9Lly41+M/w3LlznDhxAg8PD4OfypUrA6mJtdm5Vl6kn+upbNmyGfbl12ucH7LzHmzVqhU9e/ZkwoQJlCpViq5duzJv3jyTORzZVblyZRYuXMi9e/c4evQoX3zxBVZWVgwfPpwNGzboy2X389mnTx/i4uL4999/AYiOjmb16tX06tVL/zs4d+4cAG3bts3wXlm3bp3+fWLq/x8w/vkyxdHRkcDAQAIDA+nQoQNvvfUW//77L2fOnGHy5MmA+v9GREQEP/74Y4Y66f7I1dXrgw8+wMnJiYYNG1KpUiWCg4Pztfv2+eefx9nZmSVLlvD777/ToEEDKlasaLTslStXjL4WuhHKutfwypUrVKxYMcPnIP25ut/NwIEDM7wOP/30EwkJCbn6fzG/SQ5QEZD2ryKdiIgIWrVqhYuLCxMnTtTPr3Hw4EE++OCDbP0nnzZXJi1FUTI974svvuDTTz9lyJAhTJo0CXd3dywsLBg9erTR++b2Pjn18ssvs3LlSlauXEmXLl3466+/9PkMkD+vWW5cuHCBdu3aUbVqVb766it8fX2xsbFh9erVfP3114Xyl46p30Fh+/LLLxk0aBArVqxg3bp1vPnmm4SEhLB7927Kli2Lvb09W7duZfPmzaxatYq1a9eyZMkS2rZty7p167C0tESr1VKrVi2++uoro/fQ5Yxl51qm2NraGp0nBdC3XqUflVdY7/PcyO57UKPR8Oeff7J7927+++8/QkNDGTJkCF9++SW7d+/GyckpT/WwtLSkVq1a1KpViyZNmtCmTRt+//13AgMDc/T5bNy4MX5+fixdupSXXnqJ//77j7i4OPr06aMvoyu/cOFCfTJyWgU9pQVAvXr1cHV1ZevWrQZ1evnllxk4cKDRc3T5VNWqVePMmTOsXLmStWvX8tdffzFr1izGjh2bL1MH2Nra0qNHD3799VcuXrxYqBNu6l6HadOmZZgOQiev77X8IAFQERUWFsb9+/f5+++/admypX7/pUuXCvzef/75J23atOHnn3822B8REaFPxMsJ3eiQc+fOGSRJJiUlcenSJerUqZOt63Tp0gVnZ2cWLVqEtbU1Dx8+NOj+ys/XzMPDA3t7e/1fMmmdOXPGYPu///4jISGBf//916A1TNcEn1Z2Z5DWvWZnzpzJkFh65swZoyNuCkrauqRtsUtMTOTSpUsEBgYalNd9AX7yySfs3LmTZs2aMWfOHP08ThYWFrRr14527drx1Vdf8cUXXzBmzBg2b95MYGAgAQEBHDlyhHbt2mX5emV1rcyeU/rfo45uf25e4/yeITy7cvIeBDXAaNy4MZ9//jmLFi2if//+/PHHHwwbNizfnkP9+vUBtQsKcv757N27N9988w1RUVEsWbIEPz8/g2kwAgICAHWkWVa/ayBbn+XcSElJ0XfVeXh44OzsTEpKSqZ10nF0dKRPnz706dOHxMREevToweeff85HH32EnZ1dnn8XL730Er/88gsWFhb07dvXZDlTnwddt6TuNSxfvjzHjx9HURSDuqU/V/e7cXFxydbrYC7SBVZE6f7aTPvXZWJiIrNmzSqUe6f/q3bZsmUZhl9nV/369fHw8GDOnDkkJibq98+fPz/bM5yC+hd/9+7dWb16NbNnz8bR0ZGuXbsa1Bvy5zWztLQkKCiIf/75h6tXr+r3nzp1itDQ0Axl0983MjKSefPmZbiuo6Njtp5z/fr18fT0ZM6cOQbdE2vWrOHUqVN06tQpp08p1wIDA7GxseHbb781eI4///wzkZGR+rpERUWRnJxscG6tWrWwsLDQP4cHDx5kuL7uL0Rdmd69e3Pjxo0M+SOgjkKJiYnJ9rVMef7559m9ezcHDhww2B8REcHvv/9O3bp1jbYqZMXR0VF/ncKU3ffgw4cPM3y2079mujnIsvsctm3bZjSfaPXq1UBq90hOP599+vQhISGBX3/9lbVr12aY0yYoKAgXFxe++OILo/fX5eWULl2aunXr8uuvv2aYjkGX25JbmzdvJjo6Wv9HnKWlJT179uSvv/7i+PHjJusE6ujftGxsbKhevTqKouifT17fT23atGHSpEnMnDkz0/fz888/z969e9m1a5d+X0xMDD/++CN+fn76XKnnn3+emzdv8ueff+rLxcbG8uOPPxpcr169egQEBDB9+nSDPC6dtK+DOUkLUBHVtGlTSpQowcCBA3nzzTfRaDQsXLiwUJrbX3jhBSZOnMjgwYNp2rQpx44d4/fff892vk561tbWfPbZZ7z22mu0bduWPn36cOnSJebNm5fja7788sssWLCA0NBQ+vfvr/8PAvL/NZswYQJr166lRYsWvPHGGyQnJ+vn7Th69Ki+XPv27bGxsaFz58689tprREdHM3fuXDw9PfV//erUq1eP2bNn89lnn1GxYkU8PT2NDh22trZmypQpDB48mFatWtGvXz/9MHg/Pz/efvvtXD0nU+7evWt0pm1/f3/69+/PRx99xIQJE+jQoQNdunThzJkzzJo1iwYNGuhzODZt2sTIkSPp1asXlStXJjk5mYULF+q/FAAmTpzI1q1b6dSpE+XLl+fOnTvMmjWLsmXL6hMwX3nlFZYuXcrrr7/O5s2badasGSkpKZw+fZqlS5cSGhpK/fr1s3UtUz788EOWLVtGy5Ytee2116hatSo3b95k/vz53Lp1y2jwmh0BAQG4ubkxZ84cnJ2dcXR0pFGjRvj7++fqemlt3LjR6Ay+3bp1y/Z78Ndff2XWrFl0796dgIAAHj16xNy5c3FxceH5558H1D80qlevzpIlS6hcuTLu7u7UrFnT5HIpU6ZM4cCBA/To0UPfvXPw4EEWLFiAu7u7fth3Tj+fzz77LBUrVmTMmDEkJCQYdH+B2rowe/ZsXnnlFZ599ln69u2Lh4cHV69eZdWqVTRr1oyZM2cCEBISQqdOnWjevDlDhgzhwYMH+s+ysS9oYyIjI/ntt98ASE5O1k8FYW9vz4cffqgvN3nyZDZv3kyjRo149dVXqV69Og8ePODgwYNs2LBBH7i3b98eb29vmjVrhpeXF6dOnWLmzJl06tQJZ2dnQP3/AmDMmDH07dsXa2trOnfubPD/XmYsLCwyXUZI58MPP2Tx4sV07NiRN998E3d3d3799VcuXbrEX3/9pR/88OqrrzJz5kwGDBjAgQMHKF26NAsXLswwca+FhQU//fQTHTt2pEaNGgwePJgyZcpw48YNNm/ejIuLC//991+2nkOBKuxhZ8WZqWHwpoZC7tixQ2ncuLFib2+v+Pj4KO+//74SGhqaYYiuqWHwxobNko3hrfHx8cr//vc/pXTp0oq9vb3SrFkzZdeuXUqrVq0MhnDrhm6nH/asu3/6ocCzZs1S/P39FVtbW6V+/frK1q1bM1wzK8nJyUrp0qUVQFm9enWG47l9zRTF+GuzZcsWpV69eoqNjY1SoUIFZc6cOcq4ceMy/B7//fdfpXbt2oqdnZ3i5+enTJkyRfnll18yDO8MDw9XOnXqpDg7OxtMAZB+GLzOkiVLlGeeeUaxtbVV3N3dlf79+yvXr183KJN26GtaxuppjKkhvoB+GK2iqMPeq1atqlhbWyteXl7KiBEjlIcPH+qPX7x4URkyZIgSEBCg2NnZKe7u7kqbNm2UDRs26Mts3LhR6dq1q+Lj46PY2NgoPj4+Sr9+/ZSzZ88a1CkxMVGZMmWKUqNGDcXW1lYpUaKEUq9ePWXChAlKZGRkjq5lyvXr15Vhw4YpZcqUUaysrBR3d3flhRdeUHbv3m30NTL2OTX2PlqxYoVSvXp1xcrKyuBzkNdh8KZ+Fi5cqChK9t6DBw8eVPr166eUK1dOsbW1VTw9PZUXXnhB2b9/v8E9d+7cqX/fZ1XHHTt2KMHBwUrNmjUVV1dXxdraWilXrpwyaNAg5cKFCxnKZufzqTNmzBgFUCpWrGjy/ps3b1aCgoIUV1dXxc7OTgkICFAGDRqU4Tn99ddfSrVq1RRbW1ulevXqyt9//230d2JM+s+IRqNR3N3dlS5duigHDhzIUP727dtKcHCw4uvrq1hbWyve3t5Ku3btlB9//FFf5ocfflBatmyplCxZUrG1tVUCAgKU9957T//+1pk0aZJSpkwZxcLCIsvh4qb+L0jL1PfDhQsXlBdffFFxc3NT7OzslIYNGyorV67McP6VK1eULl26KA4ODkqpUqWUt956S1m7dq3R3+GhQ4eUHj166J9j+fLlld69eysbN27UlzHnMHiNohSBDD4hhBBCiEIkOUBCCCGEKHYkABJCCCFEsSMBkBBCCCGKHQmAhBBCCFHsSAAkhBBCiGJHAiAhhBBCFDsyEaIRWq2Wmzdv4uzsbLap7YUQQgiRM4qi8OjRI3x8fPQTOJoiAZARN2/e1C+4KIQQQogny7Vr1yhbtmymZSQAMkI3Dfm1a9dwcXExc22EEEIIkR1RUVH4+vrqv8czIwGQEbpuLxcXFwmAhBBCiCdMdtJXJAlaCCGEEMWOBEBCCCGEKHYkABJCCCFEsSM5QEIIIfKdVqslMTHR3NUQTxlra2ssLS3z5VoSAAkhhMhXiYmJXLp0Ca1Wa+6qiKeQm5sb3t7eeZ6nTwIgIYQQ+UZRFG7duoWlpSW+vr5ZTkYnRHYpikJsbCx37twBoHTp0nm6ngRAQggh8k1ycjKxsbH4+Pjg4OBg7uqIp4y9vT0Ad+7cwdPTM0/dYRKaCyGEyDcpKSkA2NjYmLkm4mmlC6yTkpLydB0JgIQQQuQ7WUdRFJT8em9JACSEEEKIYkcCICGEEKIA+Pn5MWPGDHNXQ5ggAZAQQohiTaPRZPozfvz4XF133759DB8+PE91a926NaNHj87TNYRxMgqsED2KTyIyLgl7a0tKOtmauzpCCCGAW7du6R8vWbKEsWPHcubMGf0+Jycn/WNFUUhJScHKKuuvTw8Pj/ytqMhX0gJUiBbsukLzKZuZFnom68JCCCEKhbe3t/7H1dUVjUaj3z59+jTOzs6sWbOGevXqYWtry/bt27lw4QJdu3bFy8sLJycnGjRowIYNGwyum74LTKPR8NNPP9G9e3ccHByoVKkS//77b57q/tdff1GjRg1sbW3x8/Pjyy+/NDg+a9YsKlWqhJ2dHV5eXrz44ov6Y3/++Se1atXC3t6ekiVLEhgYSExMTJ7q8ySRFqBCZGmhZq4npShmrokQQhQORVGIS0oxy73trS3zbcTQhx9+yPTp06lQoQIlSpTg2rVrPP/883z++efY2tqyYMECOnfuzJkzZyhXrpzJ60yYMIGpU6cybdo0vvvuO/r378+VK1dwd3fPcZ0OHDhA7969GT9+PH369GHnzp288cYblCxZkkGDBrF//37efPNNFi5cSNOmTXnw4AHbtm0D1Favfv36MXXqVLp3786jR4/Ytm0bilJ8vp/MGgCFhITw999/c/r0aezt7WnatClTpkyhSpUqJs/5+++/+eKLLzh//jxJSUlUqlSJ//3vf7zyyiv6MoqiMG7cOObOnUtERATNmjVj9uzZVKpUqTCelklWjwOgFJkeXghRTMQlpVB9bKhZ7n1yYhAONvnzNTdx4kSee+45/ba7uzt16tTRb0+aNInly5fz77//MnLkSJPXGTRoEP369QPgiy++4Ntvv2Xv3r106NAhx3X66quvaNeuHZ9++ikAlStX5uTJk0ybNo1BgwZx9epVHB0deeGFF3B2dqZ8+fI888wzgBoAJScn06NHD8qXLw9ArVq1clyHJ5lZu8C2bNlCcHAwu3fvZv369SQlJdG+fftMm+Dc3d0ZM2YMu3bt4ujRowwePJjBgwcTGpr6AZs6dSrffvstc+bMYc+ePTg6OhIUFER8fHxhPC2TdAFQsrb4RNhCCPE0qF+/vsF2dHQ07777LtWqVcPNzQ0nJydOnTrF1atXM71O7dq19Y8dHR1xcXHRL+2QU6dOnaJZs2YG+5o1a8a5c+dISUnhueeeo3z58lSoUIFXXnmF33//ndjYWADq1KlDu3btqFWrFr169WLu3Lk8fPgwV/V4Upm1BWjt2rUG2/Pnz8fT05MDBw7QsmVLo+e0bt3aYPutt97i119/Zfv27QQFBaEoCjNmzOCTTz6ha9euACxYsAAvLy/++ecf+vbtWyDPJTssLdV4M1m6wIQQxYS9tSUnJwaZ7d75xdHR0WD73XffZf369UyfPp2KFStib2/Piy++SGJiYqbXsba2NtjWaDQFtmiss7MzBw8eJCwsjHXr1jF27FjGjx/Pvn37cHNzY/369ezcuZN169bx3XffMWbMGPbs2YO/v3+B1KeoKVJJ0JGRkQDZ7gtVFIWNGzdy5swZfcB06dIlwsPDCQwM1JdzdXWlUaNG7Nq1y+h1EhISiIqKMvgpCNbSAiSEKGY0Gg0ONlZm+SnI2ah37NjBoEGD6N69O7Vq1cLb25vLly8X2P2MqVatGjt27MhQr8qVK+vXyLKysiIwMJCpU6dy9OhRLl++zKZNmwD1d9OsWTMmTJjAoUOHsLGxYfny5YX6HMypyCRBa7VaRo8eTbNmzahZs2amZSMjIylTpgwJCQlYWloya9Ysfd9seHg4AF5eXgbneHl56Y+lFxISwoQJE/LhWWTOUh8ASQ6QEEI8ySpVqsTff/9N586d0Wg0fPrppwXWknP37l0OHz5ssK906dL873//o0GDBkyaNIk+ffqwa9cuZs6cyaxZswBYuXIlFy9epGXLlpQoUYLVq1ej1WqpUqUKe/bsYePGjbRv3x5PT0/27NnD3bt3qVatWoE8h6KoyARAwcHBHD9+nO3bt2dZ1tnZmcOHDxMdHc3GjRt55513qFChQobusez66KOPeOedd/TbUVFR+Pr65upambGy1CVBSwuQEEI8yb766iuGDBlC06ZNKVWqFB988EGB9R4sWrSIRYsWGeybNGkSn3zyCUuXLmXs2LFMmjSJ0qVLM3HiRAYNGgSAm5sbf//9N+PHjyc+Pp5KlSqxePFiatSowalTp9i6dSszZswgKiqK8uXL8+WXX9KxY8cCeQ5FkUYpAmPeRo4cyYoVK9i6dWuu+h6HDRvGtWvXCA0N5eLFiwQEBHDo0CHq1q2rL9OqVSvq1q3LN998k+X1oqKicHV1JTIyEhcXlxzXx5Rtm9ewbcMKLLxq8OGbo/LtukIIUVTEx8dz6dIl/P39sbOzM3d1xFMos/dYTr6/zZoDpCgKI0eOZPny5WzatCnXiVdarZaEhAQA/P398fb2ZuPGjfrjUVFR7NmzhyZNmuRLvXPL894ePrZeTMO4rWathxBCCFHcmbULLDg4mEWLFrFixQqcnZ31OTqurq7Y29sDMGDAAMqUKUNISAig5uvUr1+fgIAAEhISWL16NQsXLmT27NmAmtQ1evRoPvvsMypVqoS/vz+ffvopPj4+dOvWzSzPU89Szf7XKMnmrYcQQghRzJk1ANIFLelzd+bNm6fvw7x69SoWFqkNVTExMbzxxhtcv34de3t7qlatym+//UafPn30Zd5//31iYmIYPnw4ERERNG/enLVr15q9OdbicQBkqZUASAghhDCnIpEDVNQUVA7QuZVfU2n/eLZbN6X5mDX5dl0hhCgqJAdIFLSnIgeouNFY2gBgKV1gQgghhFlJAFSILKzUAMhCAiAhhBDCrCQAKky6HCAJgIQQQgizkgCoEFlYSQAkhBBCFAUSABUifQBEiplrIoQQQhRvEgAVIovHSdBWSpKZayKEECK/tW7dmtGjR+u3/fz8mDFjRqbnaDQa/vnnnzzfO7+uU5xIAFSIpAVICCGKns6dO9OhQwejx7Zt24ZGo+Ho0aM5vu6+ffsYPnx4XqtnYPz48QbLPOncunWrwNfxmj9/Pm5ubgV6j8IkAVAh0o0Cs5IcICGEKDKGDh3K+vXruX79eoZj8+bNo379+tSuXTvH1/Xw8MDBwSE/qpglb29vbG1tC+VeTwsJgAqRtAAJIUTR88ILL+Dh4cH8+fMN9kdHR7Ns2TKGDh3K/fv36devH2XKlMHBwYFatWqxePHiTK+bvgvs3LlztGzZEjs7O6pXr8769esznPPBBx9QuXJlHBwcqFChAp9++ilJSWraxPz585kwYQJHjhxBo9Gg0Wj0dU7fBXbs2DHatm2Lvb09JUuWZPjw4URHR+uPDxo0iG7dujF9+nRKly5NyZIlCQ4O1t8rN65evUrXrl1xcnLCxcWF3r17c/v2bf3xI0eO0KZNG5ydnXFxcaFevXrs378fgCtXrtC5c2dKlCiBo6MjNWrUYPXq1bmuS3aYdSmM4sZS1wKEtAAJIYoJRYGkWPPc29oBNJosi1lZWTFgwADmz5/PmDFj0Dw+Z9myZaSkpNCvXz+io6OpV68eH3zwAS4uLqxatYpXXnmFgIAAGjZsmOU9tFotPXr0wMvLiz179hAZGWmQL6Tj7OzM/Pnz8fHx4dixY7z66qs4Ozvz/vvv06dPH44fP87atWvZsGEDoK6dmV5MTAxBQUE0adKEffv2cefOHYYNG8bIkSMNgrzNmzdTunRpNm/ezPnz5+nTpw9169bl1VdfzfL5GHt+uuBny5YtJCcnExwcTJ8+fQgLCwOgf//+PPPMM8yePRtLS0sOHz6MtbXaMBAcHExiYiJbt27F0dGRkydP4uTklON65IQEQIVI1wVmTTKKoug/ZEII8dRKioUvfMxz749vgo1jtooOGTKEadOmsWXLFv36lPPmzaNnz564urri6urKu+++qy8/atQoQkNDWbp0abYCoA0bNnD69GlCQ0Px8VFfjy+++CJD3s4nn3yif+zn58e7777LH3/8wfvvv4+9vT1OTk5YWVnh7e1t8l6LFi0iPj6eBQsW4OioPv+ZM2fSuXNnpkyZgpeXFwAlSpRg5syZWFpaUrVqVTp16sTGjRtzFQBt3LiRY8eOcenSJXx9fQFYsGABNWrUYN++fTRo0ICrV6/y3nvvUbVqVQAqVaqkP//q1av07NmTWrVqAVChQoUc1yGnpAusEFk+7gKzIoUUrSzBJoQQRUXVqlVp2rQpv/zyCwDnz59n27ZtDB06FICUlBQmTZpErVq1cHd3x8nJidDQUK5evZqt6586dQpfX1998APQpEmTDOWWLFlCs2bN8Pb2xsnJiU8++STb90h7rzp16uiDH4BmzZqh1Wo5c+aMfl+NGjWwtLTUb5cuXZo7d+7k6F5p7+nr66sPfgCqV6+Om5sbp06dAuCdd95h2LBhBAYGMnnyZC5cuKAv++abb/LZZ5/RrFkzxo0bl6uk85ySFqBCZGmtawFKIVmrYGWZxQlCCPGks3ZQW2LMde8cGDp0KKNGjeL7779n3rx5BAQE0KpVKwCmTZvGN998w4wZM6hVqxaOjo6MHj2axMTEfKvurl276N+/PxMmTCAoKAhXV1f++OMPvvzyy3y7R1q67icdjUaDVqstkHuBOoLtpZdeYtWqVaxZs4Zx48bxxx9/0L17d4YNG0ZQUBCrVq1i3bp1hISE8OWXXzJq1KgCq4+0ABUiK2s1Q9+KFBJTCu5NJoQQRYZGo3ZDmeMnh2kGvXv3xsLCgkWLFrFgwQKGDBmiT1XYsWMHXbt25eWXX6ZOnTpUqFCBs2fPZvva1apV49q1a9y6dUu/b/fu3QZldu7cSfny5RkzZgz169enUqVKXLlyxaCMjY0NKSmZD6SpVq0aR44cISYmRr9vx44dWFhYUKVKlWzXOSd0z+/atWv6fSdPniQiIoLq1avr91WuXJm3336bdevW0aNHD+bNm6c/5uvry+uvv87ff//N//73P+bOnVsgddWRAKgQ2dikJkEnJksAJIQQRYmTkxN9+vTho48+4tatWwwaNEh/rFKlSqxfv56dO3dy6tQpXnvtNYMRTlkJDAykcuXKDBw4kCNHjrBt2zbGjBljUKZSpUpcvXqVP/74gwsXLvDtt9+yfPlygzJ+fn5cunSJw4cPc+/ePRISEjLcq3///tjZ2TFw4ECOHz/O5s2bGTVqFK+88oo+/ye3UlJSOHz4sMHPqVOnCAwMpFatWvTv35+DBw+yd+9eBgwYQKtWrahfvz5xcXGMHDmSsLAwrly5wo4dO9i3bx/VqlUDYPTo0YSGhnLp0iUOHjzI5s2b9ccKigRAhUjzeCZoG00KiUkyFF4IIYqaoUOH8vDhQ4KCggzydT755BOeffZZgoKCaN26Nd7e3nTr1i3b17WwsGD58uXExcXRsGFDhg0bxueff25QpkuXLrz99tuMHDmSunXrsnPnTj799FODMj179qRDhw60adMGDw8Po0PxHRwcCA0N5cGDBzRo0IAXX3yRdu3aMXPmzJy9GEZER0fzzDPPGPx07twZjUbDihUrKFGiBC1btiQwMJAKFSqwZMkSACwtLbl//z4DBgygcuXK9O7dm44dOzJhwgRADayCg4OpVq0aHTp0oHLlysyaNSvP9c2MRlEUycZNJyoqCldXVyIjI3Fxccm/C8c+gKn+AFwecQU/L7f8u7YQQhQB8fHxXLp0CX9/f+zs7MxdHfEUyuw9lpPvb2kBKkyWqQlnSUn5lzgnhBBCiJyRAKgwWaQNgDL22wohhBCicEgAVJikBUgIIYQoEiQAKkwWlqQ8fsmTE6UFSAghhDAXCYAKWQrq7IfJ0gIkhHiKyfgaUVDy670lAVAhS9Gok28n5+PsoUIIUVTollbIzxmShUgrNlZdXDf9TNY5JUthFDJdC1BKsvznIIR4+lhZWeHg4MDdu3extrbGwkL+zhb5Q1EUYmNjuXPnDm5ubgbrmOWGBECFLEVjBYoEQEKIp5NGo6F06dJcunQpwzIOQuQHNzc3vL2983wdCYAKmb4LTHKAhBBPKRsbGypVqiTdYCLfWVtb57nlR0cCoEKWolH7LFMkABJCPMUsLCxkJmhRpEnnbCHTPm4Bki4wIYQQwnwkACpkKY9ng1aSZR4gIYQQwlwkACpkKRa2AChJ8WauiRBCCFF8SQBUyLTSAiSEEEKYnQRAhSzFUm0BIllagIQQQghzkQCokGl1XWCSBC2EEEKYjQRAhUyxtAFAkyItQEIIIYS5SABUyJTHXWAaaQESQgghzEYCoEKW2gIkSdBCCCGEuUgAVNis1JlRJQASQgghzEcCoEKmWKldYBYp0gUmhBBCmIsEQIVMowuAtNICJIQQQpiLBECFTR8ASQuQEEIIYS4SABUyXQuQpQRAQgghhNlIAFTINNZqErQEQEIIIYT5SABUyCysJAASQgghzE0CoEJm8bgFyEqRAEgIIYQwFwmACpmFjRoAWUsLkBBCCGE2EgAVMksbNQnaSkkyc02EEEKI4ksCoEJmaW0PgDXSAiSEEEKYiwRAhczycQ6QtbQACSGEEGZj1gAoJCSEBg0a4OzsjKenJ926dePMmTOZnjN37lxatGhBiRIlKFGiBIGBgezdu9egzKBBg9BoNAY/HTp0KMinkm1WtroWIAmAhBBCCHMxawC0ZcsWgoOD2b17N+vXrycpKYn27dsTExNj8pywsDD69evH5s2b2bVrF76+vrRv354bN24YlOvQoQO3bt3S/yxevLign062WD9OgrYlieQUrZlrI4QQQhRPVua8+dq1aw2258+fj6enJwcOHKBly5ZGz/n9998Ntn/66Sf++usvNm7cyIABA/T7bW1t8fb2zv9K55GuBciGJBJTtFhZSi+kEEIIUdiK1LdvZGQkAO7u7tk+JzY2lqSkpAznhIWF4enpSZUqVRgxYgT37983eY2EhASioqIMfgqKtY0aANmSRGKytAAJIYQQ5lBkAiCtVsvo0aNp1qwZNWvWzPZ5H3zwAT4+PgQGBur3dejQgQULFrBx40amTJnCli1b6NixIykpKUavERISgqurq/7H19c3z8/HFN0weBsJgIQQQgiz0SiKopi7EgAjRoxgzZo1bN++nbJly2brnMmTJzN16lTCwsKoXbu2yXIXL14kICCADRs20K5duwzHExISSEhI0G9HRUXh6+tLZGQkLi4uOX8ymYm5D9MqAHD9zRuUdXfK3+sLIYQQxVRUVBSurq7Z+v4uEi1AI0eOZOXKlWzevDnbwc/06dOZPHky69atyzT4AahQoQKlSpXi/PnzRo/b2tri4uJi8FNgrGz0D5MS4gruPkIIIYQwyaxJ0IqiMGrUKJYvX05YWBj+/v7ZOm/q1Kl8/vnnhIaGUr9+/SzLX79+nfv371O6dOm8VjnvHi+GCpCUEGvGigghhBDFl1lbgIKDg/ntt99YtGgRzs7OhIeHEx4eTlxcasvIgAED+Oijj/TbU6ZM4dNPP+WXX37Bz89Pf050dDQA0dHRvPfee+zevZvLly+zceNGunbtSsWKFQkKCir055iBpTXJWAKQIgGQEEIIYRZmDYBmz55NZGQkrVu3pnTp0vqfJUuW6MtcvXqVW7duGZyTmJjIiy++aHDO9OnTAbC0tOTo0aN06dKFypUrM3ToUOrVq8e2bduwtbUt9OdoTBxqK1ByfLSZayKEEEIUT2bvAstKWFiYwfbly5czLW9vb09oaGgealXw4jV2OCsxaBNMT/gohBBCiIJTJJKgi5t4jToXUEqCtAAJIYQQ5iABkBkkWqhdYNp4aQESQgghzEECIDNI0gVA0gIkhBBCmIUEQGaQZOkAgDZRWoCEEEIIc5AAyAySLNUcIEUCICGEEMIsJAAygxQrtQUICYCEEEIIs5AAyAxSrNQWIE2iTIQohBBCmIMEQGagtXYEQJMkLUBCCCGEOUgAZAaKtdoFZpEsi6EKIYQQ5iABkBkoj1uALJOlBUgIIYQwBwmAzEBjo7YAWSVLDpAQQghhDhIAmYHG1gUA6xQJgIQQQghzkADIHOxcAbBPkZmghRBCCHOQAMgMNPZuANhrJQASQgghzEECIDOwtFe7wBwkABJCCCHMQgIgM7ByKAGAgxILimLm2gghhBDFjwRAZmDt5AaAJVpIlFYgIYQQorBJAGQGdnaOJCqW6kZ8pHkrI4QQQhRDEgCZgZ2NFVGokyESH2XeygghhBDFkARAZmBvY8kjRV0QVRsXYd7KCCGEEMWQBEBm4GBjqW8BSox5aObaCCGEEMWPBEBmYGeV2gKUJAGQEEIIUegkADIDCwsNMRZqC1ByjCRBCyGEEIVNAiAzidU4AZAiOUBCCCFEoZMAyEziLJ0BUGLum7kmQgghRPEjAZCZPLTyAMDi0U0z10QIIYQofiQAMpNIa08ArGLCzVwTIYQQoviRAMhM4m3cAbCKf2DmmgghhBDFjwRAZpJoqy6Iap0gAZAQQghR2CQAMpMkW7UFyCb5ESQnmrk2QgghRPEiAZCZ3E9xIEXRqBuxMhJMCCGEKEwSAJlJ2Ln7PEQdCk/sPfNWRgghhChmJAAyExsrCx4ojwOgGAmAhBBCiMIkAZCZfNW7Dg9wUTekC0wIIYQoVBIAmUlpVzvCFXUkGFE3zFsZIYQQopiRAMhMbK0sua6os0ETcdW8lRFCCCGKGQmAzMTOWgIgIYQQwlwkADITO2sLbiil1I2Ia+atjBBCCFHMSABkJmlbgJSIq6AoZq6REEIIUXxIAGQmdtaW3FRKAqBJioFYWRJDCCGEKCwSAJmJnZUFCdhwR3FTd0RcMWt9hBBCiOJEAiAzsbJUX/rrujygSMkDEkIIIQqLBEBmJiPBhBBCiMInAZCZpY4EkwBICCGEKCwSAJnRqy38uap4AqA8uGTm2gghhBDFhwRAZhSdkMwlpbS6cf+8eSsjhBBCFCMSAJmRRqPhovZxABRxBZITzVshIYQQopiQAMiMLDRwBzdiFFs0ilaGwgshhBCFxKwBUEhICA0aNMDZ2RlPT0+6devGmTNnMj1n7ty5tGjRghIlSlCiRAkCAwPZu3evQRlFURg7diylS5fG3t6ewMBAzp07V5BPJVc0aACNdIMJIYQQhcysAdCWLVsIDg5m9+7drF+/nqSkJNq3b09MTIzJc8LCwujXrx+bN29m165d+Pr60r59e27cuKEvM3XqVL799lvmzJnDnj17cHR0JCgoiPj4+MJ4WtlWvqQDAJcUb3XH/QtmrI0QQghRfGgUpegsQnX37l08PT3ZsmULLVu2zNY5KSkplChRgpkzZzJgwAAURcHHx4f//e9/vPvuuwBERkbi5eXF/Pnz6du3b5bXjIqKwtXVlcjISFxcXPL0nDKTkJxClU/W8rbVMt6yWg71BkHnbwrsfkIIIcTTLCff30UqBygyMhIAd3f3bJ8TGxtLUlKS/pxLly4RHh5OYGCgvoyrqyuNGjVi165d+VvhPLK1sgTgsvZxC9CDi2asjRBCCFF8WJm7AjparZbRo0fTrFkzatasme3zPvjgA3x8fPQBT3h4OABeXl4G5by8vPTH0ktISCAhIUG/HRUVldPq58kV5XFdZS4gIYQQolAUmRag4OBgjh8/zh9//JHtcyZPnswff/zB8uXLsbOzy/W9Q0JCcHV11f/4+vrm+lq5cVmXAxR5HZKKVp6SEEII8TQqEgHQyJEjWblyJZs3b6Zs2bLZOmf69OlMnjyZdevWUbt2bf1+b281mLh9+7ZB+du3b+uPpffRRx8RGRmp/7l2rXAXJn2AMzEaR0CBh9IKJIQQQhQ0swZAiqIwcuRIli9fzqZNm/D398/WeVOnTmXSpEmsXbuW+vXrGxzz9/fH29ubjRs36vdFRUWxZ88emjRpYvR6tra2uLi4GPwUlu7PlAE0RNs9Ds6Wv1Zo9xZCCCGKK7PmAAUHB7No0SJWrFiBs7OzPkfH1dUVe3t7AAYMGECZMmUICQkBYMqUKYwdO5ZFixbh5+enP8fJyQknJyc0Gg2jR4/ms88+o1KlSvj7+/Ppp5/i4+NDt27dzPI8M+NfyhGAE87N8Yq7ALeOQOwDcMh+IrgQQgghcsasLUCzZ88mMjKS1q1bU7p0af3PkiVL9GWuXr3KrVu3DM5JTEzkxRdfNDhn+vTp+jLvv/8+o0aNYvjw4TRo0IDo6GjWrl2bpzyhgmJrpf4KVpYcAs6PJ0S8tjeTM4QQQgiRV2ZtAcrOFERhYWEG25cvX87yHI1Gw8SJE5k4cWIua1Z47KzVofDxySkQ0BYO/w5XdkCVDmaumRBCCPH0KhJJ0MWZnbX6K1h9LJw4v3bqzqNLZDSYEEIIUYAkADIzXQsQQO0/rMDRA6Jvw9WdZqyVEEII8XSTAMjMdDlAAElYgf/jJUBOLDdTjYQQQoinnwRAZpaQrDXcUfvxWmUXwqDoLNMmhBBCPFUkADIz3TB4vfJNwcIKIq/Cw8tmqZMQQgjxtJMAyMxqlXE13GHrBGUbqI8vbCr8CgkhhBDFgARAZqbRaDLurNBG/XfVO9INJoQQQhQACYCKohrdUh+f32iymBBCCCFyRwKgoqhU5dTHu783Xz2EEEKIp5QEQEWRRgOvPs7/ubgFEh6Ztz5CCCHEU0YCoKKqTD1wKwdKClzeYe7aCCGEEE8VCYCKmOiE5NQNv8eTIi7uI0tjCCGEEPlIAqAiZviC/akbzUenPv7cS1qChBBCiHwiAVARMLv/s/rHOy/cTz1QqhJU6ZS6Pf95iElzXAghhBC5IgFQEdCqiofBdkzabrB2nxoWlkVShRBCiDyTAKgIsEg3GeKtyLjUDc9qMCYcytRXt6/tLcSaCSGEEE8nCYCKACsLwwDozqMEwwLW9tBgmPr4zBpISSqkmgkhhBBPJwmAigDLdAHQzE3nMxbyaw6WNnD/HEwqBTcPFVLthBBCiKePBEBFgEajoV9DX/32xbsxGQu5+UL7z1O3/36tEGomhBBCPJ0kACoiQnrU5v0OVQBo6O9uvFDdl1If3zsDcREFXzEhhBDiKSQBUBFSytEWSDcZYlq2TvDGntTto0sKoVZCCCHE00cCoCLExd4agHN3Mln7y7MqdJyqPl7zPoQfL4SaCSGEEE8XCYCKkCrezgBcexDHl+vOmC74zMupj+c0K+BaCSGEEE8fCYCKEP9SjvrH3xkbCaZj4wh+LVK3/3kDYu4VYM2EEEKIp4sEQE+qvotSHx/+HWbUlmUyhBBCiGySAKiIeaN1AABNA0pmXtDOxTAISoqBI4sLsGZCCCHE0yNXAdC1a9e4fv26fnvv3r2MHj2aH3/8Md8qVlzV8HEFIFmrZF24aid4+wSUrqtu7/8Ztk6HNR/C3UxyiIQQQohiLlcB0EsvvcTmzZsBCA8P57nnnmPv3r2MGTOGiRMn5msFixsHW0sA9l56QHxSStYnuJaFTl+pjx9chE2TYM9smPc8KNkIooQQQohiKFcB0PHjx2nYsCEAS5cupWbNmuzcuZPff/+d+fPn52f9ih1HGyv948lrTmfvJI/KGffF3oM7J/OpVkIIIcTTJVcBUFJSEra26qR9GzZsoEuXLgBUrVqVW7du5V/tiiEHG0v94/1XHqBkpxXH1hn6/5W67Vxa/ff3XnBxSz7XUAghhHjy5SoAqlGjBnPmzGHbtm2sX7+eDh06AHDz5k1KlswieVdkyj5NAHT8RhStpoWRlKLN+sRKgTA+Uv15doC6L+oGLOgCx//K/FwhhBCimMlVADRlyhR++OEHWrduTb9+/ahTpw4A//77r75rTOSOJt321QexHLjyMGcXca9guP3nEIh7CFd2wtqPIDE2T3UUQgghnnRWWRfJqHXr1ty7d4+oqChKlCih3z98+HAcHBzyrXLFkV9Jxwz77kUn5Owi1brAhU2Ga4Xt+QHCQtTHEVeh16+w5j3wrA4NX81DjYUQQognT65agOLi4khISNAHP1euXGHGjBmcOXMGT0/PfK1gcWNhoeHXIYataNkZEW/AxgF6/Kh2h/X4Sd2nC34ATq+ESSVh/y+w+l2IvqOOGIu8kbfKCyGEEE+IXAVAXbt2ZcGCBQBERETQqFEjvvzyS7p168bs2bPztYLFUdpEaICft13M/cWqdMy6zPRKMMENvq4OJ/4xPJaSrP4IIYQQT5FcBUAHDx6kRQt1Lao///wTLy8vrly5woIFC/j222/ztYLFkb21YQB05Hokp25F5e5itk6G2wNXQuNg0+WXDYRfOsClrer6YpNKqj9nQ3N3fyGEEKIIylUAFBsbi7OzunL5unXr6NGjBxYWFjRu3JgrV67kawWLI/t0LUAAD2MSc3/BXvPVf7v/AP4toMMXatdY20/Bs0bG8ld3wa+d4eCC1H2LesOp/2S9MSGEEE+FXCVBV6xYkX/++Yfu3bsTGhrK22+/DcCdO3dwcXHJ1woWR+lbgACsLPOwbFuN7upPWrV7qf9WbAe7voeyDeH6Pji2NLXMxgmG5yx5Wf330/tgmau3jhBCCFEk5OpbdezYsbz77rv4+fnRsGFDmjRpAqitQc8880y+VrA4MhYAWVqkHyCfT3yegZ4/QaPh0PpDcCiV9Tl/y6gxIYQQT7ZcBUAvvvgiV69eZf/+/YSGpuaGtGvXjq+//jrfKldcOdpmbF3RFsa6XiUD4H+nYexDqPu4taeEH7x9EmxdU8ud+Bv2zi34+gghhBAFJNf9GN7e3nh7e+tXhS9btqxMgphPbKwyxqVJydmYDTo/WFqr/3adCR1CwO5xl+ZHV+HOKZjVWN1e/S54VIGoW1DpOXBwT73Go9sQfRtK1y6cOgshhBA5lKsWIK1Wy8SJE3F1daV8+fKUL18eNzc3Jk2ahFZbSF/UT7laZVwNthOzsxxGftJoUoMfHc9qMODf1O1fO8Py4TDVH7bPUPfFRcCXleGHFjDeVf25GFZIlRZCCCGyJ1ctQGPGjOHnn39m8uTJNGvWDIDt27czfvx44uPj+fzzz/O1ksVRh5reHLsRqd+OTyoigWWFVlChDVzcbLh/wzhwLAUrjAyxX9AV3r8E27+ChGjo9BVY5CGpWwghhMgjjZKt5cYN+fj4MGfOHP0q8DorVqzgjTfe4MaNJ3tG4aioKFxdXYmMjDTbqLakFC2VxqzRbzeu4M4fw5uYpS4ZxD2EKX65P//VzVDm2XyrjhBCCAE5+/7O1Z/hDx48oGrVqhn2V61alQcPHuTmkiIda0sLnq/lrd/effEB8UkpZqxRGvYl4JM78MIMKFkp4/F+f6hD5T+9p641ll7YZLi2r8CrKYQQQpiSqwCoTp06zJw5M8P+mTNnUru2JL7ml486VjPYfnvJYfNUxBgrW6g/GEbth+C9qfubjFSX37C0UhOqGwzNeO65UPg5EE6vKrz6CiGEEGnkqgtsy5YtdOrUiXLlyunnANq1axfXrl1j9erV+mUynlRFoQtMZ8Guy4xdcUK/fXlyJzPWxgRFgX9HQnykusq8RZp5jBJj4Asf0+e+MEMNpEAdPbb5c3DzhRbvqonYOju+hWPLYMAKwxFnQgghxGMF3gXWqlUrzp49S/fu3YmIiCAiIoIePXpw4sQJFi5cmKtKC+PsjEyKWORoNND1e+jzm2HwA2DjCG/shmqdYdBqqPqC4fGVo9WcopRkdfTYwV9h02fqivU6BxfA+k8h/Cj80ApkpKEQQog8ylULkClHjhzh2WefJSWliOSq5FJRagH678hNRi0+pN8e3rIC7wVVwTovS2OY2+r3YO+PmZdpOkodbXZtL2yZbHis7SfQ8r2Cq58QQognUoG3AOWXkJAQGjRogLOzM56ennTr1o0zZ85kes6JEyfo2bMnfn5+aDQaZsyYkaHM+PHj0Wg0Bj/GkrafBOmXxfhx60UW771qptrkk8AJ0PkbKN/cdJmd38FvPTIGP6C2EOnidkWB6wdg8UtwdXfB1FcIIcRTx6wB0JYtWwgODmb37t2sX7+epKQk2rdvT0xMjMlzYmNjqVChApMnT8bb29tkuRo1anDr1i39z/bt2wviKRQ4YyvDj11xgqv3Y81Qm3xi4wD1BkHN7hmPDQ8DK/uM+0cdNJyE8ad2MP8FmOAGP7WFM6tg7YcFVGEhhBBPG7Mu6b127VqD7fnz5+Pp6cmBAwdo2bKl0XMaNGhAgwYNAPjwQ9NfeFZWVpkGSE8KO2vjMer3m88z5cUnfMRd/aHgUgYcPeHcOnDxURdnfWMnfPt4UV0re3jzELiUVtcq86oJt4/DjQMZr3fzEBxZAnausGE8PDsAAtrCoYXQ6DVw8obDv0GVTuDsVahPVQghRNGSowCoR48emR6PiIjIS12IjFRnPnZ3z/son3PnzuHj44OdnR1NmjQhJCSEcuXKGS2bkJBAQkKCfjsqKirP988vliZmTDa2XtgTR6NRh8wDlK2Xut+9gjqHkDYZLKxS1ycD6PItzG1r+prLh6c+Dv0o9fH9CxB7D67vg5Vvw6uboGRFNVhKT1EMR6AJIYR46uToW9TV1TXTn/LlyzNgwIBcVUSr1TJ69GiaNWtGzZo1c3UNnUaNGjF//nzWrl3L7NmzuXTpEi1atODRo0dGy4eEhBg8D19f3zzdPz+ZylF/KgKgzFhag7W9YfADUKae2h1W92WwtFX3VWgDbuUzv97ZNWrwozO3LXxZVV3DLPwYnHzcvRZ9F76uAWs/zrenIoQQoujJ11FgeTFixAjWrFnD9u3bKVu2bLbO8fPzY/To0YwePTrTchEREZQvX56vvvqKoUMzTsxnrAXI19e3SIwCuxUZR5OQTUaP7fqoLaVdjeTLFCdarbqumFardqMdXKDmA9m6qpM1xtzJ/rWemwgPLsKB+er2uAi4fQLcymVcGFYIIUSRk5NRYGbNAdIZOXIkK1euZOvWrdkOfnLCzc2NypUrc/78eaPHbW1tsbW1zff75ofSrvbMH9yAQfMyLh3R7sstnJzYwQy1KkJ0XYQWFlClA1QMhMvbwLehOr/Q/QvqIqy6Fel9G8M1E6PF1o813P53lJo/BOpirjIBoxBCPDXMGgApisKoUaNYvnw5YWFh+Pv7F8h9oqOjuXDhAq+88kqBXL+gta7iaXR/bOKTPd9SgbC0goA26mMbR3AtC/4t4dBvagBTtRMc/wsubFL3ZeZQmkk9p/rD2AcZJ3oUQgjxRDJrIklwcDC//fYbixYtwtnZmfDwcMLDw4mLi9OXGTBgAB99lJrMmpiYyOHDhzl8+DCJiYncuHGDw4cPG7TuvPvuu2zZsoXLly+zc+dOunfvjqWlJf369SvU5yeKCI0Gnn1FDX4AavZUZ64eFwHdZkPF59QFXnVMzU90dImaIL3uU5hYCm4eLuiaCyGEKCBmzQHSmBhpM2/ePAYNGgRA69at8fPzY/78+QBcvnzZaEtRq1atCAsLA6Bv375s3bqV+/fv4+HhQfPmzfn8888JCAjIVr2K0kzQOn4fGl84tK6vG8vfaGrytRS5NN7I6DBHT3h+KiwblLrPu7bx9cmWv64mXfdeAF41DI/p8pZ0Iq6CtSM4loTkBHXkW3Zamh5eUZPEXTJZa00IIYqRnHx/F5kk6KLkSQqAAE5ODMLBpkikcz097pyCn56DFm9D5HXY/4u6v4QfPLxsWLZ2H+iRZmmPy9thfppFa8c+UFuOpleCuAfqvg8uq61Od07BrMbqvgpt4OJmcPaBkXvB1lldI83SyO82PhImlwONBXxy13gZIYQoZp64JGiRNWdbKx4lJBs9JiFsAfCsBh9fVx9H30kNgNIHP6B2jR1dYvpaE40kTx/7Exq+Cuc3pO67uFn999FNWPtRag5S20+hyvOw9wewsFYnhdw4UT2maNV8psrtc/T08sW59WpwWG+QzJtkTFwEJMWpvy8hRJHzlE8m8/T4642mNKlQ0uix5BSJgAqUkyf0XWS4b+BK42Wza/W76kKv5zcaP542AXvTJJjdRB2ev29uavCjc+MAaAs5IX756/D7i7BytOnnUJCu74eNk2BmA1j1v8z/CkiIhiN/qP8WhnvnYdlgmFJerd+j8MK5rxAiRyQAekJU9nJm8fDGRo8la7X6x38euM5fB64XVrWKj0pBYO2gPm4yEvxbqEnU/q2Ml3f2gbeOZH7Nn59LbfV5+S9o/bE60WNObZkMM+tD+HG12+7SVvXf8a7qz7zn8/fL/+EVOLI4dfvYUtNld3wLPwXC4UX511SpTVHXgts2He6dhX0/qXNAmbKgCyx/TZ34Mjkxf+pgSsx9+O8tOPG3up34yLCVTwhRZEgX2FPg74M3aF3FA29XO95dpn7pBtX0xslWfr35xtIKPr6pfonrEpg1Ghj4eAbp5ES4dVidSDH2gTr0voQffHhNTWg+9qc6N1H0bVjQNeP1vWurcxg1HQmL+8GlLcbrYesCCVFQ5yXwrgmhj2esfnARFvWBqOvwa2fDc67sUIOkNp+AtV3eX4sL6Vp8ji6BoC/AsZTh/ug7sP5T9bFuFu6qncDGKWfTCRz6HVa8oSaKlwxQ14JL7+xaqBz0+F4H1AkwvWtDYnTqunGJj+Dgr2rXI6i/s7unwLNG3nOoru6GpQMh2khrz+HFULd/6tIuxroLdcuvbJ8Bt46oOWXpZ0EvTmLuqa129QaquXCicGlT4O5p8KhmOGDjKSNJ0EYUxSRoncySoQ9++hzPTloPwL4xgXg4F83JHYu9c+vV7qO0xkUYfjHG3FMTnM+uhX9GqPvGPoRHt+D+OajQWh0x9pnxOaJM6vkz1Hox63L7flK7lgBe/ludDXvzF9AkWG19ATX5W5f71P0HqNPX8BqXtmYMxnRK+MHIA2rgodWqk056VIFmbxqWS4iGkDKm6+laDiKvqv++fUy91sQSpssDtBkD9YfAtMejQgPHQ/O31cfRd2Hnt1C2AaQkQvWuGQMRRVFzn1zLqr+zpDj43MjCy05easAL6v0OLlBbD8s3VWcpr9BaPXZhMyzsZnhux6nqAr5Pu+i76mvkXROS4tVJS0vXVbt5755Sy7T9RN1XqpL6viluEqLVgN69QvbKP7qtdtsbC7QXdldzBmv3VacAMRXcbJkKmz+H56en/sHwhJBRYHlUlAOgzafvMHh+xlmhAQ58Eki9z9Tm9t0ftcPbNR/+2hcFZ+MkOBcKL86HUhWNl1EUOBsKPs8YX8E+PhKmV4bk+IzHBv4HaODXFwz3j49Mvfbu2eoM2NokKF0HevwEbr4Zv9Artc/YzdTqAzVQ2/+zul3leei9EK7uBIdSat5SVrp8ByUrwbzHM5qPOqjmOt08pAYg1/aYPrdWL+j0FUx+vHZfuSbwwgyY1Sjr+6blUQ2CH88Ovvx1w+69MvVg2EbDL5NDv8GKYLU1bniY2lq38m3Da3abDXVfMj6dgk6jEWqr2aZJGY/5tYBBK9XuRteyhi1migJXd4FLGSjxeA28GwfV0YdNgs03WeexP9Vg8JmXs5cUr9Wq75F75+D1bWqAfTqL3LqPb6oTnD5J4iPV7vPctugt7K7OZP/aVvCuZbyMokDsfQibrOYJNhwODYapf1RE31VbfuOj4OvqqecM/E9tqTYm7fv2/UvqZ7LeoCdiNnwJgPKoKAdAiclaKn+yxuixvR+3o+EXavfEtvfb4OvuUJhVE+aSvqWlVm91okcrG+MtIt61oPM36oKweTE8TP3rNH2AlVOOHhBzN+tyg1apS5tsGKc+rzcPqgHE1zUh8lrm5w7bBBsnGO9atHGGQf+pgWZYSMbjVvYwfDNE3VSDjswCLK9a6pe5LgA48Cv896bp8pnp8xsseRmqdVG7VUvXhsZvwI5v1C85AN9GakK48jgJPrstfPkh5j7cPw/lGqndjGnfT69uhjLPqo8fhav196pueP65DfB7z5zft+8iNSfv+l418K/QpvBGIeoCjYeX4dcu0GAItP/MdPnbJ1P/EHhxHtTskb17rBytfi60KWqrGECL/0G7sRnLa1OMjzRNy8oekuMy7u81H2p0z7jfVOD+vzPgbKS1M62ER2bttpQAKI+KcgCkKAr+H602eqxfQ18W71W/CDa805KKntJ3XmycXqW25tQfYvo/2WkVsxdoZMdrW9UWI0htVjflvQtqoHJ5u9qFEfsAfmiRs/ul/Y83JUkd/m/1uIv34RX4prbx88o2UBe5Ld9U7WJZ0j97ScnlmqitLDnhWg5GHVADz7R+bp95SxaogdOI7erQ+Snlc3bf9Fq+Dy3fU1sc7pwCd3+wsoMdM+DKTijXGPxaqhNvng1VW2yMfWGFH1ODreR4NbDSvd43DsDeuYYtZca4B8CDC6nbg9eovwedNR/Cntmmz6/WBU79m/Xz7fET1O6Vdbm0dF97WQVOibFqAOvzrNqq8lM7NdcvrVYfgLV9ajdqWnPbwY39qdtjwtWymcksaNadn/BIDXzs3dTBD3OaZX7NtNK35v7vjDqYQNcapCgwyUNtFU7Puxa8vt30tXVd5/UGq+/BM6vh2YGpn4m4h3D3rNqinf5zkk8kAMqjohwAQeZ5QDorRzWnZplMmt9F8XPntPHWi7afqAnDi3ob7u+9QG3x0OX86NTqBT1/Mtxn7Ivb1hWGbQCPyhnveWUnHFuWOr9SehpLNam5bn+olo0Wpg0TUv9S1gloB6/8bbhPq1VHhB3/C/r+rs639PBSxuuNfai2qkwqlfGYTuM3YPcs9bFbeQjeazzJ/N55OLQAKneAeR2NX6v/X1ApUH387yg1Xyiv0gYgLmXVBHlTesyF2ml+/w8uwrfPGJYZuV8Niv4cnLv6lKkHQzeowaulFXxdS83fSuv56WpgFdAO2o5Rk8v/e0vNnfp3pPHrOpRUA0/7EhmP3b8Ad89AlY6pwY6iwFfV1fm23j6hdjEmxqqfjYir8MYeNYfGvoQ6sOBcqHqeSxmIumH6+XX9Xg0mQZ3AdON42Pmd6TLGxEepwXzcQ+PHA8errTlrPzB9jawM2wQ/GWn9rdZZrd+iPqaDf8/q8Ea6Y7eOqgG0u7860vLRLcPjHtVS87l0Gr6mzqpfACQAyqOnIQBa+loTGvoX/f5aUcge3YYv0wQkFVqrS3mA2jITfVvtUqvaSf1iSCslWc1ncDQ+HxUzakPEFXXJkPfOZa8+Z0Phr1fV/3xbva8OmW/4qvo4J5IT4PjfakDi4qN2S7j6mh71lhSn/iX9e+/ULzgd30Yw9PFfyFE34YeW6n/wDy6mlhm2CcrWS+0q6DYH6mZjrcEtU+H2CQgcp+ZenV4Nz01Qv+B1X9BaLXxbR/0y1tOoAdTVXRAfoQaGh39PPTwkFH7vpY4QzIvq3dSE9m/qGB/Rlhm38mq36J45sGWK8TKl62ZsQXn5cZBasV360oa0WvilfeqIwvSavaW2Otg6q60jf7+qBrpg2NVzfgP8lq7rrdHrar3TSpvEnl26/Lrjf8GfQzIet3eH/52Go0vVUVYuPmqujpWtGpjNbKAOckircTBc3qoGn5nxqqlOmpoUq+bQObjDps8NW3J0AdiB+WpgmV2vbUttte3+gzo4wNoeom6pAVtKLqaXSD/wI59IAJRHT0MA5GpvzZFxZpgdWBR9V3fDL4+HjL+0LH9nkb51VA2cnoBkSUDtTtO18rR8H1p/ZHpkzJJX1BF8r25KzWe5cQCu7VNHbOXnf+Zpu4cGrlRfU/d0ayDqhs7rLB8BR9JN2JkbbT6BzZnktaTV82c1SKzdO7WLMike5jRXv8hf/kutZ/pRjzo1ekCvedmvm6LAnh/g8jboEAIzjCQFO3mpXcHp87lG7FLzYPKa+9b5G7WlJvo27JppeCygLQSFGLa0NhgGzUbDjJq5u9+IXWqL0PznMy/30tLUqSDS0qaoLbR2robTPZhqEdZp+qbazW3nprZOfldPzfnScfLOeZCcVrtx0OKd3J9vggRAeVTUA6AtZ+9yMyKOj/7O/C+CSyHPyyKpwrhjf6qjhYwlQBY3e35Qc5h6zAW7TD7vyYmQFGO8qyW/3TsPaz+Exq+r80Nlx6PbsHSAGoxUDMyYF/X8dDXP42xo6vxMWeU6vXsewo9kbDGp+oI6V1F2R2Ttnq0+n/Teu2i6RTErigIT3HJ3bk4EfaF+8eu6a3WtPClJ6mtZtj58WcX4uQHtoN9itYXnM2/jicjpaSzUruO5bdVA7oWv1S66b+umtkh9fEtdV3D37NQg7N3z4OSR/eelKOrgicvbMrZ29V6otsqm/f5In89kSo+5ap5RxFX1vbVxopqL1/kb9bOjG7XpHqAOZMhnEgDlUVEPgHRqjF1LTKLpJRDmvPwsHWrKOkRCFEtbpqktB0GfG36RKYraOnJhszqR581DsHU6VH0+de4nnbRf9pbWalfjsT/VLlJ7t+zXRVHUbrWIK6n7gvcZzw/LiSu71DXyTvwDGPkq6zZbfU5JsYb7vWpC15mw6bPUpPi0o6xOr4I/XlKTdQf+B5a26ujDgLZQ6bmM93kUbjwIGrlfnb8IjHe9GfPpfeMTcyqK2nWr0aQmUiuK2q1qbZ9xDq3sSE5Uu3ZLVoTvG6iTjQ5ZC7ZOGcvePqF2B2uNrEnZ7C11WgcUtVsvM8f/VvPIXMrCOydyXucsSACUR09KAHQrMo6/Dlxn+rqzRo8PbFKeCV1z2ewqhCh+LmxSv1AD2kHTUfkzc3haCY/g4EK1Ja3le/l3XW2K2h258h24/bhlXDfPTfqE7pIV1aRpfZ2iITHG+DxbOZF2yDtkTCxPeKQGgbH31cRtv+bqpJznN6pzbx1bBs3fAZ+6eatHQUpOUBdk3jFDnVZCRxcoZ0dCtNoKpGjVKQ2qdsrXKkoAlEdPSgAEEJeYQrWxa40eG9TUj/FdahRyjYQQoohJSVZH15V5tmBnNk6MVZO0/VuaXvIk7XI6T6q0s9AbG22ZlVlN4c4JeOYVtSUuH+Xk+1sWi3rCZfY5Ono9ggt3ownwMNKcKYQQxYWlFXTPZM6h/GLjABVMLJAMalD0NORlWtnCiJ2wa1bOR2wCBH2mrvUW0Cb/65YDT3gYKqwyiYAOXo2g3ZcmFtUUQgjxRAsJCaFBgwY4Ozvj6elJt27dOHPmTKbnnDhxgp49e+Ln54dGo2HGjBkZysyePZvatWvj4uKCi4sLTZo0Yc0awxUIflyxg9YzTuBSvhYajYaIiAiD42FhYWg0GqM/+x44q0n0NXMxE3g+kgDoCWeRjT8mEpJNJ0oLIYR4Mm3ZsoXg4GB2797N+vXrSUpKon379sTExJg8JzY2lgoVKjB58mS8vY0va1G2bFkmT57MgQMH2L9/P23btqVr166cOHHC4DodOnTg448/NnqNpk2bcuvWLYOfYcOG4e/vT/369fP2xPOJ5AAZ8STlAEHW8wLZW1uydnQLypd8whYRFEIIkW13797F09OTLVu20LKliYVO0/Dz82P06NGMHj06y7Lu7u5MmzaNoUOHGuwPCwujTZs2PHz4EDc3N5PnJyUlUaZMGUaNGsWnn36a5f1yKyff39ICVAzEJaXQaloYJ2/mcZZYIYQQRVZkpDoay909/yYiTUlJ4Y8//iAmJoYmTZpkfYIJ//77L/fv32fw4FwupVIAJAB6CnzVuw4fP181y3IvztlZCLURQghR2LRaLaNHj6ZZs2bUrJn36U+OHTuGk5MTtra2vP766yxfvpzq1avn+no///wzQUFBlC1bNuvChUQCoKdAj2fLMrxlQJblYhNT+GnbxSzLCSGEeLIEBwdz/Phx/vjjj3y5XpUqVTh8+DB79uxhxIgRDBw4kJMnT+bqWtevXyc0NDRD95m5SQBUzHy26lTWhYQQQjwxRo4cycqVK9m8eXO+tbDY2NhQsWJF6tWrR0hICHXq1OGbb77J1bXmzZtHyZIl6dKlS77ULb9IAPQUKeFgna1ya47dIjlFW8C1EUIIUZAURWHkyJEsX76cTZs24e/vn/VJuaTVaklISMjxeYqiMG/ePAYMGIC1dfa+owqLTIT4FFn5Zgs2n75DrTKudP1+h8lyI34/yMfPV81Wt5kQQoiiKTg4mEWLFrFixQqcnZ0JD1dXZ3d1dcXeXl0vbMCAAZQpU4aQkBAAEhMT9V1ZiYmJ3Lhxg8OHD+Pk5ETFihUB+Oijj+jYsSPlypXj0aNHLFq0iLCwMEJDQ/X3Dg8PJzw8nPPn1RXijx07hrOzM+XKlTNIwt60aROXLl1i2LBhBf+C5JAMgzfiSRsGb0xWQ+Nr+Liw6s0WhVQbIYQQ+U1jYlbpefPmMWjQIABat26Nn58f8+fPB+Dy5ctGW4patWpFWFgYAEOHDmXjxo3cunULV1dXateuzQcffMBzz6UuBDt+/HgmTJiQ4Tpp7w3w0ksvceXKFXbsMP1HeX6StcDyqDgEQACXJ+fvInRCCCGEOck8QIIWlUplWSY+SWaIFkIIUTxJAPSUmjsg66nGp4eqa8Ys3XeN5lM2ce72o4KulhBCCFEkSBL0U8rO2jLLMj9tv8TRG5HsvfQAgDd+P8j6d4yvZJyUouXGwzj8SslyGkIIIZ580gJUzOmCH4Bzd6IJj4xn4a7LRMYmGZQb9ut+Wk8PY+3xW4VdRSGEECLfSQuQMNA4ZCMAR65HMr1XHf3+LWfvAjB/52U61CxtlroJIYQQ+UVagJ5i3/StS52yrvw2tFGOz11+6Aanw6OQQYJCCCGeRhIAPcW61i3DipHNaZ6NEWHppWgVOszYxrL91wugZkIIIYR5SQBUTFTydDLYLlvCPlvn/bTdcPFUDcYn3pIh9UIIIZ4kEgAVE591q6l/HFjNi/rlS2TrPG02esB+3XmZqp+uZcPJ27mtnhBCCFGoJAAqJtIGMrZWFtkKbNTzFLRZFB737wkA3vrjUG6rJ4QQQhQqCYCKicQ0q79bWmhISM5el5VWq9B+xtZslY1JTCE6ITlX9RNCCCEKkwRAxURicmoApFUUIuOSMimd6vL9WM7fidZvm1h7T2922Plc1U8IIYQoTBIAFRONK7jrH2sVhUb+JfPlujHpWnzuPkow2L5wN9pgskUhhBCiKJAAqJhwtrPWP9ZqYUTrAJ4t55bj6ySlaFl+6Dp3HsUbPZ5+2qB2X26h9w+7uHI/Jsf3EkIIIQqKzARdjDT0d2fvpQf0beiLnbUlf7/RjITkFKasOcPG07e5cj82y2vsu/yQfZcfAnB5cieuP4wzOG4qXfri3RjKl5R1xIQQQhQNEgAVI78NbcTNCMMFTW2tLBnbuTofdKxClU/W5uh64ZHxBGWSIJ12FmkryyySh4QQQohCJF1gxYiNlYXJ1dxtrbJePT69w9ciMj2enGb4/NUHWbcuCSGEEIVFAiCRgbWlhrq+blmWe/23Axn2XU3TjZaQZuTZmOXH86VuQgghRH6QAEhkYGVhwXf9nsnVuXsvP2Dt8XA2nrpNiymb8rlmQgghRP6QHCCRgVZR8HV3yPX5xlqGhBBCiKJEWoBEBtlcJSPn100/Rv6xiNhEk8eEEEKIgmDWACgkJIQGDRrg7OyMp6cn3bp148yZM5mec+LECXr27Imfnx8ajYYZM2YYLff999/j5+eHnZ0djRo1Yu/evQXwDJ5OFUwkSudVxTFrMuzbeeEedSeu591lRwvknkIIIYQxZg2AtmzZQnBwMLt372b9+vUkJSXRvn17YmJMT5oXGxtLhQoVmDx5Mt7e3kbLLFmyhHfeeYdx48Zx8OBB6tSpQ1BQEHfu3Cmop/JUWBHcjA41vJnzcr0CuX6KViE+KYWv1p3h6PUIAL7bqC6d8dfB6wVyTyGEEMIYjVKE+h7u3r2Lp6cnW7ZsoWXLllmW9/PzY/To0YwePdpgf6NGjWjQoAEzZ84EQKvV4uvry6hRo/jwww+zvG5UVBSurq5ERkbi4uKSq+fyNPD7cFW+X/Od5yrz1fqzAEzvVYel+66x97K6VMblyZ3y/X5CCCGKj5x8fxepHKDIyEgA3N3dsyhpWmJiIgcOHCAwMFC/z8LCgsDAQHbt2mX0nISEBKKiogx+BHzYsWq+X1MX/AC8u+yIPvgBmPDfCS7ejWbF4Rss2HU53+8thBBC6BSZUWBarZbRo0fTrFkzatasmevr3Lt3j5SUFLy8vAz2e3l5cfr0aaPnhISEMGHChFzf82n1eqsAzt5+xN8HbxTK/ebtuMza4+HcilTXGWtd2ZOE5BQqeDhhaSEzSQshhMg/RSYACg4O5vjx42zfvr3Q7/3RRx/xzjvv6LejoqLw9fUt9HoURZ92qg4K9Krvy+FrEey8cI/EZC17CmiFd13wA/DV+jP8c/gm/Rr6EtKjNgAHrjzAwcaKaqWLb9ekEEKIvCsSAdDIkSNZuXIlW7dupWzZsnm6VqlSpbC0tOT27dsG+2/fvm0yadrW1hZbW9s83fdpVcLRhq/61AWgSUBJRrQO4HZUPI2+2Fjg9/7n8E0AFu+9xtDm/rja29BzttqNKflCQggh8sKsOUCKojBy5EiWL1/Opk2b8Pf3z/M1bWxsqFevHhs3pn5Ba7VaNm7cSJMmTfJ8fQFeLnZoCrlHKvCrrVx/mLrMhlZbZHL3hRBCPIHM2gIUHBzMokWLWLFiBc7OzoSHhwPg6uqKvb09AAMGDKBMmTKEhIQAapLzyZMn9Y9v3LjB4cOHcXJyomLFigC88847DBw4kPr169OwYUNmzJhBTEwMgwcPNsOzfDqZY+ygNs1Nk7RabC0siUlIZsOp27Su4omrvXXhV0oIIcQTyawB0OzZswFo3bq1wf558+YxaNAgAK5evYqFRWpD1c2bN3nmmdR1qqZPn8706dNp1aoVYWFhAPTp04e7d+8yduxYwsPDqVu3LmvXrs2QGC3yV80yLhy/UXAj6JJT0gRAKQq2VvDJP8dZfugGTQNKsujVxgV2byGEEE8XswZA2ZmCSBfU6Pj5+WXrvJEjRzJy5MjcVk3kQJ/6vjStWJLnqntx6V4Mnb5VE9nfC6rClfsxLN2fP5Mc7rhwX/84KVkLtrD8kDpCbWeaY/FJKdhZW+bLPYUQQjyditQ8QOLJNOXF2nStWwYHGytq+Ljq91toNLxQ2yff7vPtxnP6x0kpWqNlxq04TtVP13LqlszlJIQQwjQJgESBqeLtRItKpfh5YP18v3ZsYgrzdlzKsP/XXVcAmLnpfIZjEbGJhJ4IJzHZePAkhBCi+CgSw+DF0+Xfkc04eTOKNlU80Wg01CzjmvVJOfTZqpNsOGV6bbdVx27hvfIkFTwccXewoZ5fCQbP28eJm1G8WK8s03vVyfc6CSGEeHJIACTyXe2ybtQu66bfLuFgk+/3yCz40fl5e2oLUUlHG+7HJALw54HrfNatpj5P6PC1CGZuOs/Hz1elgodTvtdVCCFE0SNdYCJX2lb1BKBpQMksy9pYWXBkXHuOjW9foHU6fC3C5DFd8KPzMDZ1u9v3O9hw6jbDFx4oqKoJIYQoYqQFSOTK173rsurYLZ6vZXx27fR0c/TM6v8sNx7G8d/Rmxy9HpmvdRr9x6Fsl11+6AZvtK5osO/K/Zh8rY8QQoiiS1qARK64OljzUqNyuOWwe+v5WqV5tWUF/h3ZnAOfBOZrnS7fj8260GNT154xeez4jUje//MIt6PiTZYRQgjxZJMWIGE25p65WVEU/j1y02DfxbvRvPCdOo/RzYh4fhvWKFfXvng3mq1n79KvUTlsrWROIiGEKGokABJmY2lRyAuKpfPP4Ru8veSIfltRYNTi1G607efv8c2Gc7zeukKOg5i2X24BIDIumbcCK+VPhYUQQuQb6QITZqPRaCjpqHahNauYdTI1wI4P2+bb/dMGPzo3I+IMtr/ecJYqn6wl2cTEi1nZc+l+1oWEEEIUOgmAhFn9O6o5b7WrxDd9n2HsC9WzLF/GzR6/kg4FUpdkrcLD2CSjx9YcD8/1NYUQQhQ9EgAJsyrjZs/bz1WmlJMtQTUzH1H2Zlt11JY5QopRiw8Rl5ii3l9RSErRsuXsXaITkjM9L0UCICGEKJIkB0gUGaWcUkeUlS1hz/WHcVhooHd9X6p4O/NK4/JmrB2MWnyQVpU9+HTFCYP9HWt68/1Lz2JhJKdJAiAhhCiaJAASRYatlSVb3muNVgELDXy/+TyvtQogIN3szK80Ls9nq07RuII7uy8+AGBgk/JM6FqTR/FJ1Bq/rkDqt+HUHaMzUK85Hs628/f4ev1ZOj0e5q/zIN0EjMYoikJkXFKOpxQQQgiRe9IFJoqU8iUd8S/lSPmSjkx9sU6G4AdgcDN/lr3ehF8GNdDvexSvdkU521nzXHWvQquvzncbz3H4WgSfrz7FvssP9PuvPojl+80ZF2ZNa/Ka09SduJ7QE7nLMxJCCJFzEgCJJ46lhYYGfu442KQ2YEbFpyYvT3+x8Bc63X/lof5xrzm7DI5NCz2Doij6HKI7UfEGK9L/sPUioC7wml27L97nzwPXAYhJSCYyznjythBCCOMkABJPtM51fAAY1iK128nVwXCCxaPj2zOpW81CrVd6/h+tptrYtfx75CYNv9hI5++2ExmbxOK9V/Vlrj2IY82xW4DaLZaZvj/u5t1lRzh+I5Ja40OpM2Ed8UkpBfochBDiaSIBkHiifdOnLvvGBNK4gul5hBysLanp41KItTLtzccTLZ65/Yg6E9fx0d/HDI6P+P0gS/ddo86Edaw4fCPL652/E40uz/pWpCzdIYQQ2SUBkHiiWVho8HC2zbB/2ou1aV6xFPs/CcTK0gIbqyfnrf7+X0eJik/mrT8OZ1n2UZph+NaW5p1ZWwghniQyCkw8lXrV96VXfV/9to3lkxMA5URMmgDo6oNYypYomEkihRDiafN0fisIkY6xOXqeBMN+3c/ssAtsPHWbl+bu5ka6pTqi41MDoJfm7kFRFM7efpTrpTtyKmTNKX7adrFQ7iWEEPlJWoBEsWCheTIDoA2nbrPh1G399pjlx5g/uKF+e2a6Ifa/7rzM+P9O0rWuD9/0fcboNX/ZfomDVx8yo09drPLQMnbu9iN+2KIGP2mT0IUQ4kkgLUCiWHgyw5+M7kUnEHYm42SMOuP/U4fSrzh80+QkjBNXnmTl0VusP3nb6PHsikmUUWdCiCeXBECiWCjn7kAdXzeaBmRv1fmi6viNKAbN25etss9OWs/R6xEmj8dmEsAcuPKQsSuOZzq/UNqgUitLfgghnjASAIliwcJCwz9vNOX3YY34onstAL7uY3zCRGe7p6dneMRvB00es8wkL6rn7J0s2HWFaaGnTZZJ262YksW8RUIIUdRIACSKDY1Gg0aj4aVG5Tg1sQPdnylrcNzX3Z5Frzbi3fZVzFTD/Jd2tfq7jxIMJlgcveQw5+9E67djEpKJik8ySKBOezy9tGlVsuirEOJJ8/T8qStEDtjbWBpsl3GzZ9v7bQE4G/7IHFUqECUez4odduaO0a6zwK+20LmOD1N61qL911szjDJLT1EUNEYSyq8+iGXNsXBeaVIed0dZ1FUIUfRJC5Ao1ip4OAIQVMNbv8/BNvXvgnVvtyz0OuUnWytLDlx5mGne0H9HblJ9bKjR4OdGRBzfbz7P3ksPeH3hAep/toEz4Y/QahWDLrCes3by9YazvLfsSL7VXfKKhBAFSVqARLG29LUmbD17l+drldbv61rXh1VHb9GsYknc0q0rplO/fAmDBVCLqjO3H9Fz9s5cn3/tQRzTQs8Y7AuasRVnWyveaV9Zv083I/XOC/dzfa+0zt+JpufsnQxr7s+odpXy5ZpCCJGWtACJYq2Uky09ni2LnXVql5itlSW/DmnI8JYBeDrbUcnTKcN5bg7Gu3lsrCw4PalDgdW3qHiUkMyE/zKuXp+X+SZDVp+ix6wdJCSn8M3Gc0TGJfHl+rN5qKUQQpgmAZAQWeidZkmNhv7uAPRvXI7BzfwylK1VxtUgmCpu0k84mZSiZeHuK1y4qyZTa7UKD03MT/TD1oscvBpB62lh/HfkZoHXVQhRvEkXmBBZ6N+4HBtP3yawmhcDmvhxKzKO8iUduWkkZ0ZJNxx82ou1ufYgFn8PR95eknV+zKz+z/LG76aHrhd16fOjf915mc9WnQLgnecqc+jqQzafucvqN1tQ3cfF6DVkVXshRGGQAEiILDjYWPHH8Cb67fIl1cRpTSbzSx8e+xyP4pPxdVcXJ91gYtblJcMb0+fH3QDYWlkYXdm+jJt9lqOzigqNRsOKwzd4xrcE5Uo6sPfSA/2xr9J0Z/2254p+PqasmBp5llOxicmcDn9E3bJuT+zacEKI/CNdYELkUgkjCdK69h83Bxt98ANQx9ctQ9lOtUrTqEJJfd7MipHNsDLyxfzrkNS1v7rV9clTnQtaZFwSb/1xmJbTNvPngeusMxH4paRkf4TXuTvRjF1xnBpj13InKvetQwN/2UuPWTv5Y9+1XF9DCPH0kBYgIXKpfQ1vej5blmfLuzFm+XEATE2I7OFsy9b32uBga0l0fDKrjt3ilSblATjwyXPceZRAFW9njt+INDhvSDN/gxmbBzXz55/DT0Z+zLuZDInPyczRHb/Zpp9oseEXG7k8uZPB8UV7rvLD1gvMH9wQ/1KOJq+z77I6au+PfVd5qVG5bN9fCPF0kgBIiFyytNDwZW91OQ1dAGSsC0unXEm1RaiUky3BbSrq95dwtKHE48kD0wY7f77ehPp+7gazMXtmcv207K0tiUsquouV/nngOu+2r4K3qx1frD7F3UcJJsumn2U6Kj6JPRcf0DSgJI62Vny8/BgAk1aepGYZV2r6uNA+zbxO6UnnlxACJAASIl/MG9SAeTsvM7FrjTxdx9oy9etZFwwlJqcuTZFZgAUwoEl59l56wPguNej7OLcorTplXTlyPdLImYWvcchGXmtVgR+3XszRebXHrwOgRaVSLBzaSL9/0+k7bDp9B4B5gxtQ08cVD2dbfthygcV7r6ZeIJf5RFfvx1LK2QYHG/lvU4ingXyShcgHbap60qaqZ56vY2lhkeax+kVd2cuJqt7OeDjbYm1pgY+rHTdNjJSa2LUmoLaSGFPW3YFGFUrmOOgoKD9syX09tp27x9gVx40eGzxvH24O1iwc0oiQNaYXdE3rzqN4jt+IpHVlTywsNCSnaLGyVH8fZ28/ov3XWynjZs+OD9vmus5CiKJDkqCFKELSJkHrAiArSwtWv9mCBY+TocPea5PldeysjM9F1Ll2aT7qWJWST8l6XQt2XTF5LCI2ic4zt2fYb6r9p92XWxgyfz/LD91gyb6r1Bq/jp3n7wGw/nEyd0GMxrtwN5r4XHZXRsYmGSx4K4TIPgmAhChCSjnZ4uZgjbOdFRVKpc5AbWGh0Q8Ft7HK+LGt6+vGiuBm+u20XWnDW1ZAo4HgNgF0qFkajUaDs13xbfw11QP2KF4NJFYcuckHfx0jLimF1347ABjmZuWn7efu0e7LLXT7fkeOz41PSqHOxHXUHBeaYf4pIUTWiu//gkIUQfY2lmz+X2ssLDQZVqxPq2lASf26W3V93fhtWCOc0izimnbenMBqXvyvfWVs07QKFeevy6xCma1n7+of6/Kv0sY/Cckp/Lz9Ei0reVCzjKt+/5S1p1l97Ba/D2tE2RKpUyBk5u+D1wE4Hf4oe5VPI+2EkQnJ2mI9A7kQuSEBkBBFTIlsdE992+8Zftt9hV71fSnjZm+0zP+eq8zFezE08CuRYSLB11oG6EdP6YxoHcDssAu5r/gT4uDVCPw+XEXnOj4EVvMkIjaJZhVLGS2bkKzlTlS8wRIfVT5ZC8DUtWe4PLkTS/ddY/q6M9x5PJKtw4xt9GvoS5c6ZahV1tXodXXyMsFj2u7S+KQUCYCEyCEJgIR4ApVysmV0YOVMy2S2inq/hr408CvBc19v1e97q10lgmp456o75kn035Gb2VpzbOSiQ3SoaXxY/aV7Mbz/11GDfdEJyczddom52y7Rr6Evn3erpZ95+k5UPA9iE6nqrS4DkpeetbSxU1xSCm65v1S2RcYlsWz/NV6o7YO3q10h3FGIgiM5QEIUQxqNhkpezgb77KwtqWtkxmqAN9MEU/XLlyjIqhU5ey8/4Mj1CKPH2kwPy/TcxXuv0f+nPfqWtYZfbKTDjG1cexALZFw8FtTE5m3n7vLV+rPsetzNaUza+ZHiEvM251NcYkqmczHpfLz8GJ+tOsVLP2WcYkGIJ420AAkhMlXR04l3nqvMyDYV2XnhHg393ak+NtSgzLb329Bi6mYz1bDgrcjD7Nu7Lt5n18X7BFZLnSbh+I1IyrjZY2HkT9Cu32/n8n01QPp247kMM1+vPnaLcu4O/LEvdW6jvE562ThkI5FxSez9uB2eLqZbdnRr2l28G5On+wlRFEgLkBDFWL+Gvtkua2NlQesqnkYnAky77lleuT8lQ/TTS9vdOOL3g3T9fkeGpVMOXn2oD37S0o3y2nPxPm/8fpAXvtvOb7vTBECPW4Cu3o9l/o5LGYbVJySn8N+Rm9yPNt7KExmnzhu1J83itcYU5+R58fSRFiAhirH3gqpyJyqBXvWzHwgVtF8GNSgWeUjHbkRyLM3abydvRtFj1s4M5Zbuu8bU0NN82buuwbIoaelagNp+GUayVuHOI/V3WsLBGjcHG2aHXWDGhnNU9nJi3dutTNZJm9Vw+hxEQFHxSfy09SJd6vpQ0dM56xOEKGTSAiREMebuaMPPgxoYJPmue7sln75QXb9dp6xbrq7tmMkwflNaVCplMJw/OwY388vxfYqizWfuGN3//l9HuRedyMBf9pKi1Roto2sBSn6cFzQr7AJtpofxzKT1AISeULuuzt6OznTOoKQUhbErjrP8kDo8f+3xW3y+6iTax9fNMkBKY9J/J/l203kCv9qadWEhzMCsAVBISAgNGjTA2dkZT09PunXrxpkzZ7I8b9myZVStWhU7Oztq1arF6tWrDY4PGjQIjUZj8NOhQ4eCehpCPFUqezkztLk/695uyRutAxjbuXrWJxmx5q2WtKzsod+eO6A+i4Y1yuQMtfXH0yV7C76q92jBuM41WPd2S8qWMD4dwJNix+NZpzOTrDUegMQlpXDPSPeWosDtqHhc0kx8+fP2Syavv/Z4OAt2XeHtJUcAeP23g8zddomVx26p18uyhqkOXHmYg9JCFD6zBkBbtmwhODiY3bt3s379epKSkmjfvj0xMaYT7Hbu3Em/fv0YOnQohw4dolu3bnTr1o3jxw3XBOrQoQO3bt3S/yxevLign44QT5XKXs6836EqrvbWmZZLOwO1zokJQZQr6cDELqmLw7o72lDHxCgzHWtLC5yz0QL08fNVeb9DFaqVdtHXNW2w9STamcmILx1To8KO34ik/mcbjB5r9MVGg4kWP1t1CoDYRHXm6+82ntMfux9jPEfoTpQ66WJ2ZpyOTkgmMVlLSrqyp8Oj+HzVSR7GJHInKp61x8P1LUvFWXxSCqduRcls3mZg1hygtWvXGmzPnz8fT09PDhw4QMuWLY2e880339ChQwfee+89ACZNmsT69euZOXMmc+bM0ZeztbXF29v43B1CiPzRt4Gv0aDG8XEQY51m2Q4LDVilWaLj5cblSE5R+GPfNYNzNRoNh8c+R92J603ed3jLgAz7Pn6+Gl7OdnSq7f3UdrtsO2e8lWjuNtOtOpCa5KwzPfQMMzefz/H9s/qKfhSfRK3x64we6zBjG6DOYL3n0gPuPkrgs241aV6xFCN+P8jrrSrQtW6ZDOcduvqQc3ei6Z2LPLWj1yNYuOsK7wVVyXR0mzn1m7ubQ1cj+P6lZ+lUu7S5q1OsFKkcoMhINSHQ3d3dZJldu3YRGBhosC8oKIhdu3YZ7AsLC8PT05MqVaowYsQI7t/P+q8rIUTuVSjlCEANHxf9Prs0AZBGo8E6zbjvrnXLMLlnbaPXcnPI+UgwJ1sr3gqsZDLhNj+W8wqs5pX3ixQB2Ql+bkfFZ9iXVSPF0euRmRdATf7WzTk0Zc1pWk8P49StKN7647DR8t1n7eT9P49mOieSKV1m7mDZgeu8/9dRImOT6P3DLubtuMSCXZcLZGHb3Dh0NQLAYFoDUTiKTACk1WoZPXo0zZo1o2bNmibLhYeH4+Vl+J+Ql5cX4eHh+u0OHTqwYMECNm7cyJQpU9iyZQsdO3YkJcX4XBkJCQlERUUZ/Aghspb2C3HB0Ia81rICPw2sr9+Xdj2zpBStfkZkMJzIz5hv+z1DQ3/TfwxlZuHQhhlWvL8Y0slE6exrXeXJ7mbLiQ/SzXBtyrL91wj8agu7LtzPMkACw0TqR1msZJ+2W+jiPeMj4NKXM+bc7Wh6/bCTvZceMOG/k4xdcYKeRkbcFZTohGS+2XCO83dMr/mWkwRzkT+KzDD44OBgjh8/zvbt2/N8rb59++of16pVi9q1axMQEEBYWBjt2rXLUD4kJIQJEybk+b5CFGdlSzjw0fPVDPbZpVmAVbewqE5Wo7261PGhSx0fft15mbAzd9h85m6m5dNqUcmDQU39+HL9WaPHW1X2YMvZzK83b1ADBs/fZ7DPu4h2o+QXXWsEQFia1/ubjecY2NTPoGyHGVu5fD+G+CT199pv7m59K2B6a4+n/oFqYiCbUZtOp46M06Dh2oNYbkTE4WpvzY2HcUwNPU2PZ8vy07ZLTH2xFm2rGm+hM9baE26khUvnUXwSFhqNvis3N5JStNyKiKdcSQcmrznFb7uv8vWGsxkmttTJ6g8Ckf+KRAvQyJEjWblyJZs3b6Zs2bKZlvX29ub27dsG+27fvp1pvk+FChUoVaoU588bb/b96KOPiIyM1P9cu3bNaDkhhCEli6yQtC0+CclqC+yUnrV4s21Fg5XUMzOwqR/zBjfMcd2MrTM65+V6DGnmz2stK2Q4FjraMO+wfMmMkzu2q+bJd/2ewdoyH/rTniCP4pNpPS3MYN/p8Ef64Efn4j3jA1he/+2A/nFOup5+SpPbNDX0NC2mbqbvj7vp+M02hi3Yz9nb0Uxec5p70QkMmb8/29fNTHxSCrXGr6PGuNA8JSa/8vMeWk7bzNazdzlwJSLL8jkJDEFt9Tp+I5LoLFrRhGlmDYAURWHkyJEsX76cTZs24e/vn+U5TZo0YePGjQb71q9fT5MmTUyec/36de7fv0/p0sYTzGxtbXFxcTH4EULkL8vH+T99GpTjnfZV9Pu/6l0HjQZm9X82X+/XuY5Phn0danoztnN1AjydMhxzc7A26HJLP+N1ZS8nNBoNnev4cGJChwxdbE+7wsiZWXv8Fq8t3E+LqZvw+3AVuy6m5v1ExCZlcmbuXL0fy8LdV/TBORg+T1PTDmTH7ovqrNoLd1/JViCVftRcVjaeusML322ny8y895oUV2btAgsODmbRokWsWLECZ2dnfR6Pq6sr9vbqnB4DBgygTJkyhISEAPDWW2/RqlUrvvzySzp16sQff/zB/v37+fHHHwGIjo5mwoQJ9OzZE29vby5cuMD7779PxYoVCQoKMs8TFeIp07mOD/8ducmQ5ln/0fLOc5U5cTOS5hVLGT3e49mydKpdGlurnE+cmJnyJR0p5+7A1QcZl5bwcrFjzVstiE1MoedsNRfE2tKC8MjUbhE7a8O/Dxe92lj/2MbKAmvLgv37sbSrHbciTXfTPI1e/+1god6v5TR1/br70Qm83iqA2MQUg9m2U7QK1pbqlAHGloDJjoRkrcE0BDrJKVqDbl1j8zgBaLUKF+/FEODhiCZNs+bywzcAdV22+KQU7Kzz9/NTHJi1BWj27NlERkbSunVrSpcurf9ZsmSJvszVq1e5deuWfrtp06YsWrSIH3/8kTp16vDnn3/yzz//6BOnLS0tOXr0KF26dKFy5coMHTqUevXqsW3bNmxtsz/BmhDCtG/71uXEhCCqemfdWvpmu0r88Ep9LDMZhpWd4Ofbfs9gkcOWIjcH03MYVSvtYjB5oqVGY9CdkP4LpZST4f8frzQpD0DTgJLULOOCu6MNpZyy3yr0TDm3TI+nz6cSBWfn+fu0nLqZZyet57WFqd11WkXhy3VnqDkulBWHb3ArMo4wIzN2xyYmm8zh2Woi1+yXHZd4dUFqt92V+7GsPR7OuduGwdKE/04Q+NUWZoVdMNifkpJ6v9AT4YicM2sLUHaaBcPCwjLs69WrF7169TJa3t7entDQUKPHhBD5Q5PHBNHc6FLHh441vXPU8lLDxzXTodlpn4OttYVBAGSbZgj/z2lGtum83iqAZ8uVoI6vKxYaDRYaDYFfbQESs6xXVW9nlr/RDL8PV5ks06lWaVK0Wr5af5ZrD4rGkO2iTlEUg1aS7Lp4L8ZoC0yKVuG7TWru6MxN57n6IJaEZC0/D6xPu8dTIkTEJlJ34npql3Xl35HNs503tOpYxqBFlyuVNlH6111XAJgWeobgNhUBWHn0JmvTBD3pBxhk5sLdaL7fdJ7gthUJ8MjYFVycFIkkaCGEyI6cdjt99HxVRrQOYM1bLYwed7K1Yv7gBvw6pCF21paMDqwEwEuNyhl8kXoZGf1laaGhSUBJHGyssLO2xMbKsG6+7oZLc/yTZsbsTrUyn/AusJoXlhYauj9TluqlU1vZ0s6xJDKq8PFqWkzdxPEbWc9HlJbp7qfUx+fuRJPwOND4ekPq6MKtjyenPHo9kr2XHuD/0WpGLj6U5T2PXIvIUR11FEVh5CLD62c2guzOI3XWbV2Zl+bu5u9DN+j+/Q6Cvt7KVyZGShYHRWYYvBBC5DcXO2s+6FA10zKtq3jqH49oFUCbKp5UepwkPfXF2lx/GJftEWtpGx9aVPJg0Z7Uye3q+rqx4Z2WHLkWSZe6GRO0PZxtaV3Zg3bVvAwWp03boPDb0Eb6BU5FRooC1x7E8cJ3+ZMYvPH0baP7j99InSvOMs0vvfcP6oS8q47eynBOTiSlaI0G+4/ik4zm+mSWQN1hxjYexCQyrnN1Bjfz53aUGuxFxScTFf+IM7cf8c5zlfNU3yeVtAAJIcRjGo2GaqVdsHr85dO7vm++fjlU9HSmZ72y+i83XY6SnbUFez9ux7RedQyCHzBs9SqRbuRZs4ols7znRx0zDwDfC6qS6fHi7J2lR0wei09K4Uz4I3ZfzP9VBhp/sZH4pBSi4g1Hvp24GUVSSsburszWVHsQo3bJrjkezsqjN/OtjumXV3kSSQAkhBD5ZHxndfHXEa0DsjVp4uJXG9OqsgfLXmtqMnflw45VKeNmzyedMiZF/zywQYZ9+z8JZFRbNVcksJonr7UKYHAzPwA+6FBVf51BTf04/3lHgttUzJDgLbLW7sstBM3YysLdV3J03n0T3W0GZWISOX4jkn8O3chwzFi+T9ousG3n7hrt0tt76UGGrjOdA1ce0vm77Xz419EMs1Unp2j56O9jrDicWpcNJ29TZ8I6Pl91MsvnYszGU7d5Z+lhYsw8h5F0gQkhRD5pU9WTY+Pb42xnTVxiSpb5FdVKu/DrkMwnefR1d2DHh22NHrOztuSf4GasOXaLH7ZeBNTRaqMDK1O9tAuNKqgtROM612DsC9X1QdbztUpT2tVOvz1/cAM++ec4h3OZl1Ic5XZepN4/7GJiV9PLPWUl0UgLkG6+ov2XH/DKz3txsLHk5MQOGVqQTBk8by9R8ckcuxHJH/uuGSRhrz4ezuK9V1m896p+sdpJjwOfudsuMaZT9RzV/0z4I4b+qo5+K+/uyFuP8+7MQVqAhBAiHznbqd1a9jaWvNpCnScpqyHvOfHx82qX1le96wBqblFdX8PrW1po6FirNO5puszStjD5uNkbbNcs42qQpJ0T03vVYfO7rbkU8nyuzs+pXAzyKlIu3I2h/097siz34pxdjF1xwmDfxlO3CTudcVh91OPuKN2SI7GJKdx9lEDt8euyVaeoeMOWmIjYRB7EJPLj1gv8nqaFKy8zYwOcuBlJ0Iyt+u0HMVm3hhUkaQESQogC8m5QFeqVL0GTCsYngcyN4S0D6NuwHC52puc4KiyvtazAi/UyX77IGGdbK4OFULOzNpuOm701DwtgVugnwdw0S4Ok9e2m81T3ceGn7anHczI3kJWFxmDW6x+3XuTI9Qh2nDfMb/L/aDWTe9Ti7qPUwCUhOSXDPF73oxMIPXGbbefuMq5zDbxd1e7gsHTr+RX2VBrpSQAkhBAFxNbKkg41Mx/ynhvpg5/CbhV5L6gKrzQpn+sgrEYZF/1SEWA451Jak3vUom45NzrM2Kbf90mn6vxvmenk5OIq/SzaeRmJln7SxbQ+/PuYwfaNh3FUeDyf0KSVJ3kYm8jfB1PzhSLjkvSzqKdvQTJ3ACRdYEIIIQzUL1+ClaOa42dkQViA4DYV89QClX7emkpexifkq+HjSkUPJ3STiL/ROoCe9crS45kyub53cbErB6PT8rLm2ZnHy3xotQo/b79kEPwA7LyQWo/0PWg2BbycTFYkABJCiCdcZS/nfLnOfyOb0/2ZMnzb7xlqlnFl6etN6NewHCtHNafr47mLOqYbpp9WnbKm50tK+2WXmGL4Taib4Tg9K0sNVpYWHB7XnmPj2/P+4zmdvuxdhyPj2mf7eaXXKxfddsK4Eb+rLU8fLz9mssyy/ddQFIXl6Ua1GUvoLkzSBSaEEE+4Ch5OLBnemFLOeRvOXqusK1/3qavf9nS2I6RHLQBCetSiQw1vWlT2MHn+3AH1Wbr/GlW9XRi2YL9Brs8Pr9Rj8Px9gOFQ8EqeTiYXGrW2VJt+Mnb5aXC1z14L1OBmfszbcVm//VXvOvR4tizLDlzP1vkia3cfJfDHvmsmj3+26hT2NpZcvBdjsD8hB0t4FARpARJCiKdAowolC3RtJwcbKzrWKo1TJnkbni52jGxbicDqXqx+swXbP0gdvm9lqaH7466r4S0rpNlv+DWUdrkPN4fsLy5rSvqZwHWzetvnYfV0BxtZeT2tiNjM17+LjEsyOgdRQnJKQVUpW6QFSAghRL6rnm7dsmqlXZjSsyRDm/tTw8dFP8TbIl0Ct721JT++Uo/4ZG2eJ2hc8HiNNx1LC42+u7Bl5VKEnjC+1IUpk7rVJLCaJ01CNuWpXvkpctdSYs/uIunBdTRWNtiWqUaJVoOwLpl5N1/M6e1EbPuN5MjbWJfwoUTrQdgHpE6sGbH9d2JObSPl0V00FlbYeFfEreUAbH1SZw6/PnsIKVF3qDwl9bpurQbi2jjjYuVJD29ya/5boLGg3OglAIxqa745gEACICGEEAXo4KfPEZOQrA9m0q+rln4Em5WlhvY1TOcZZWVoc3+2nL3L4GZ+tEzXXde+upf+ca96vkYDoE61S+tHUH38fFWeKVeChbuu0MDfnVcalwegW10f/jmc92UlAjwcuXA3JuuCmYi/dhznZzth410JlBQitizg9tJP8Rk6Gwsb47ORx18/xb1/p+LWaiAOAQ2JORnGnb8/p/SgGdh4+AFg7V4G9+dex8rNGyUpgUf7V3B7yaeUeW0ulg6pv0PX5v1xrtNBv62xsU9/O5SUZO79Ow3bstVJuHFavz+z1sTCIF1gQgghCoy7ow2+7sZHkwFoMIyA8jq/kV9JBza804r+jcpnOGaRJtpqV82TH16pZ3A8uE0A37/0LG8HVqZe+RK80tiPBn7ufNvvGX3wA/B+h6p45DDfqqq3M/+ONJxsMj8CAK/eE3GqFYiNR3lsPCtQstPbpETdJfH2eZPnPDrwL/YV6uHaqCfWpXxxa/kKNl4BPDq4Ul/GsXpr7P3qYu3mjY1HeUq0HYaSGEviHcO5iCxsHLB0KqH/MRZ0RWxbiHXJsjhWbZHn55ufJAASQghhNrousCk9a1HFy5mxnXO2tILOm20rUquMKz0zG+GVJtbSaDQE1fDm9KTU1oumAeqElW8FVuKvEU2xN5Hr4+Nmz96P2xns+2N44wzdeWn9N6o5NXwMW79MXT8vtAlqi5KFnel8sIQbp7ErX9ewLv7PGrTOpKWkJPHo8Fo0to7YePobHIvcs4xr3/Tj5rw3idzzF4rWMK8n7soRYk9vx/25Ebl4NgVLusCEEEKYjZOd+jXUp0E5+jQol+vrvNO+Cu+0z3xle2PxiZ21JTs/bMvFuzE0q5j9Gbs1Gg0fdKjKlLWn+XlgfRpXKIlrJrNUWxuZ86ZDDW+DCSHzwsXOisi4RB5unIttmer6rixjUmIeYunoZrDP0tGNlJgIg32x5/dy79+pKEkJWDqVwKvPJIPuL5d6nbHxDsDCzpmEG6eI2PIrKdEPcG/3qnqfuCjur5pBqc7/w8LWdCuguUgLkBBCiEL3Za86VPJ04rNutXJ9jW/7PYODjSXzBjXIujCGXWBp+bjZ07xSzpcrGdE6gJMTg2hXzSvDseMTgvSPP30htVUrbWL3K038mPnSM1nex9RM2WlZWmh4sG42iXevUKrL+1mWzw67crUpPfhbvF+ehp1/Pe6umGIQJLk07I5dudrYePrj/MzzlGg7lEcHV6Ikq0Hg/bXf4Vi9FXa+GRd/DaqR8TUrbBIACSGEKHQ965Vl/Tut8C/lmOtrdKnjw7HxQbSp6pmt8gWxZEjaOYzSLjCbNr+njFtqXsy8QQ2o6u3M/MENsLTQ8EJtH4PrDWvuz6mJHQwCIxtLC3zdMyYXpxW+dhZxF/bh1e8LrFxKUb20i8mylo4lMrT2pMREZGgVsrCxw7qED7ZlqlLq+bfQWFgQfdT0Aqs2pauANoXkSDW5PP7KUaL2/s2VqV24MrUL99d8i5IQw5VpXagVnXFYfGGTAEgIIcQTyzKzxJt0avqYnqm6IKWd8LhWWVfWjm5J6yqpQdsvg+rrH7/9XGXsbSwNAiMvVzv+eaMZPw2oj3O6xGlFUXiwfvb/27v3qKjKfg/g3xlgBgYZhovAoICgiIpKXpDm9VImh0vW8UIpxnFhp+JV0ZOl1mqVQn+0NH2PnVPLKM8pqfd0tOikmSW+iIJHQzTCO7E0KUtF8kIMCIrwO38YW3fipfPCjLC/n7X2WjP7efae55kfs+fHnmfvB79W7EZg6mtws1y7gu5fJkTe8o7Xxl4D0PTjftW6ph/KYew1APE3nM26aaJbEUjLrSeivVJzAtDpof8tkbL+019gffJNZbGMSYPO4IHlf81HWupjt9yPozABIiKibm3z/DF4MWkA0v/Up1Nf57n4a/e1mTZSnTjcKUmLDLg+lcmNZ6lyn4zFsFALctKGw6+HEfGDArFlgfpKqqaiNag/UoR33vsA02z98aytJzb9czQe6OuN6bEhAIBzm/8VF4tzlW28RvwjGqu+Rd3ez9B8/ifU7voIl6uPw2v4I/jL40PRz8cVoSc+x2MhjSjMGITL1cdx7qt/Q4v9PExRYwAAl09VoG7f57hScwLNtdWoP7IDF7f/JzyjH4TLbwOw3fxDYOjZR1lcvPwAnR59owbCx8fnj725nYCDoImIqFsb3Mv7pvsPdYaZtmv3HgrxuTbg98/jIlB+shYP3eEnOovp+qX/rvrr5yUejApQnSkCgN4+JsSF+6K06trg6Zq9XwAA0lMmquqtXbsW/cc8AgC4WvcLoLu+3yHDY5Ecvgr/9fYKXNz5Idx8ghEw9WUYevaBl7sbNj/7AJ544l2kpKTg3Llz8PCywNMaidzNf8OiomtXmQ3q7YddhTtRu/u/gZZmuHoHwjxyEsyxU+74PrVNceJsTICIiIg6SJjf9TFNLz088K628XJ3w8cZ98NFr4PhLgY833iWSH4/xfoNLjZcm6Ii6InlCPf3RNVvc3H9e+owDAp+ADNSpyHjr2WqbfQ6wN3dHZ999lm7+3QNPIUaexOSB4/HWLl1Yjeuf09EBfbAf/zv9fsG9RgSjx5D4lVjpZyJP4ERERE5WVyEH0b28b2rur+/eeSt+Hga8LfnxmHe+H5Y98z9yvrg3wZl/8OgQGSO76ve9x2Sk8nDeiFjXF94uavPn0y6Tz2Y2+iqR7Dl+sDtmBCL8vhys3PnAGvDM0BERETdVP9ALyxKvHZ/pK0LxuHy1RZlklmdTofFiQMQ28cXs9bu+0P7vfEmjsumDkFqbAj0Oh02lJ8CAPj3MKiuRPvkz/cj6pV8AEAjEyAiIiJylKggr3bXW71vf4l9eww33NgxzNcEnU6HVdNi8Ke+fvji4BksThwAX08DVk2LQbi/J4yu1xOm202N4khMgIiIiLqQjh5CExXkhfkP9UOguf3JU9tvgw6psSE4eeESRoX7KuseHxmCx0eGKPWmDr9+Rdz/zLHh0M+/4sHfTVLrLEyAiIiIuhBTJ8whtvAO04i0Z3nK0D9Uf0SYL0aE3d04J0dgAkRERNSFLH0kGt//0oCnx4bfuTLdEhMgIiKiLiTUz4Qdix50djO6PF4GT0RERJrDBIiIiIg0hwkQERERaQ4TICIiItIcJkBERESkOUyAiIiISHOYABEREZHmMAEiIiIizWECRERERJrDBIiIiIg0hwkQERERaQ4TICIiItIcJkBERESkOUyAiIiISHNcnd2Ae5GIAADq6uqc3BIiIiK6W23f223f47fDBKgddrsdABASEuLklhAREdEfZbfb4e3tfds6OrmbNEljWltbcfr0aXh5eUGn03Xovuvq6hASEoKffvoJZrO5Q/dNfz/G597G+Nz7GKN7W3ePj4jAbrcjODgYev3tR/nwDFA79Ho9evfu3amvYTabu+UfX3fB+NzbGJ97H2N0b+vO8bnTmZ82HARNREREmsMEiIiIiDSHCZCDGY1GZGVlwWg0Orsp1A7G597G+Nz7GKN7G+NzHQdBExERkebwDBARERFpDhMgIiIi0hwmQERERKQ5TICIiIhIc5gAOdDq1avRp08fuLu7Iy4uDnv37nV2kzQhOzsbOp1OtQwYMEApb2pqQmZmJvz8/NCjRw+kpKTg7Nmzqn2cPHkSEydOhMlkQkBAABYvXoyrV686uivdws6dO/Hoo48iODgYOp0OGzduVJWLCJYuXQqr1QoPDw/Ex8fj2LFjqjoXLlxAWloazGYzLBYLnnrqKdTX16vqHDx4EGPHjoW7uztCQkKwYsWKzu5at3GnGM2aNeumz1RSUpKqDmPUeZYtW4bY2Fh4eXkhICAAkydPRmVlpapORx3XioqKMHz4cBiNRvTr1w+5ubmd3T2HYQLkIB9//DGef/55ZGVl4dtvv0VMTAwSExNRU1Pj7KZpQnR0NM6cOaMsu3btUsqee+45fPHFF8jLy0NxcTFOnz6NqVOnKuUtLS2YOHEirly5gq+//hoffPABcnNzsXTpUmd0pctraGhATEwMVq9e3W75ihUr8Oabb+Kdd95BaWkpPD09kZiYiKamJqVOWloajhw5goKCAmzevBk7d+5ERkaGUl5XV4eEhASEhYWhrKwMK1euRHZ2NtasWdPp/esO7hQjAEhKSlJ9ptatW6cqZ4w6T3FxMTIzM7Fnzx4UFBSgubkZCQkJaGhoUOp0xHGtqqoKEydOxPjx47F//34sWLAATz/9NLZu3erQ/nYaIYcYNWqUZGZmKs9bWlokODhYli1b5sRWaUNWVpbExMS0W1ZbWytubm6Sl5enrKuoqBAAUlJSIiIiX331lej1eqmurlbq5OTkiNlslsuXL3dq27s7ALJhwwbleWtrqwQFBcnKlSuVdbW1tWI0GmXdunUiInL06FEBIPv27VPqbNmyRXQ6nZw6dUpERN5++23x8fFRxefFF1+UqKioTu5R9/P7GImIpKeny6RJk265DWPkWDU1NQJAiouLRaTjjmsvvPCCREdHq15r+vTpkpiY2NldcgieAXKAK1euoKysDPHx8co6vV6P+Ph4lJSUOLFl2nHs2DEEBwcjIiICaWlpOHnyJACgrKwMzc3NqtgMGDAAoaGhSmxKSkowZMgQBAYGKnUSExNRV1eHI0eOOLYj3VxVVRWqq6tV8fD29kZcXJwqHhaLBSNHjlTqxMfHQ6/Xo7S0VKkzbtw4GAwGpU5iYiIqKytx8eJFB/WmeysqKkJAQACioqIwZ84cnD9/XiljjBzr119/BQD4+voC6LjjWklJiWofbXW6y/cWEyAHOHfuHFpaWlR/aAAQGBiI6upqJ7VKO+Li4pCbm4v8/Hzk5OSgqqoKY8eOhd1uR3V1NQwGAywWi2qbG2NTXV3dbuzayqjjtL2ft/usVFdXIyAgQFXu6uoKX19fxsxBkpKS8OGHH6KwsBCvv/46iouLkZycjJaWFgCMkSO1trZiwYIFGD16NAYPHgwAHXZcu1Wduro6NDY2dkZ3HIqzwVO3l5ycrDweOnQo4uLiEBYWhk8++QQeHh5ObBlR15Samqo8HjJkCIYOHYq+ffuiqKgIEyZMcGLLtCczMxOHDx9WjWuku8MzQA7g7+8PFxeXm0bgnz17FkFBQU5qlXZZLBb0798fx48fR1BQEK5cuYLa2lpVnRtjExQU1G7s2sqo47S9n7f7rAQFBd108cDVq1dx4cIFxsxJIiIi4O/vj+PHjwNgjBxl3rx52Lx5M3bs2IHevXsr6zvquHarOmazuVv888gEyAEMBgNGjBiBwsJCZV1raysKCwths9mc2DJtqq+vx/fffw+r1YoRI0bAzc1NFZvKykqcPHlSiY3NZsOhQ4dUB/SCggKYzWYMGjTI4e3vzsLDwxEUFKSKR11dHUpLS1XxqK2tRVlZmVJn+/btaG1tRVxcnFJn586daG5uVuoUFBQgKioKPj4+DuqNdvz88884f/48rFYrAMaos4kI5s2bhw0bNmD79u0IDw9XlXfUcc1ms6n20Van23xvOXsUtlasX79ejEaj5ObmytGjRyUjI0MsFotqBD51joULF0pRUZFUVVXJ7t27JT4+Xvz9/aWmpkZERGbPni2hoaGyfft2+eabb8Rms4nNZlO2v3r1qgwePFgSEhJk//79kp+fLz179pSXXnrJWV3q0ux2u5SXl0t5ebkAkFWrVkl5ebn8+OOPIiKyfPlysVgs8vnnn8vBgwdl0qRJEh4eLo2Njco+kpKSZNiwYVJaWiq7du2SyMhImTFjhlJeW1srgYGBMnPmTDl8+LCsX79eTCaTvPvuuw7vb1d0uxjZ7XZZtGiRlJSUSFVVlWzbtk2GDx8ukZGR0tTUpOyDMeo8c+bMEW9vbykqKpIzZ84oy6VLl5Q6HXFcO3HihJhMJlm8eLFUVFTI6tWrxcXFRfLz8x3a387CBMiB3nrrLQkNDRWDwSCjRo2SPXv2OLtJmjB9+nSxWq1iMBikV69eMn36dDl+/LhS3tjYKHPnzhUfHx8xmUwyZcoUOXPmjGofP/zwgyQnJ4uHh4f4+/vLwoULpbm52dFd6RZ27NghAG5a0tPTReTapfBLliyRwMBAMRqNMmHCBKmsrFTt4/z58zJjxgzp0aOHmM1mefLJJ8Vut6vqHDhwQMaMGSNGo1F69eoly5cvd1QXu7zbxejSpUuSkJAgPXv2FDc3NwkLC5Nnnnnmpn/mGKPO015sAMjatWuVOh11XNuxY4fcd999YjAYJCIiQvUaXZ1ORMTRZ52IiIiInIljgIiIiEhzmAARERGR5jABIiIiIs1hAkRERESawwSIiIiINIcJEBEREWkOEyAiIiLSHCZARER3QafTYePGjc5uBhF1ECZARHTPmzVrFnQ63U1LUlKSs5tGRF2Uq7MbQER0N5KSkrB27VrVOqPR6KTWEFFXxzNARNQlGI1GBAUFqZa2WcN1Oh1ycnKQnJwMDw8PRERE4NNPP1Vtf+jQITz00EPw8PCAn58fMjIyUF9fr6rz/vvvIzo6GkajEVarFfPmzVOVnzt3DlOmTIHJZEJkZCQ2bdrUuZ0mok7DBIiIuoUlS5YgJSUFBw4cQFpaGlJTU1FRUQEAaGhoQGJiInx8fLBv3z7k5eVh27ZtqgQnJycHmZmZyMjIwKFDh7Bp0yb069dP9Rqvvvoqpk2bhoMHD+Lhhx9GWloaLly44NB+ElEHcfZsrEREd5Keni4uLi7i6empWl577TURuTY79uzZs1XbxMXFyZw5c0REZM2aNeLj4yP19fVK+Zdffil6vV6ZxTw4OFhefvnlW7YBgLzyyivK8/r6egEgW7Zs6bB+EpHjcAwQEXUJ48ePR05Ojmqdr6+v8thms6nKbDYb9u/fDwCoqKhATEwMPD09lfLRo0ejtbUVlZWV0Ol0OH36NCZMmHDbNgwdOlR57OnpCbPZjJqamv9vl4jIiZgAEVGX4OnpedNPUh3Fw8Pjruq5ubmpnut0OrS2tnZGk4iok3EMEBF1C3v27Lnp+cCBAwEAAwcOxIEDB9DQ0KCU7969G3q9HlFRUfDy8kKfPn1QWFjo0DYTkfPwDBARdQmXL19GdXW1ap2rqyv8/f0BAHl5eRg5ciTGjBmDjz76CHv37sV7770HAEhLS0NWVhbS09ORnZ2NX375BfPnz8fMmTMRGBgIAMjOzsbs2bMREBCA5ORk2O127N69G/Pnz3dsR4nIIZgAEVGXkJ+fD6vVqloXFRWF7777DsC1K7TWr1+PuXPnwmq1Yt26dRg0aBAAwGQyYevWrXj22WcRGxsLk8mElJQUrFq1StlXeno6mpqa8MYbb2DRokXw9/fHY4895rgOEpFD6UREnN0IIqK/h06nw4YNGzB58mRnN4WIugiOASIiIiLNYQJEREREmsMxQETU5fGXfCL6o3gGiIiIiDSHCRARERFpDhMgIiIi0hwmQERERKQ5TICIiIhIc5gAERERkeYwASIiIiLNYQJEREREmsMEiIiIiDTn/wA2JLcBkuZ+AAAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPo8M6Aooyc3"},"outputs":[],"source":["#load weights of best model\n","\n","path = drive_path + \"temp_labels.pkl/saved_weights_10000_uncase.pkl\"\n","model.load_state_dict(torch.load(path))"]},{"cell_type":"markdown","metadata":{"id":"SKmzY2R9oyc4"},"source":["<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">9. Make Predictions</h1>"]},{"cell_type":"code","source":["with open(drive_path + 'tokens_test.pkl', 'rb') as f:\n","    tokens_test = pickle.load(f)\n","\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels.tolist())\n","\n","del tokens_test"],"metadata":{"id":"OHRa4lExvmX9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hm9C8K4goyc4"},"outputs":[],"source":["# get predictions for test data\n","with torch.no_grad():\n","    preds = model(test_seq.to(device), test_mask.to(device))\n","    preds = preds.detach().cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1690752906875,"user":{"displayName":"Daria Bromot","userId":"04565585103339462068"},"user_tz":-180},"id":"00LG5cUioyc4","outputId":"f76f86a5-7c86-437f-9e50-cf3c6148c010"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.26      0.11      0.15        75\n","           1       0.37      0.37      0.37        75\n","           2       0.18      0.11      0.13        75\n","           3       0.17      0.24      0.20        75\n","           4       0.14      0.47      0.22        75\n","           5       0.17      0.05      0.08        75\n","           6       0.19      0.17      0.18        75\n","           7       0.20      0.28      0.24        75\n","           8       0.43      0.21      0.29        75\n","           9       0.33      0.05      0.09        75\n","\n","    accuracy                           0.21       750\n","   macro avg       0.25      0.21      0.19       750\n","weighted avg       0.25      0.21      0.19       750\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","# model's performance\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"]},{"cell_type":"markdown","metadata":{"id":"KNtFXHd3oyc5"},"source":["<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">REFERENCES & CREDITS</h1></center>"]},{"cell_type":"markdown","metadata":{"id":"VrTs1qhboyc5"},"source":["<ol>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.reddit.com/r/MachineLearning/comments/ao23cp/p_how_to_use_bert_in_kaggle_competitions_a/\">How to use BERT in Kaggle competitions - Reddit Thread</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\">A visual guide to using BERT by Jay Alammar</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\">Demystifying BERT: Groundbreaking NLP Framework by Mohd Sanad Zaki Rizvi</a></li>\n","    <li style=\"font-size:150%;\"><a href=\"https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03\">BERT for Dummies step by step tutorial by Michel Kana</a></li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"id":"JzcrrWRToyc6"},"source":["<br>\n","<center><h1 style=\"font-size:300%; font-family:cursive; color:black; background:skyblue; padding:15px; border:solid;\">CONCLUSION</h1></center>"]},{"cell_type":"markdown","metadata":{"id":"KhNy7_-foyc6"},"source":["<p style=\"font-size:150%; font-family:verdana;\">BERT is undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The fact that it’s approachable and allows fast fine-tuning will likely allow a wide range of practical applications in the future. In this Notebook we have discussed about BERT (Theoritical + Practical Part).</p>"]},{"cell_type":"markdown","metadata":{"id":"vstECEGroyc6"},"source":["<center><h1 style=\"font-size:200%; color:green;\">Please give this kernel an UPVOTE to show your appreciation, if you find it useful.</h1></center>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQVQxH2toyc7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}